# -*- coding: utf-8 -*-
"""Tensors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Tensors.ipynb

# Why Use Œ¶<sub>ML</sub>'s Tensors

[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Tensors.ipynb)
&nbsp; ‚Ä¢ &nbsp; [üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)
&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)
&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)
&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()
&nbsp; ‚Ä¢ &nbsp; [<img src="https://github.com/tum-pbs/PhiML/blob/main/docs/images/colab_logo_small.png?raw=1" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)

While you can call many Œ¶<sub>ML</sub> function directly with native tensors, such as Jax tensors or NumPy arrays, we recommend wrapping them in Œ¶<sub>ML</sub> tensors.
These provide several benefits over the native tensors and allow you to write easy-to-read, more concise, more explicit, less error-prone code.

For an introduction into tensors and dimensions, check out the [introduction notebook](Introduction.ht).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install phiml

"""## Named Dimensions

All tensor dimensions in Œ¶<sub>ML</sub> have a name and type which are part of the [tensor shape](Shapes.html).
When creating a Œ¶<sub>ML</sub> tensor, you specify the names and types of all dimensions.
"""

from phiml import math, wrap, tensor
from phiml.math import channel, batch, spatial, instance, dual  # dimension types

wrap([0, 1, 2], channel('integers'))

data = math.random_uniform(batch(examples=2), spatial(x=4, y=3))
data

"""With all dims named, Œ¶<sub>ML</sub> can automatically match dims without requiring you to expand, squeeze or transpose.
Let's add steps to our `data` tensor along `x`.
"""

steps = wrap([0, 1, 2, 3], 'x:s')
steps

math.print(data + steps)

"""As you can see, the *n*th column values now lie in [n-1,n].
Œ¶<sub>ML</sub> automatically expanded `steps` to the shape `(1, steps, 1)` before adding it to `data`.

This feature works with all Œ¶<sub>ML</sub> functions. You can think of it in the following way:

> Tensors implicitly have all possible dimensions, but their values are constant along those not listed in the shape.

A consequence of the automatic reshaping is that

> The dim order of Œ¶<sub>ML</sub> tensors is irrelevant.

In fact, you should never count on a specific order or single out a dimension by its index.

Let's look at another example, creating a meshgrid from scratch.
"""

from phiml import stack, arange

stack({
    'x': arange(spatial(x=4)),
    'y': arange(spatial(y=3))
}, channel('vector'), expand_values=True)

"""The preferred way of referencing dims, e.g. for slicing tensors is by type or name."""

data.examples[1].x[1:3].y[0]

data[{batch: 1, 'x': slice(1, 3), 'y': 0}]

"""Many functions will operate on dims of specific types by default, so you don't need to specify the axis."""

math.fft(data)  # operates on x and y because they are spatial

"""Batch dimensions will be ignored by all functions unless explicitly specified. This makes it trivial to vectorize code w/o using something like `vmap`, simply by passing any batched input. An example of this can be seen [here](https://tum-pbs.github.io/PhiFlow/examples/grids/Batched_Smoke.html).

## Shortened Dim Creation

Functions that only require you to specify name and type, but not the size of dims, allow you to write `<name>:<type_letter>` instead of passing a `Shape` object. Single-letter names additionally default to type spatial and item names can be specified in parentheses.
"""

wrap([1, 2, 3], 'x')

tensor([1, 2, 3], 'vector:c')

stack([1, 2, 3], 'example:b')

"""## Printing Options

As you can see, Œ¶<sub>ML</sub> summarizes tensors by default and color-codes the result text.
The Python formatting options let you customize how a tensor is printed, with options being separated by colons.
Here are some examples:
"""

print(f"{data:summary:color:shape:dtype:.5e}")

print(f"{data:full:color:shape:dtype:.3f}")

print(f"{data:numpy:no-color:no-shape:no-dtype:.2f}")

"""The order of the formatting arguments is not important.
Supported options are:

**Layout:**
The layout determines what is printed and where. The following options are available:

* `summary` Summarizes the values by mean, standard deviation, minimum and maximum value.
* `row` Prints the tensor as a single-line vector.
* `full` Prints all values in the tensors as a multi-line string.
* `numpy` Uses the formatting of NumPy

**Number format**:
You can additionally specify a format string for floating-point numbers like `.3f` or `.2e`.

**Color**:
Use the keywords `color` or `no-color`.
Currently `color` will use ANSI color codes which are supported by most terminals, IDEs as well as Jupyter notebooks.

**Additional tensor information**:
The keywords `shape`, `no-shape`, `dtype` and `no-dtype` can be used to show or hide additional properties of the tensor.

## Wrapping and Unwrapping

You can wrap existing tensors in Œ¶<sub>ML</sub> tensors using [`wrap()`](phiml/math#phiml.math.wrap) or [`tensor()`](phiml/math#phiml.math.tensor).
While `tensor()` will convert the data to the [default backend](Convert.html), `wrap()` will keep the data as-is.
In either case, you need to specify the dimension names and types when wrapping a native tensor.
"""

math.use('torch')
math.tensor([0, 1, 2], batch('examples'))

"""To unwrap a tensor, you can use `tensor.native()` or `math.reshaped_native()` for more control over the result shape.
In both cases, the requested dimension order needs to be specified.
"""

data.native('examples,x,y')

"""Similarly, you can get the NumPy representation:"""

data.numpy('examples,x,y')

"""## Further Reading

Check out the [examples](https://tum-pbs.github.io/PhiML/Examples.html) to see how using Œ¶<sub>ML</sub>'s tensors is different from the other libraries.

Learn more about the [dimension types](Shapes.html) and their [advantages](Dimension_Names_Types.html).

Œ¶<sub>ML</sub> unifies [data types](Data_Types.html) as well and lets you set the floating point precision globally or by context.

While the dimensionality of neural networks must be specified during network creation, this is not the case for math functions.
These [automatically adapt to the number of spatial dimensions of the data that is passed in](N_Dimensional.html).

[üåê **Œ¶<sub>ML</sub>**](https://github.com/tum-pbs/PhiML)
&nbsp; ‚Ä¢ &nbsp; [üìñ **Documentation**](https://tum-pbs.github.io/PhiML/)
&nbsp; ‚Ä¢ &nbsp; [üîó **API**](https://tum-pbs.github.io/PhiML/phiml)
&nbsp; ‚Ä¢ &nbsp; [**‚ñ∂ Videos**]()
&nbsp; ‚Ä¢ &nbsp; [<img src="https://github.com/tum-pbs/PhiML/blob/main/docs/images/colab_logo_small.png?raw=1" height=4>](https://colab.research.google.com/github/tum-pbs/PhiML/blob/main/docs/Examples.ipynb) [**Examples**](https://tum-pbs.github.io/PhiML/Examples.html)
"""