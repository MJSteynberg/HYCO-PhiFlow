# @package _global_.trainer_params
# Note: The following behaviors are hardcoded:
# - use_sliding_window: always true (better memory efficiency)
# - validate_on_train: always false (standard ML practice)
# - validation_rollout: always true (accurate validation)
# - save_best_only: always true (saves disk space)

learning_rate: 0.0001
batch_size: 16
epochs: 100
num_predict_steps: 4

train_sim: []
val_sim: []  # Empty by default, should be specified in experiment configs

# Validation settings
validate_every: 1  # Validate every N epochs
validation_rollout_steps: 75  # Number of timesteps for validation rollout (null for full length)

# Optimizer settings
optimizer: 'adam'
scheduler: 'cosine'
weight_decay: 0.0

# Checkpoint settings
checkpoint_freq: 10

# Progress reporting
print_freq: 10

# Data augmentation configuration (optional for synthetic trainer)
augmentation:
  enabled: false  # Disabled by default for pure synthetic training
  alpha: 0.1
  cache:
    experiment_name: "${data.dset_name}"
    max_memory_samples: 1000
    reuse_existing: true
  device: "cuda"
