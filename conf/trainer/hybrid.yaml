# HYCO Hybrid Training Configuration
# Note: Hardcoded behaviors:
# - validation_rollout: always true
# - save_best_only: always true
# - augmentation uses cached strategy only

# General training settings
epochs: 50
num_predict_steps: 10
train_sim: [0, 1, 2]
val_sim: [3]

# Hybrid-specific parameters
alpha: 0.5  # Weight for real data (0.5 = equal real/synthetic)
interleave_frequency: 1  # Train both models every N epochs
warmup_epochs: 5  # Pre-train synthetic model alone

# Data augmentation configuration
augmentation:
  enabled: true  # Enable data augmentation
  alpha: 0.1  # Augmentation ratio: num_synthetic = alpha * num_real
  
  # Cache settings
  cache:
    experiment_name: "${data.dset_name}"  # Use dataset name for cache directory
    max_memory_samples: 1000  # LRU cache size (number of samples)
    reuse_existing: true  # Reuse cache from previous runs
  
  # Generation device
  device: "cuda"  # Device for prediction generation (cuda/cpu)

# Synthetic model settings
synthetic:
  learning_rate: 1e-4
  batch_size: 16
  optimizer: adam
  scheduler: cosine
  weight_decay: 0.0

# Physical model settings (epochs controls max iterations)
physical:
  method: 'L-BFGS-B'
  abs_tol: 1e-6
  suppress_convergence_errors: true  # Don't raise errors on non-convergence

# Checkpointing
checkpoint_freq: 10
checkpoint_dir: 'results/models/hybrid'
