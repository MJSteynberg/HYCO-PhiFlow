# HYCO Hybrid Training Configuration

# General training settings
epochs: 50
num_predict_steps: 10
train_sim: [0, 1, 2]
val_sim: [3]

# Hybrid-specific parameters
alpha: 0.5  # Weight for real data (0.5 = equal real/synthetic)
interleave_frequency: 1  # Train both models every N epochs
warmup_epochs: 5  # Pre-train synthetic model alone

# Data augmentation configuration
augmentation:
  enabled: true  # Enable data augmentation
  alpha: 0.1  # Augmentation ratio: num_synthetic = alpha * num_real
  strategy: "cached"  # Strategy: "cached" or "on_the_fly"
  
  # Cache settings
  cache:
    experiment_name: "${data.dset_name}"  # Use dataset name for cache directory
    format: "dict"  # Save format: "dict" or "tuple"
    max_memory_samples: 1000  # LRU cache size (number of samples)
    reuse_existing: true  # Reuse cache from previous runs
  
  # On-the-fly generation settings
  on_the_fly:
    generate_every: 1  # Regenerate every N epochs (0 = never regenerate)
    batch_size: 32  # Batch size for prediction generation
    rollout_steps: 10  # For physical models: number of rollout steps
  
  # Generation device
  device: "cuda"  # Device for prediction generation (cuda/cpu)

# Synthetic model settings
synthetic:
  learning_rate: 1e-4
  batch_size: 16
  optimizer: adam
  scheduler: cosine
  weight_decay: 0.0

# Physical model settings
physical:
  method: 'L-BFGS-B'
  abs_tol: 1e-6
  max_iterations: 5  # Low for fast iterations in hybrid training
  suppress_convergence_errors: true  # Don't raise errors on non-convergence

# Checkpointing
save_interval: 10
save_best_only: true
checkpoint_dir: 'results/models/hybrid'

# Memory and performance
enable_memory_monitoring: false
