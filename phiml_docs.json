{
    "name": "phi",
    "type": "package",
    "submodules": [
        {
            "type": "module",
            "name": "phiml_backend_torch",
            "docstring": "PyTorch integration.",
            "functions": [],
            "classes": [],
            "submodules": [
                {
                    "type": "module",
                    "name": "phiml.backend.torch.nets",
                    "docstring": "PyTorch implementation of the unified machine learning API.\nEquivalent functions also exist for the other frameworks.\n\nFor API documentation, see `phiml.nn`.",
                    "functions": [
                        {
                            "name": "adagrad",
                            "qualname": "adagrad",
                            "docstring": "",
                            "signature": "(net: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]], learning_rate: float = 0.001, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0.0, eps=1e-10)"
                        },
                        {
                            "name": "adam",
                            "qualname": "adam",
                            "docstring": "",
                            "signature": "(net: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]], learning_rate: float = 0.001, betas=(0.9, 0.999), epsilon=1e-07)"
                        },
                        {
                            "name": "conv_classifier",
                            "qualname": "conv_classifier",
                            "docstring": "",
                            "signature": "(in_features: int, in_spatial: Union[tuple, list], num_classes: int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)"
                        },
                        {
                            "name": "conv_net",
                            "qualname": "conv_net",
                            "docstring": "",
                            "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) -> torch.nn.modules.module.Module"
                        },
                        {
                            "name": "coupling_layer",
                            "qualname": "coupling_layer",
                            "docstring": "",
                            "signature": "(in_channels: int, activation: Union[str, type] = 'ReLU', batch_norm=False, reverse_mask=False, in_spatial: Union[tuple, int] = 2)"
                        },
                        {
                            "name": "get_learning_rate",
                            "qualname": "get_learning_rate",
                            "docstring": "Returns the current learning rate of the optimizer.\n\nArgs:\n    optimizer (optim.Optimizer): The optimizer whose learning rate needs to be retrieved.\n\nReturns:\n    float: The current learning rate of the optimizer.",
                            "signature": "(optimizer: torch.optim.optimizer.Optimizer)"
                        },
                        {
                            "name": "get_mask",
                            "qualname": "get_mask",
                            "docstring": "Compute mask for slicing input feature map for Invertible Nets",
                            "signature": "(inputs, reverse_mask, data_format='NHWC')"
                        },
                        {
                            "name": "get_parameters",
                            "qualname": "get_parameters",
                            "docstring": "",
                            "signature": "(net: torch.nn.modules.module.Module, wrap=True) -> dict"
                        },
                        {
                            "name": "invertible_net",
                            "qualname": "invertible_net",
                            "docstring": "",
                            "signature": "(num_blocks: int, construct_net: Union[str, Callable], **construct_kwargs)"
                        },
                        {
                            "name": "load_state",
                            "qualname": "load_state",
                            "docstring": "",
                            "signature": "(obj: Union[torch.nn.modules.module.Module, torch.optim.optimizer.Optimizer], path: str)"
                        },
                        {
                            "name": "mlp",
                            "qualname": "mlp",
                            "docstring": "",
                            "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm=False, activation: Union[str, Callable] = 'ReLU', softmax=False) -> torch.nn.modules.module.Module"
                        },
                        {
                            "name": "res_net",
                            "qualname": "res_net",
                            "docstring": "",
                            "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) -> torch.nn.modules.module.Module"
                        },
                        {
                            "name": "rmsprop",
                            "qualname": "rmsprop",
                            "docstring": "",
                            "signature": "(net: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]], learning_rate: float = 0.001, alpha=0.99, eps=1e-08, weight_decay=0.0, momentum=0.0, centered=False)"
                        },
                        {
                            "name": "save_state",
                            "qualname": "save_state",
                            "docstring": "",
                            "signature": "(obj: Union[torch.nn.modules.module.Module, torch.optim.optimizer.Optimizer], path: str)"
                        },
                        {
                            "name": "set_learning_rate",
                            "qualname": "set_learning_rate",
                            "docstring": "Sets the global learning rate for the given optimizer.\n\nArgs:\n    optimizer (optim.Optimizer): The optimizer whose learning rate needs to be updated.\n    learning_rate (float): The new learning rate to set.",
                            "signature": "(optimizer: torch.optim.optimizer.Optimizer, learning_rate: float)"
                        },
                        {
                            "name": "sgd",
                            "qualname": "sgd",
                            "docstring": "",
                            "signature": "(net: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]], learning_rate: float = 0.001, momentum=0.0, dampening=0.0, weight_decay=0.0, nesterov=False)"
                        },
                        {
                            "name": "u_net",
                            "qualname": "u_net",
                            "docstring": "",
                            "signature": "(in_channels: int, out_channels: int, levels: int = 4, filters: Union[int, Sequence] = 16, batch_norm: bool = True, activation: Union[str, type] = 'ReLU', in_spatial: Union[tuple, int] = 2, periodic=False, use_res_blocks: bool = False, down_kernel_size=3, up_kernel_size=3) -> torch.nn.modules.module.Module"
                        },
                        {
                            "name": "update_weights",
                            "qualname": "update_weights",
                            "docstring": "",
                            "signature": "(net: Union[torch.nn.modules.module.Module, Sequence[torch.nn.modules.module.Module]], optimizer: torch.optim.optimizer.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs)"
                        }
                    ],
                    "classes": [
                        {
                            "name": "ConvClassifier",
                            "qualname": "ConvClassifier",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "ConvClassifier.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "ConvNet",
                            "qualname": "ConvNet",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "ConvNet.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "CouplingLayer",
                            "qualname": "CouplingLayer",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(construct_net: Callable, construction_kwargs: dict, reverse_mask)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "CouplingLayer.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x, invert=False)"
                                }
                            ]
                        },
                        {
                            "name": "DenseNet",
                            "qualname": "DenseNet",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(layers: list, activation: type, batch_norm: bool, use_softmax: bool)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "DenseNet.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "DenseResNetBlock",
                            "qualname": "DenseResNetBlock",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(layers, batch_norm, activation)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "DenseResNetBlock.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "DoubleConv",
                            "qualname": "DoubleConv",
                            "docstring": "(convolution => [BN] => ReLU) * 2\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool, kernel_size=3)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "DoubleConv.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "Down",
                            "qualname": "Down",
                            "docstring": "Downscaling with maxpool then double conv or resnet_block\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic, kernel_size: int)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "Down.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "InvertibleNet",
                            "qualname": "InvertibleNet",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(num_blocks: int, construct_net, construction_kwargs: dict)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "InvertibleNet.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x, backward=False)"
                                }
                            ]
                        },
                        {
                            "name": "ResNet",
                            "qualname": "ResNet",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "ResNet.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "ResNetBlock",
                            "qualname": "ResNetBlock",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool, kernel_size=3)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "ResNetBlock.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "UNet",
                            "qualname": "UNet",
                            "docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n\n    class Model(nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will also have their\nparameters converted when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, down_kernel_size: int, up_kernel_size: int)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "UNet.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x)"
                                }
                            ]
                        },
                        {
                            "name": "Up",
                            "qualname": "Up",
                            "docstring": "Upscaling then double conv\n\nInitialize internal Module state, shared by both nn.Module and ScriptModule.",
                            "signature": "(d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool, kernel_size: int)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "forward",
                                    "qualname": "Up.forward",
                                    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.",
                                    "signature": "(self, x1, x2)"
                                }
                            ]
                        }
                    ],
                    "submodules": []
                }
            ]
        },
        {
            "type": "module",
            "name": "phiml_dataclasses",
            "docstring": "PhiML makes it easy to work with custom classes.\nAny class decorated with `@dataclass` can be used with `phiml.math` functions, such as `shape()`, `slice()`, `stack()`, `concat()`, `expand()` and many more.\nWe recommend always setting `frozen=True`.\n\nPhiML's dataclass support explicitly handles properties defined decorated with `functools.cached_property`.\nTheir cached values will be preserved whenever possible, preventing costly re-computation when modifying unrelated properties, slicing, gathering, or stacking objects.\nThis will usually affect all *data fields*, i.e. fields that hold `Tensor` or composite properties.\n\nDataclass fields can additionally be specified as being *variable* and *value*.\nThis affects which data_fields are optimized / traced by functions like `phiml.math.jit_compile` or `phiml.math.minimize`.\n\n**Template for custom classes:**\n\n>>> from typing import Tuple from dataclasses import dataclass\n>>> from phiml.dataclasses import sliceable, cached_property\n>>> from phiml.math import Tensor, Shape, shape\n>>>\n>>> @sliceable\n>>> @dataclass(frozen=True)\n>>> class MyClass:\n>>>     # --- Attributes ---\n>>>     attribute1: Tensor\n>>>     attribute2: 'MyClass' = None\n>>>\n>>>     # --- Additional fields ---\n>>>     field1: str = 'x'\n>>>\n>>>     # --- Special fields declaring attribute types. Must be of type Tuple[str, ...] ---\n>>>     variable_attrs: Tuple[str, ...] = ('attribute1', 'attribute2')\n>>>     value_attrs: Tuple[str, ...] = ()\n>>>\n>>>     def __post_init__(self):\n>>>         assert self.field1 in 'xyz'\n>>>\n>>>     @cached_property\n>>>     def shape(self) -> Shape:  # override the default shape which is merged from all attribute shapes\n>>>         return self.attribute1.shape & shape(self.attribute2)\n>>>\n>>>     @cached_property  # the cache will be copied to derived instances unless attribute1 changes (this is analyzed from the code)\n>>>     def derived_property(self) -> Tensor:\n>>>         return self.attribute1 + 1",
            "functions": [
                {
                    "name": "config_fields",
                    "qualname": "config_fields",
                    "docstring": "List all dataclass Fields of `obj` that are not considered data_fields or special.\nThese cannot hold any Tensors or shaped objects.\n\nArgs:\n    obj: Dataclass type or instance.\n\nReturns:\n    Sequence of `dataclasses.Field`.",
                    "signature": "(obj) -> Sequence[dataclasses.Field]"
                },
                {
                    "name": "copy",
                    "qualname": "copy",
                    "docstring": "Create a copy of `obj`, including cached properties.\n\nArgs:\n    obj: Dataclass instance.\n    call_metaclass: Whether to copy `obj` by invoking `type(obj).__call__`.\n        If `obj` defines a metaclass, this will allow users to define custom constructors for dataclasses.",
                    "signature": "(obj: ~PhiMLDataclass, /, call_metaclass=False) -> ~PhiMLDataclass"
                },
                {
                    "name": "data_eq",
                    "qualname": "data_eq",
                    "docstring": "Decorator for dataclasses that overrides the default `__eq__` method to compare data fields by shape and value instead of equality.\nNon-data fields are compared by equality (`==`).\n\nSee Also:\n    `equal()`, `data_fields()`, `non_data_fields()`.\n\nArgs:\n    rel_tolerance: Relative tolerance for comparing floating point tensors.\n    abs_tolerance: Absolute tolerance for comparing floating point tensors.\n    equal_nan: Whether to consider NaN values as equal.\n    compare_tensors_by_ref: If True, compares all tensors by reference (location in memory) instead of value.\n        This avoids costly element-wise comparisons, but may count objects as unequal even if they hold the same data.",
                    "signature": "(cls=None, /, *, rel_tolerance=0.0, abs_tolerance=0.0, equal_nan=True, compare_tensors_by_ref=False)"
                },
                {
                    "name": "data_fields",
                    "qualname": "data_fields",
                    "docstring": "List all dataclass Fields of `obj` that are considered data, i.e. can hold (directly or indirectly) one or multiple `Tensor` instances.\nThis includes fields referencing other dataclasses.\n\nArgs:\n    obj: Dataclass type or instance.\n\nReturns:\n    Sequence of `dataclasses.Field`.",
                    "signature": "(obj) -> Sequence[dataclasses.Field]"
                },
                {
                    "name": "equal",
                    "qualname": "equal",
                    "docstring": "Checks if two dataclass instances are equal by comparing their data fields by value and shape.\nNon-data fields are compared by equality (`==`).\n\nArgs:\n    obj1: First dataclass instance.\n    obj2: Second dataclass instance.\n    rel_tolerance: Relative tolerance for comparing floating point tensors.\n    abs_tolerance: Absolute tolerance for comparing floating point tensors.\n    equal_nan: Whether to consider NaN values as equal.\n\nReturns:\n    `bool`",
                    "signature": "(obj1, obj2, rel_tolerance=0.0, abs_tolerance=0.0, equal_nan=True)"
                },
                {
                    "name": "get_cache_files",
                    "qualname": "get_cache_files",
                    "docstring": "Searches the data structure for all disk-cached tensors and returns all referenced files.\n\nArgs:\n    obj: `Tensor` or pytree or dataclass (`phiml.math.magic.PhiTreeNode`).\n\nReturns:\n    Collection of file paths.",
                    "signature": "(obj) -> Set[str]"
                },
                {
                    "name": "getitem",
                    "qualname": "getitem",
                    "docstring": "Slice / gather a dataclass by broadcasting the operation to its data_fields.\n\nYou may call this from `__getitem__` to allow the syntax `my_class[component_str]`, `my_class[slicing_dict]`, `my_class[boolean_tensor]` and `my_class[index_tensor]`.\n\n```python\ndef __getitem__(self, item):\n    return getitem(self, item)\n```\n\nArgs:\n    obj: Dataclass instance to slice / gather.\n    item: One of the supported tensor slicing / gathering values.\n    keepdims: Dimensions that will not be removed during slicing.\n        When selecting a single slice, these dims will remain with size 1.\n\nReturns:\n    Slice of `obj` of same type.",
                    "signature": "(obj: ~PhiMLDataclass, item, keepdims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = None) -> ~PhiMLDataclass"
                },
                {
                    "name": "load_cache_as",
                    "qualname": "load_cache_as",
                    "docstring": "Context manager to temporarily set the backend for loading disk-cached tensors into memory.\n\nArgs:\n    host_backend: Backend name or instance to use in the current process.\n        This also applies to `parallel_compute()` stages that cannot be parallelized and thus run in the host process.\n    worker_backend: Backend name or instance to use in worker processes, see `parallel_compute()`. If `None`, the workers will use the same backend per tensor as the host.\n\nUsage:\n\n    >>> with load_cache_as('torch', worker_backend='numpy'):",
                    "signature": "(host_backend: Union[str, phiml.backend._backend.Backend], worker_backend: Union[str, phiml.backend._backend.Backend] = None)"
                },
                {
                    "name": "non_data_fields",
                    "qualname": "non_data_fields",
                    "docstring": "List all dataclass Fields of `obj` that cannot hold tensors (directly or indirectly).\n\nArgs:\n    obj: Dataclass type or instance.\n\nReturns:\n    Sequence of `dataclasses.Field`.",
                    "signature": "(obj) -> Sequence[dataclasses.Field]"
                },
                {
                    "name": "on_load_into_memory",
                    "qualname": "on_load_into_memory",
                    "docstring": "Register a function to be called every time a cached tensor is loaded from disk into memory in this process.\n\nTensors are stored on disk to save memory, e.g. in `parallel_compute()` if a memory limit is specified.\n\nWhen accessing the tensor values or using them in any computation, the values are temporarily loaded into memory and `callback` is called.\n\nArgs:\n    callback: Function `callback(Tensor)`.",
                    "signature": "(callback: Callable[[phiml.math._tensors.Tensor], NoneType])"
                },
                {
                    "name": "parallel_compute",
                    "qualname": "parallel_compute",
                    "docstring": "Compute the values of properties decorated with `@cached_property` or `@parallel_property` of a dataclass instance in parallel.\n\n**Multiple stages via `requires=...`**\nIf `@parallel_property` are computed whose `requires` overlaps with `parallel_dims`, a separate computation stage is set up to compute these properties with fewer parallel workers.\nIn the presence of different `requires`, the computation is split into different stages in accordance with the property dependency graph.\nProperties that cannot be parallelized (because it requires all `parallel_dims`) are computed on the host process.\n\n**Caching tensors to disk**\nWhen `memory_limit` and `cache_dir` are set, the evaluation will try to adhere to the given memory limits by moving tensors out of memory onto disk.\nThis is only applied to the outputs of `@cached_property` and `@parallel_property` calls, not to intermediate values used in their computation.\nThe per-process memory limit is calculated per stage, dividing the total memory by the active worker count.\nCached tensors behave like regular tensors and are temporarily loaded back into memory when accessing their values or using them in a computation.\nWhen parallelizing, the full result is assembled by stacking multiple disk-backed tensors from different files created by different processes.\nThese composite tensors will reference multiple binary files and can be pickled/unpickled safely without loading the data into memory.\nThis enables passing large data references to different processes or saving the structure to a file without the data content.\n\nSee Also:\n    `cached_property`, `parallel_property`, `get_cache_files()`, `on_load_into_memory()`.\n\nWarnings:\n    `parallel_compute` breaks automatic differentiation.\n\nArgs:\n    instance: Dataclass instance for which to compute the values of `@cached_property` or `@parallel_property` fields.\n    properties: References to the unbound properties. These must be `cached_property` or `parallel_property`.\n    parallel_dims: Dimensions to parallelize over.\n    max_workers: Number of processes to spawn.\n    memory_limit: Limit to the total memory consumption from `Tensor` instances on property outputs.\n    cache_dir: Directory path to store cached tensors in if `memory_limit` is set.\n    keep_intermediate: Whether the outputs of cached properties required to compute `properties` but not contained in `properties` should be kept in memory.\n        If `False`, these values will not be cached on `instance` after this call.",
                    "signature": "(instance, properties: Sequence, parallel_dims=<function batch at 0x00000213FF433560>, max_workers=20, memory_limit: Optional[float] = None, cache_dir: str = None, keep_intermediate=False)"
                },
                {
                    "name": "parallel_property",
                    "qualname": "parallel_property",
                    "docstring": "Similar to `@cached_property` but with additional controls over parallelization.\n\nSee Also:\n    `parallel_compute()`.\n\nArgs:\n    func: Method to wrap.\n    requires: Dimensions which must be present within one process. These cannot be parallelized when computing this property.\n    on_direct_eval: What to do when the property is accessed normally (outside `parallel_compute`) before it has been computed.\n        Option:\n\n        * `'raise'`: Raise an error.\n        * `'host-compute'`: Compute the property directly, without using multi-threading.",
                    "signature": "(func: Callable = None, /, requires: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = None, on_direct_eval='raise')"
                },
                {
                    "name": "replace",
                    "qualname": "replace",
                    "docstring": "Create a copy of `obj` with some fields replaced.\nUnlike `dataclasses.replace()`, this function also transfers `@cached_property` members if their dependencies are not affected.\n\nArgs:\n    obj: Dataclass instance.\n    call_metaclass: Whether to copy `obj` by invoking `type(obj).__call__`.\n        If `obj` defines a metaclass, this will allow users to define custom constructors for dataclasses.\n    **changes: New field values to replace old ones.\n\nReturns:\n    Copy of `obj` with replaced values.",
                    "signature": "(obj: ~PhiMLDataclass, /, call_metaclass=False, **changes) -> ~PhiMLDataclass"
                },
                {
                    "name": "set_cache_ttl",
                    "qualname": "set_cache_ttl",
                    "docstring": "Sets the time to live (TTL) for data loaded into memory for disk-backed tensors.\nThis function should be called before the tensor cache is used.\n\nArgs:\n    ttl_seconds: Time to live. If `None`, data will be unallocated immediately after use.",
                    "signature": "(ttl_seconds: Optional[float])"
                },
                {
                    "name": "sliceable",
                    "qualname": "sliceable",
                    "docstring": "Decorator for frozen dataclasses, adding slicing functionality by defining `__getitem__` and enabling the `instance.dim` syntax.\nThis enables slicing similar to tensors, gathering and boolean masking.\n\nArgs:\n    dim_attrs: Whether to generate `__getattr__` that allows slicing via the syntax `instance.dim[...]` where `dim` is the name of any dim present on `instance`.\n    t_props: Whether to generate the properties `Tc`, `Ts` and `Ti` for transposing channel/spatial/instance dims.\n    keepdims: Which dimensions should be kept with size 1 taking a single slice along them. This will preserve labels.\n    dim_repr: Whether to replace the default `repr` of a dataclass by a simplified one based on the object's shape.\n    lazy_dims: If `False`, instantiates all dims of `shape(self)` as member variables during construction. Dataclass must have `slots=False`.\n        If `True`, implements `__getattr__` to instantiate accessed dims on demand. This will be skipped if a user-defined `__getattr__` is found.",
                    "signature": "(cls=None, /, *, dim_attrs=True, t_props=True, keepdims=None, dim_repr=True, lazy_dims=True)"
                },
                {
                    "name": "special_fields",
                    "qualname": "special_fields",
                    "docstring": "List all special dataclass Fields of `obj`, i.e. fields that don't store data related to the object but rather meta-information relevant to PhiML.\n\nThese include `variable_attrs` and `value_attrs`.\n\nArgs:\n    obj: Dataclass type or instance.\n\nReturns:\n    Sequence of `dataclasses.Field`.",
                    "signature": "(obj) -> Sequence[dataclasses.Field]"
                }
            ],
            "classes": [
                {
                    "name": "cached_property",
                    "qualname": "cached_property",
                    "docstring": "",
                    "signature": "(func)",
                    "type": "class",
                    "methods": []
                }
            ],
            "submodules": []
        },
        {
            "type": "module",
            "name": "phiml_latents",
            "docstring": "Dimensionality reduction and latent space models.\n\nThis module provides functionality for fitting and using latent models, such as UMAP, to reduce the dimensionality of data.",
            "functions": [
                {
                    "name": "fit",
                    "qualname": "fit",
                    "docstring": "Fit a latent model to the data.\n\nArgs:\n    data: Tensor to fit the model to. Must contain `list_dim`. If no `feature_dim` is present, 1D data is assumed.\n    model: Model type to fit. Currently only 'UMAP' is supported.\n    latent_dim: The dimension of the latent space to use. This determines the number of components in the latent representation.\n        The shape can be passed as a string spec, e.g. '(x,y)'.\n        Alternatively, the number of components can be passed as an `int`, in which case the shape will be channel of name 'vector'.\n    feature_dim: The dimension of the feature space used in distance computations.\n    list_dim: Dimension along which data points belonging to one model are listed. Any dims not marked as list or feature are considered as batch dims.\n\nReturns:\n    LatentModel: A LatentModel object containing the fitted model and its parameters.",
                    "signature": "(data: phiml.math._tensors.Tensor, model: str, latent_dim: Union[str, phiml.math._shape.Shape, int] = (vector\u1d9c=l1,l2), feature_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000021CFF883600>, list_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function instance at 0x0000021CFF8836A0>, **model_kwargs) -> phiml.latent.LatentModel"
                }
            ],
            "classes": [
                {
                    "name": "LatentModel",
                    "qualname": "LatentModel",
                    "docstring": "LatentModel(model_type: str, model: Any, feature_dim: phiml.math._shape.Shape, latent_dim: phiml.math._shape.Shape, can_reconstruct: bool)",
                    "signature": "(model_type: str, model: Any, feature_dim: phiml.math._shape.Shape, latent_dim: phiml.math._shape.Shape, can_reconstruct: bool) -> None",
                    "type": "class",
                    "methods": [
                        {
                            "name": "embed",
                            "qualname": "LatentModel.embed",
                            "docstring": "Embed the data using the fitted model or by training a new dimensionality reduction model.\n\nArgs:\n    data: Tensor to embed. Must contain `list_dim`. If no `feature_dim` is present, 1D data is assumed.\n\nReturns:\n    The embedded data as a `Tensor`.",
                            "signature": "(self, data: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                        },
                        {
                            "name": "reconstruct",
                            "qualname": "LatentModel.reconstruct",
                            "docstring": "Reconstruct the data from the latent representation if supported.\nCheck `self.can_reconstruct` to see if reconstruction is supported.\n\nArgs:\n    latent: Tensor containing the latent representation. Must contain `self.latent_dim`.\n\nReturns:\n    The reconstructed data as a `Tensor`.",
                            "signature": "(self, latent: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                        }
                    ]
                }
            ],
            "submodules": []
        },
        {
            "type": "module",
            "name": "phiml_math",
            "docstring": "Vectorized operations, tensors with named dimensions.\n\nThis package provides a common interface for tensor operations.\nIs internally uses NumPy, TensorFlow or PyTorch.\n\nMain classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.\n\nThe provided operations are not implemented directly.\nInstead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.\nThis allows the user to write simulation code once and have it run with various computation backends.\n\nSee the documentation at https://tum-pbs.github.io/PhiML/",
            "functions": [
                {
                    "name": "abs",
                    "qualname": "abs_",
                    "docstring": "Computes *||x||<sub>1</sub>*.\nComplex `x` result in matching precision float values.\n\n*Note*: The gradient of this operation is undefined for *x=0*.\nTensorFlow and PyTorch return 0 while Jax returns 1.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode`\n\nReturns:\n    Absolute value of `x` of same type as `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "abs_square",
                    "qualname": "abs_square",
                    "docstring": "Squared magnitude of complex values.\n\nArgs:\n  complex_values: complex `Tensor`\n\nReturns:\n    Tensor: real valued magnitude squared",
                    "signature": "(complex_values: Union[phiml.math._tensors.Tensor, complex]) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "all",
                    "qualname": "all_",
                    "docstring": "Tests whether all entries of `boolean_tensor` are `True` along the specified dimensions.\n\nArgs:\n    boolean_value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(boolean_value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "all_available",
                    "qualname": "all_available",
                    "docstring": "Tests if all tensors contained in the given `values` are currently known and can be read.\nPlaceholder tensors used to trace functions for just-in-time compilation or matrix construction are considered not available, even when they hold example values like with PyTorch's JIT.\n\nTensors are not available during `jit_compile()`, `jit_compile_linear()` or while using TensorFlow's legacy graph mode.\n\nTensors are typically available when the backend operates in eager mode and is not currently tracing a function.\n\nThis can be used instead of the native checks\n\n* PyTorch: `torch._C._get_tracing_state()`\n* TensorFlow: `tf.executing_eagerly()`\n* Jax: `isinstance(x, jax.core.Tracer)`\n\nArgs:\n    values: Tensors to check.\n\nReturns:\n    `True` if no value is a placeholder or being traced, `False` otherwise.",
                    "signature": "(*values) -> bool"
                },
                {
                    "name": "always_close",
                    "qualname": "always_close",
                    "docstring": "Checks whether two tensors are guaranteed to be `close` in all values.\nUnlike `close()`, this function can be used with JIT compilation and with tensors of incompatible shapes.\nIncompatible tensors are never close.\n\nIf one of the given tensors is being traced, the tensors are only equal if they reference the same native tensor.\nOtherwise, an element-wise equality check is performed.\n\nSee Also:\n    `close()`.\n\nArgs:\n    t1: First tensor or number to compare.\n    t2: Second tensor or number to compare.\n    rel_tolerance: Relative tolerance, only used if neither tensor is traced.\n    abs_tolerance: Absolute tolerance, only used if neither tensor is traced.\n    equal_nan: If `True`, tensors are considered close if they are NaN in the same places.\n\nReturns:\n    `bool`",
                    "signature": "(t1: Union[numbers.Number, phiml.math._tensors.Tensor, bool], t2: Union[numbers.Number, phiml.math._tensors.Tensor, bool], rel_tolerance=1e-05, abs_tolerance=0, equal_nan=False) -> bool"
                },
                {
                    "name": "angle",
                    "qualname": "angle",
                    "docstring": "Compute the angle of a complex number.\nThis is equal to *atan(Im/Re)* for most values.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode`\n\nReturns:\n    Angle of complex number in radians.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "any",
                    "qualname": "any_",
                    "docstring": "Tests whether any entry of `boolean_tensor` is `True` along the specified dimensions.\n\nArgs:\n    boolean_value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(boolean_value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "arange",
                    "qualname": "arange",
                    "docstring": "Returns evenly spaced values between `start` and `stop`.\nIf only one limit is given, `0` is used for the start.\n\nSee Also:\n    `range_tensor()`, `linspace()`, `meshgrid()`.\n\nArgs:\n    dim: Dimension name and type as `Shape` object.\n        The `size` of `dim` is interpreted as `stop` unless `start_or_stop` is specified.\n    start_or_stop: (Optional) `int`. Interpreted as `start` if `stop` is specified as well. Otherwise this is `stop`.\n    stop: (Optional) `int`. `stop` value.\n    step: Distance between values.\n    backend: Backend to use for creating the tensor. If unspecified, uses the current default.\n\nReturns:\n    `Tensor`",
                    "signature": "(dim: phiml.math._shape.Shape, start_or_stop: Optional[int] = None, stop: Optional[int] = None, step=1, backend=None) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "arccos",
                    "qualname": "arccos",
                    "docstring": "Computes the inverse of *cos(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.\nFor real arguments, the result lies in the range [0, \u03c0].",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "arccosh",
                    "qualname": "arccosh",
                    "docstring": "Computes the inverse of *cosh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "arcsin",
                    "qualname": "arcsin",
                    "docstring": "Computes the inverse of *sin(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.\nFor real arguments, the result lies in the range [-\u03c0/2, \u03c0/2].",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "arcsinh",
                    "qualname": "arcsinh",
                    "docstring": "Computes the inverse of *sinh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "arctan",
                    "qualname": "arctan",
                    "docstring": "Computes the inverse of *tan(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.\n\nArgs:\n    x: Input. The single-argument `arctan` function cannot output \u03c0/2 or -\u03c0/2 since tan(\u03c0/2) is infinite.\n    divide_by: If specified, computes `arctan(x/divide_by)` so that it can return \u03c0/2 and -\u03c0/2.\n        This is equivalent to the common `arctan2` function.",
                    "signature": "(x: ~TensorOrTree, divide_by=None) -> ~TensorOrTree"
                },
                {
                    "name": "arctanh",
                    "qualname": "arctanh",
                    "docstring": "Computes the inverse of *tanh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "argmax",
                    "qualname": "argmax",
                    "docstring": "Finds the maximum value along one or multiple dimensions and returns the corresponding index.\n\nSee Also:\n    `argmin`, `at_max`.\n\nArgs:\n    x: `Tensor`\n    dim: Dimensions along which the maximum should be determined. These are reduced in the operation.\n    index_dim: Dimension listing the index components for multidimensional argmax.\n\nReturns:\n    Index tensor `idx`, such that `x[idx] = max(x)`.",
                    "signature": "(x: phiml.math._tensors.Tensor, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], index_dim=(index\u1d9c=None))"
                },
                {
                    "name": "argmin",
                    "qualname": "argmin",
                    "docstring": "Finds the minimum value along one or multiple dimensions and returns the corresponding index.\n\nSee Also:\n    `argmax`, `at_min`.\n\nArgs:\n    x: `Tensor`\n    dim: Dimensions along which the minimum should be determined. These are reduced in the operation.\n    index_dim: Dimension listing the index components for multidimensional argmin.\n\nReturns:\n    Index tensor `idx`, such that `x[idx] = min(x)`.",
                    "signature": "(x: phiml.math._tensors.Tensor, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], index_dim=(index\u1d9c=None))"
                },
                {
                    "name": "as_extrapolation",
                    "qualname": "as_extrapolation",
                    "docstring": "Creates an `Extrapolation` from a descriptor object.\n\nArgs:\n    obj: Extrapolation specification, one of the following:\n\n        * `Extrapolation`\n        * Primitive name as `str`: periodic, zero, one, zero-gradient, symmetric, symmetric-gradient, antisymmetric, reflect, antireflect\n        * `dict` containing exactly the keys `'normal'` and `'tangential'`\n        * `dict` mapping spatial dimension names to extrapolations\n\nReturns:\n    `Extrapolation`",
                    "signature": "(obj) -> phiml.math.extrapolation.Extrapolation"
                },
                {
                    "name": "assert_close",
                    "qualname": "assert_close",
                    "docstring": "Checks that all given tensors have equal values within the specified tolerance.\nRaises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.\n\nDoes not check that the shapes match as long as they can be broadcast to a common shape.\n\nArgs:\n    values: Tensors or native tensors or numbers or sequences of numbers.\n    rel_tolerance: Relative tolerance.\n    abs_tolerance: Absolute tolerance.\n    msg: Optional error message.\n    verbose: Whether to print conflicting values.\n    equal_nan: If `False`, `NaN` values will always trigger an assertion error.",
                    "signature": "(*values, rel_tolerance: float = 1e-05, abs_tolerance: float = 0, msg: str = '', verbose: bool = True, equal_nan=True)"
                },
                {
                    "name": "at_max",
                    "qualname": "at_max",
                    "docstring": "Looks up the values of `value` at the positions where the maximum values in `key` are located along `dim`.\n\nSee Also:\n    `at_min`, `phiml.math.max`.\n\nArgs:\n    value: Tensors or trees from which to lookup and return values. These tensors are indexed at the maximum index in `key\u00b4.\n        You can pass `range` (the type) to retrieve the picked indices.\n    key: `Tensor` containing at least one dimension of `dim`. The maximum index of `key` is determined.\n    dim: Dimensions along which to compute the maximum of `key`.\n\nReturns:\n    The values of `other_tensors` at the positions where the maximum values in `value` are located along `dim`.",
                    "signature": "(value, key: phiml.math._tensors.Tensor, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>)"
                },
                {
                    "name": "at_max_neighbor",
                    "qualname": "at_max_neighbor",
                    "docstring": "Computes the min of neighboring values in `key_grid` along each dimension in `dims` and retrieves the corresponding values from `values`.\n\nArgs:\n    values: Values to look up and return. `Tensor` or tree structure.\n    key_grid: Values to compare.\n    dims: Dimensions along which neighbors should be averaged.\n    padding: Padding at the upper edges of `grid` along `dims'. If not `None`, the result tensor will have the same shape as `grid`.\n    offsets: Relative neighbor indices as `int`. `0` refers to self, negative values to earlier (left) neighbors and positive values to later (right) neighbors.\n    diagonal: If `True`, performs sequential reductions along each axis, determining the minimum value along each axis independently.\n        If the values of `key_grid` depend on `values` or their position in the grid, this can lead to undesired behavior.\n\nReturns:\n    Tree or `Tensor` like values.",
                    "signature": "(values, key_grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, offsets=(0, 1), diagonal=True) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "at_min",
                    "qualname": "at_min",
                    "docstring": "Looks up the values of `value` at the positions where the minimum values in `key` are located along `dim`.\n\nSee Also:\n    `at_max`, `phiml.math.min`.\n\nArgs:\n    value: Tensors or trees from which to lookup and return values. These tensors are indexed at the minimum index in `key\u00b4.\n        You can pass `range` (the type) to retrieve the picked indices.\n    key: `Tensor` containing at least one dimension of `dim`. The minimum index of `key` is determined.\n    dim: Dimensions along which to compute the minimum of `key`.\n\nReturns:\n    The values of `other_tensors` at the positions where the minimum values in `value` are located along `dim`.",
                    "signature": "(value, key: phiml.math._tensors.Tensor, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>)"
                },
                {
                    "name": "at_min_neighbor",
                    "qualname": "at_min_neighbor",
                    "docstring": "Computes the max of neighboring values in `key_grid` along each dimension in `dims` and retrieves the corresponding values from `values`.\n\nArgs:\n    values: Values to look up and return.\n    key_grid: Values to compare.\n    dims: Dimensions along which neighbors should be averaged.\n    padding: Padding at the upper edges of `grid` along `dims'. If not `None`, the result tensor will have the same shape as `grid`.\n    offsets: Relative neighbor indices as `int`. `0` refers to self, negative values to earlier (left) neighbors and positive values to later (right) neighbors.\n    diagonal: If `True`, performs sequential reductions along each axis, determining the minimum value along each axis independently.\n        If the values of `key_grid` depend on `values` or their position in the grid, this can lead to undesired behavior.\n\nReturns:\n    Tree or `Tensor` like values.",
                    "signature": "(values, key_grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, offsets=(0, 1), diagonal=True) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "b2i",
                    "qualname": "b2i",
                    "docstring": "Change the type of all *batch* dims of `value` to *instance* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "batch",
                    "qualname": "batch",
                    "docstring": "Returns the batch dimensions of an existing `Shape` or creates a new `Shape` with only batch dimensions.\n\nUsage for filtering batch dimensions:\n>>> batch_dims = batch(shape)\n>>> batch_dims = batch(tensor)\n\nUsage for creating a `Shape` with only batch dimensions:\n>>> batch_shape = batch('undef', batch=2)\n(batch=2, undef=None)\n\nHere, the dimension `undef` is created with an undefined size of `None`.\nUndefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.\n\nTo create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.\n\nSee Also:\n    `channel`, `spatial`, `instance`\n\nArgs:\n    *args: Either\n\n        * `Shape` or `Tensor` to filter or\n        * Names of dimensions with undefined sizes as `str`.\n\n    **dims: Dimension sizes and names. Must be empty when used as a filter operation.\n\nReturns:\n    `Shape` containing only dimensions of type batch.",
                    "signature": "(*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('Tensor')]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "boolean_mask",
                    "qualname": "boolean_mask",
                    "docstring": "Discards values `x.dim[i]` where `mask.dim[i]=False`.\nAll dimensions of `mask` that are not `dim` are treated as batch dimensions.\n\nAlternative syntax: `x.dim[mask]`.\n\nImplementations:\n\n* NumPy: Slicing\n* PyTorch: [`masked_select`](https://pytorch.org/docs/stable/generated/torch.masked_select.html)\n* TensorFlow: [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask)\n* Jax: Slicing\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.Sliceable`.\n    dim: Dimension of `x` to along which to discard slices.\n    mask: Boolean `Tensor` marking which values to keep. Must have the dimension `dim` matching `x\u00b4.\n    preserve_names: This only supports uniform 1D slicing. Batched slicing will remove labels if incompatible.\n\nReturns:\n    Selected values of `x` as `Tensor` with dimensions from `x` and `mask`.",
                    "signature": "(x, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], mask: phiml.math._tensors.Tensor, preserve_names=False)"
                },
                {
                    "name": "brange",
                    "qualname": "brange",
                    "docstring": "Construct a range `Tensor` along one batch dim.",
                    "signature": "(start: int = 0, **stop: int) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "broadcast",
                    "qualname": "broadcast",
                    "docstring": "Function decorator for non-vectorized functions.\nWhen passing `Tensor` arguments to a broadcast function, the function is called once for each slice of the tensor.\nHow tensors are sliced is determined by `dims`.\nDecorating a function with `broadcast` is equal to passing the function to `phi.math.map()`.\n\nSee Also:\n    `phiml.math.map`\n\nArgs:\n    function: Function to broadcast.\n    dims: Dimensions which should be sliced.\n        `function` is called once for each element in `dims`, i.e. `dims.volume` times.\n        If `dims` is not specified, all dimensions from the `phiml.math.magic.Sliceable` values in `args` and `kwargs` will be mapped.\n    range: Optional range function. Can be used to generate `tqdm` output by passing `trange`.\n    unwrap_scalars: If `True`, passes the contents of scalar `Tensor`s instead of the tensor objects.\n    simplify: If `True`, reduces constant dims of output tensors that don't vary across broadcast slices.\n    name: Name to pass to `phiml.math.map()`. This may be displayed using `tqdm`. If `True`, uses the function name.\n\nReturns:\n    Broadcast function",
                    "signature": "(function=None, dims=<function shape at 0x0000024F475F3380>, range=<class 'range'>, unwrap_scalars=True, simplify=False, name: Union[str, bool] = True)"
                },
                {
                    "name": "c2b",
                    "qualname": "c2b",
                    "docstring": "Change the type of all *channel* dims of `value` to *batch* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "c2d",
                    "qualname": "c2d",
                    "docstring": "Change the type of all *channel* dims of `value` to *dual* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "cast",
                    "qualname": "cast",
                    "docstring": "Casts `x` to a different data type.\n\nImplementations:\n\n* NumPy: [`x.astype()`](numpy.ndarray.astype)\n* PyTorch: [`x.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to)\n* TensorFlow: [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast)\n* Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)\n\nSee Also:\n    `to_float`, `to_int32`, `to_int64`, `to_complex`.\n\nArgs:\n    x: `Tensor`\n    dtype: New data type as `phiml.math.DType`, e.g. `DType(int, 16)`.\n\nReturns:\n    `Tensor` with data type `dtype`",
                    "signature": "(x: ~MagicType, dtype: Union[phiml.backend._dtype.DType, type]) -> ~OtherMagicType"
                },
                {
                    "name": "ceil",
                    "qualname": "ceil",
                    "docstring": "Computes *\u2308x\u2309* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "channel",
                    "qualname": "channel",
                    "docstring": "Returns the channel dimensions of an existing `Shape` or creates a new `Shape` with only channel dimensions.\n\nUsage for filtering channel dimensions:\n>>> channel_dims = channel(shape)\n>>> channel_dims = channel(tensor)\n\nUsage for creating a `Shape` with only channel dimensions:\n>>> channel_shape = channel('undef', vector=2)\n(vector=2, undef=None)\n\nHere, the dimension `undef` is created with an undefined size of `None`.\nUndefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.\n\nTo create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.\n\nSee Also:\n    `spatial`, `batch`, `instance`\n\nArgs:\n    *args: Either\n\n        * `Shape` or `Tensor` to filter or\n        * Names of dimensions with undefined sizes as `str`.\n\n    **dims: Dimension sizes and names. Must be empty when used as a filter operation.\n\nReturns:\n    `Shape` containing only dimensions of type channel.",
                    "signature": "(*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('Tensor')]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "choose_backend",
                    "qualname": "backend_for",
                    "docstring": "Chooses an appropriate backend based on the backends of `values`.\n\nArgs:\n    *values: Input tensors to some operation.\n\nReturns:\n    `Backend` that is compatible with all `values\u00b4.\n\nRaises:\n    `NoBackendFound`: If no backend exists that can handle all `values`.",
                    "signature": "(*values: phiml.math._tensors.Tensor) -> phiml.backend._backend.Backend"
                },
                {
                    "name": "clip",
                    "qualname": "clip",
                    "docstring": "Limits the values of the `Tensor` `x` to lie between `lower_limit` and `upper_limit` (inclusive).",
                    "signature": "(x: phiml.math._tensors.Tensor, lower_limit: Union[float, phiml.math._tensors.Tensor] = 0, upper_limit: Union[float, phiml.math._tensors.Tensor, phiml.math._shape.Shape] = 1)"
                },
                {
                    "name": "clip_length",
                    "qualname": "clip_length",
                    "docstring": "Clips the length of a vector to the interval `[min_len, max_len]` while keeping the direction.\nZero-vectors remain zero-vectors.\n\nArgs:\n    vec: `Tensor`\n    min_len: Lower clipping threshold.\n    max_len: Upper clipping threshold.\n    vec_dim: Dimensions to compute the length over. By default, all channel dimensions are used to compute the vector length.\n    eps: Minimum vector length. Use to avoid `inf` gradients for zero-length vectors.\n\nReturns:\n    `Tensor` with same shape as `vec`.",
                    "signature": "(vec: phiml.math._tensors.Tensor, min_len=0, max_len=1, vec_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, eps: Union[float, phiml.math._tensors.Tensor] = None)"
                },
                {
                    "name": "close",
                    "qualname": "close",
                    "docstring": "Checks whether all tensors have equal values within the specified tolerance.\n\nDoes not check that the shapes exactly match but if shapes are incompatible, returns `False`.\nUnlike with `always_close()`, all shapes must be compatible and tensors with different shapes are reshaped before comparing.\n\nSee Also:\n    `always_close()`.\n\nArgs:\n    *tensors: At least two  `Tensor` or tensor-like objects or `None`.\n        The shapes of all tensors must be compatible but not all tensors must have all dimensions.\n        If any argument is `None`, returns `True` only if all are `None`.\n    rel_tolerance: Relative tolerance\n    abs_tolerance: Absolute tolerance\n    equal_nan: If `True`, tensors are considered close if they are NaN in the same places.\n\nReturns:\n    `bool`, whether all given tensors are equal to the first tensor within the specified tolerance.",
                    "signature": "(*tensors, rel_tolerance: Union[float, phiml.math._tensors.Tensor] = 1e-05, abs_tolerance: Union[float, phiml.math._tensors.Tensor] = 0, equal_nan=False, reduce=<function shape at 0x0000024F475F3380>) -> bool"
                },
                {
                    "name": "closest_grid_values",
                    "qualname": "closest_grid_values",
                    "docstring": "Finds the neighboring grid points in all directions and returns their values.\nThe result will have 2^d values for each vector in coordinates in d dimensions.\n\nIf `coordinates` does not have a channel dimension with labels, the spatial dims of `grid` will be used.\n\nArgs:\n    grid: grid data. The grid is spanned by the spatial dimensions of the tensor\n    coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space\n    extrap: grid extrapolation\n    stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.\n    kwargs: Additional information for the extrapolation.\n\nReturns:\n    `Tensor` of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)",
                    "signature": "(grid: phiml.math._tensors.Tensor, coordinates: phiml.math._tensors.Tensor, extrap: 'e_.Extrapolation', stack_dim_prefix='closest_', **kwargs)"
                },
                {
                    "name": "concat",
                    "qualname": "concat",
                    "docstring": "Concatenates a sequence of `phiml.math.magic.Shapable` objects, e.g. `Tensor`, along one dimension.\nAll values must have the same spatial, instance and channel dims and their sizes must be equal, except for `dim`.\nBatch dims will be added as needed.\n\nArgs:\n    values: Tuple or list of `phiml.math.magic.Shapable`, such as `phiml.math.Tensor`\n    dim: Concatenation dimension, must be present in all `values`.\n        The size along `dim` is determined from `values` and can be set to undefined (`None`).\n        Alternatively, a `str` of the form `'t->name:t'` can be specified, where `t` is on of `b d i s c` denoting the dimension type.\n        This first packs all dims of the input into a new dim with given name and type, then concatenates the values along this dim.\n    expand_values: If `True`, will first add missing dims to all values, not just batch dimensions.\n        This allows tensors with different dims to be concatenated.\n        The resulting tensor will have all dims that are present in `values`.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Concatenated `Tensor`\n\nExamples:\n    >>> concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], 'b')\n    (b\u1d47=20) 0.500 \u00b1 0.500 (0e+00...1e+00)\n\n    >>> concat([vec(x=1, y=0), vec(z=2.)], 'vector')\n    (x=1.000, y=0.000, z=2.000) float64",
                    "signature": "(values: Sequence[~PhiTreeNodeType], dim: Union[str, phiml.math._shape.Shape], expand_values=False, **kwargs) -> ~PhiTreeNodeType"
                },
                {
                    "name": "concat_shapes",
                    "qualname": "concat_shapes",
                    "docstring": "Creates a `Shape` listing the dimensions of all `shapes` in the given order.\n\nSee Also:\n    `merge_shapes()`.\n\nArgs:\n    *shapes: Shapes to concatenate. No two shapes must contain a dimension with the same name.\n\nReturns:\n    Combined `Shape`.",
                    "signature": "(*shapes: Union[phiml.math._shape.Shape, Any]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "conjugate",
                    "qualname": "conjugate",
                    "docstring": "See Also:\n    `imag()`, `real()`.\n\nArgs:\n    x: Real or complex `Tensor` or `phiml.math.magic.PhiTreeNode` or native tensor.\n\nReturns:\n    Complex conjugate of `x` if `x` is complex, else `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "const_vec",
                    "qualname": "const_vec",
                    "docstring": "Creates a single-dimension tensor with all values equal to `value`.\n`value` is not converted to the default backend, even when it is a Python primitive.\n\nArgs:\n    value: Value for filling the vector.\n    dim: Either single-dimension non-spatial Shape or `Shape` consisting of any number of spatial dimensions.\n        In the latter case, a new channel dimension named `'vector'` will be created from the spatial shape.\n\nReturns:\n    `Tensor`",
                    "signature": "(value: Union[float, phiml.math._tensors.Tensor], dim: Union[phiml.math._shape.Shape, tuple, list, str])"
                },
                {
                    "name": "contains",
                    "qualname": "contains",
                    "docstring": "For each query item, checks whether it is contained in `values`.\n\nSee Also:\n    `count_occurrences()`.\n\nArgs:\n    values: Data `Tensor` containing all `feature_dims`.\n        All non-batch and dims not specified as `feature_dims` are flattened.\n    query: Items to count the occurrences of. Must contain all `feature_dims`.\n    feature_dims: One item is considered to be the set of all values along `feature_dims`.\n        The number of items in a tensor is given by all dims except `feature_dims`.\n\nReturns:\n    Integer `Tensor` matching `query` without `feature_dims`.",
                    "signature": "(values: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor, feature_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "convert",
                    "qualname": "convert",
                    "docstring": "Convert the native representation of a `Tensor` or `phiml.math.magic.PhiTreeNode` to the native format of `backend`.\n\n*Warning*: This operation breaks the automatic differentiation chain.\n\nSee Also:\n    `phiml.math.backend.convert()`.\n\nArgs:\n    x: `Tensor` to convert. If `x` is a `phiml.math.magic.PhiTreeNode`, its variable attributes are converted.\n    backend: Target backend. If `None`, uses the current default backend, see `phiml.math.backend.backend()`.\n\nReturns:\n    `Tensor` with native representation belonging to `backend`.",
                    "signature": "(x, backend: phiml.backend._backend.Backend = None, use_dlpack=True)"
                },
                {
                    "name": "convolve",
                    "qualname": "convolve",
                    "docstring": "Computes the convolution of `value` and `kernel` along the specified dims.\n\nDual dims of `kernel` are reduced against the corresponding primal dims of `value`.\nAll other primal dims of `value` are treated as batch.\n\nArgs:\n    value: `Tensor` whose shape includes all spatial dimensions of `kernel`.\n    kernel: `Tensor` used as convolutional filter.\n    dims: Which dimensions to convolve over. Defaults to all spatial dims.\n    extrapolation: If `None`, convolve only where `kernel` fits into `value`, i.e. 'valid'. Otherwise, pads `value` with the specified extrapolation. The amount of padding depends on `full`.\n    strides: Convolution strides for applying `kernel` to a subset of `value` only. This will result in a smaller output. The stride can be specified per dim, with missing dims defaulting to `1`.\n    full: If `True`, the output contains all values of the convolution, including those where the kernel extends beyond the input. If `False`, the output is the same size as the input.\n    transpose: If `True`, the kernel is transposed before convolution, and strides are replaced by up-sampling.\n\nReturns:\n    `Tensor` with all non-reduced dims of `value` and additional non-dual dims from `kernel`.",
                    "signature": "(value: phiml.math._tensors.Tensor, kernel: phiml.math._tensors.Tensor, size: Union[str, phiml.math._shape.Shape] = 'same', extrapolation: 'Union[e_.Extrapolation, float]' = 0, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, strides: Union[int, Dict[str, int]] = 1, transpose=False) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "copy",
                    "qualname": "copy",
                    "docstring": "Copies the data buffer and encapsulating `Tensor` object.\n\nArgs:\n    value: `Tensor` to be copied.\n\nReturns:\n    Copy of `value`.",
                    "signature": "(value: phiml.math._tensors.Tensor)"
                },
                {
                    "name": "copy_with",
                    "qualname": "replace",
                    "docstring": "Creates a copy of the given `phiml.math.magic.PhiTreeNode` with updated values as specified in `updates`.\n\nIf `obj` overrides `__with_attrs__`, the copy will be created via that specific implementation.\nOtherwise, the `copy` module and `setattr` will be used.\n\nArgs:\n    obj: `phiml.math.magic.PhiTreeNode`\n    **updates: Values to be replaced.\n\nReturns:\n    Copy of `obj` with updated values.",
                    "signature": "(obj: ~PhiTreeNodeType, **updates) -> ~PhiTreeNodeType"
                },
                {
                    "name": "cos",
                    "qualname": "cos",
                    "docstring": "Computes *cos(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "cosh",
                    "qualname": "cosh",
                    "docstring": "Computes *cosh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "count_intersections",
                    "qualname": "count_intersections",
                    "docstring": "Counts the number of elements that are part of each pair of lists.\n\nArgs:\n    values:\n    arg_dims: Dims enumerating the input lists.\n    list_dims: Dims listing the elements.\n    feature_dims: Vector dims of one element. Elements are equal if all values along `feature_dims` are equal.\n\nReturns:\n    `Tensor`.",
                    "signature": "(values: phiml.math._tensors.Tensor, arg_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], list_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function instance at 0x0000024F475F3600>, feature_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "count_occurrences",
                    "qualname": "count_occurrences",
                    "docstring": "For each query item, counts how often this value occurs in `values`.\n\nSee Also:\n    `contains()`.\n\nArgs:\n    values: Data `Tensor` containing all `feature_dims`.\n        All non-batch and dims not specified as `feature_dims` are flattened.\n    query: Items to count the occurrences of. Must contain all `feature_dims`.\n    feature_dims: One item is considered to be the set of all values along `feature_dims`.\n        The number of items in a tensor is given by all dims except `feature_dims`.\n\nReturns:\n    Integer `Tensor` matching `query` without `feature_dims`.",
                    "signature": "(values: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor, feature_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "cpack",
                    "qualname": "cpack",
                    "docstring": "Short for `pack_dims(..., dims=channel)",
                    "signature": "(value, packed_dim: Union[phiml.math._shape.Shape, str], pos: Optional[int] = None, **kwargs)"
                },
                {
                    "name": "crange",
                    "qualname": "crange",
                    "docstring": "Construct a range `Tensor` along one channel dim.",
                    "signature": "(start: int = 0, **stop: int) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "cross",
                    "qualname": "cross_product",
                    "docstring": "Computes the cross product of two vectors in 2D.\n\nArgs:\n    vec1: `Tensor` with a single channel dimension called `'vector'`\n    vec2: `Tensor` with a single channel dimension called `'vector'`\n\nReturns:\n    `Tensor`",
                    "signature": "(vec1: phiml.math._tensors.Tensor, vec2: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "cross_product",
                    "qualname": "cross_product",
                    "docstring": "Computes the cross product of two vectors in 2D.\n\nArgs:\n    vec1: `Tensor` with a single channel dimension called `'vector'`\n    vec2: `Tensor` with a single channel dimension called `'vector'`\n\nReturns:\n    `Tensor`",
                    "signature": "(vec1: phiml.math._tensors.Tensor, vec2: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "csize",
                    "qualname": "csize",
                    "docstring": "Returns the total number of elements listed along channel dims of an object, equal to the product of the sizes of all channel dims.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    Size as `int`. If `obj` is an undefined `Shape`, returns `None`.",
                    "signature": "(obj) -> Optional[int]"
                },
                {
                    "name": "cumulative_sum",
                    "qualname": "cumulative_sum",
                    "docstring": "Performs a cumulative sum of `x` along `dim`.\n\nImplementations:\n\n* NumPy: [`cumsum`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)\n* PyTorch: [`cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html)\n* TensorFlow: [`cumsum`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)\n* Jax: [`cumsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)\n\nArgs:\n    x: `Tensor`\n    dim: Dimension along which to sum, as `str` or `Shape`. If multiple dims are passed, `x` the cumulative sum will be computed on the flattened array.\n    include_0: If `True`, adds a 0 to the result before the first value.\n    include_sum: If `False`, the total sum will be sliced off the result.\n    index_dim: If given, adds an index dimension for `dim`.\n\nReturns:\n    `Tensor` with the same shape as `x`.",
                    "signature": "(x: phiml.math._tensors.Tensor, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], include_0=False, include_sum=True, index_dim: Union[str, phiml.math._shape.Shape, NoneType] = None)"
                },
                {
                    "name": "custom_gradient",
                    "qualname": "custom_gradient",
                    "docstring": "Creates a function based on `f` that uses a custom gradient for the backpropagation pass.\n\n*Warning* This method can lead to memory leaks if the gradient function is not called.\nMake sure to pass tensors without gradients if the gradient is not required, see `stop_gradient()`.\n\nArgs:\n    f: Forward function mapping `Tensor` arguments `x` to a single `Tensor` output or sequence of tensors `y`.\n    gradient: Function to compute the vector-Jacobian product for backpropagation.\n        Will be called as `gradient(input_dict, *y, *dy) -> output_dict` where `input_dict` contains all named arguments passed to the forward function\n        and `output_dict` contains only those parameters for which a gradient is defined.\n    auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.\n\nReturns:\n    Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.",
                    "signature": "(f: Callable, gradient: Callable, auxiliary_args: str = '')"
                },
                {
                    "name": "d2i",
                    "qualname": "d2i",
                    "docstring": "Change the type of all *dual* dims of `value` to *instance* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "d2s",
                    "qualname": "d2s",
                    "docstring": "Change the type of all *dual* dims of `value` to *spatial* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "degrees_to_radians",
                    "qualname": "degrees_to_radians",
                    "docstring": "Convert degrees to radians.",
                    "signature": "(deg: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "dense",
                    "qualname": "dense",
                    "docstring": "Convert a sparse tensor representation to an equivalent dense one in which all values are explicitly stored contiguously in memory.\n\nArgs:\n    x: Any `Tensor`.\n        Python primitives like `float`, `int` or `bool` will be converted to `Tensors` in the process.\n\nReturns:\n    Dense tensor.",
                    "signature": "(x: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "dim_mask",
                    "qualname": "dim_mask",
                    "docstring": "Creates a masked vector with 1 elements for `dims` and 0 for all other dimensions in `all_dims`.\n\nArgs:\n    all_dims: All dimensions for which the vector should have an entry.\n    dims: Dimensions marked as 1.\n    mask_dim: Dimension of the masked vector. Item names are assigned automatically.\n\nReturns:\n    `Tensor`",
                    "signature": "(all_dims: Union[phiml.math._shape.Shape, tuple, list], dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], mask_dim=(vector\u1d9c=None)) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "dot",
                    "qualname": "dot",
                    "docstring": "Computes the dot product along the specified dimensions.\nContracts `x_dims` with `y_dims` by first multiplying the elements and then summing them up.\n\nFor one dimension, this is equal to matrix-matrix or matrix-vector multiplication.\n\nThe function replaces the traditional `dot` / `tensordot` / `matmul` / `einsum` functions.\n\n* NumPy: [`numpy.tensordot`](https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html), [`numpy.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)\n* PyTorch: [`torch.tensordot`](https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot), [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)\n* TensorFlow: [`tf.tensordot`](https://www.tensorflow.org/api_docs/python/tf/tensordot), [`tf.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum)\n* Jax: [`jax.numpy.tensordot`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html), [`jax.numpy.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)\n\nArgs:\n    x: First `Tensor`\n    x_dims: Dimensions of `x` to reduce against `y`\n    y: Second `Tensor`\n    y_dims: Dimensions of `y` to reduce against `x`.\n\nReturns:\n    Dot product as `Tensor`.",
                    "signature": "(x: phiml.math._tensors.Tensor, x_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], y: phiml.math._tensors.Tensor, y_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType]) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "downsample2x",
                    "qualname": "downsample2x",
                    "docstring": "Resamples a regular grid to half the number of spatial sample points per dimension.\nThe grid values at the new points are determined via mean (linear interpolation).\n\nArgs:\n  grid: full size grid\n  padding: grid extrapolation. Used to insert an additional value for odd spatial dims\n  dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.\n  grid: Tensor: \n  padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)\n  dims: tuple or None:  (Default value = None)\n\nReturns:\n  half-size grid",
                    "signature": "(grid: phiml.math._tensors.Tensor, padding: phiml.math.extrapolation.Extrapolation = zero-gradient, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "dpack",
                    "qualname": "dpack",
                    "docstring": "Short for `pack_dims(..., dims=dual)",
                    "signature": "(value, packed_dim: Union[phiml.math._shape.Shape, str], pos: Optional[int] = None, **kwargs)"
                },
                {
                    "name": "drange",
                    "qualname": "drange",
                    "docstring": "Construct a range `Tensor` along one dual dim.",
                    "signature": "(start: int = 0, **stop: int) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "dsize",
                    "qualname": "dsize",
                    "docstring": "Returns the total number of elements listed along dual dims of an object, equal to the product of the sizes of all dual dims.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    Size as `int`. If `obj` is an undefined `Shape`, returns `None`.",
                    "signature": "(obj) -> Optional[int]"
                },
                {
                    "name": "dtype",
                    "qualname": "dtype",
                    "docstring": "Returns the data type of `x`.\n\nArgs:\n    x: `Tensor` or native tensor.\n\nReturns:\n    `DType`",
                    "signature": "(x) -> phiml.backend._dtype.DType"
                },
                {
                    "name": "dual",
                    "qualname": "dual",
                    "docstring": "Returns the dual dimensions of an existing `Shape` or creates a new `Shape` with only dual dimensions.\n\nDual dimensions are assigned the prefix `~` to distinguish them from regular dimensions.\nThis way, a regular and dual dimension of the same name can exist in one `Shape`.\n\nDual dimensions represent the input space and are typically only present on matrices or higher-order matrices.\nDual dimensions behave like batch dimensions in regular operations, if supported.\nDuring matrix multiplication, they are matched against their regular counterparts by name (ignoring the `~` prefix).\n\nUsage for filtering dual dimensions:\n\n>>> dual_dims = dual(shape)\n>>> dual_dims = dual(tensor)\n\nUsage for creating a `Shape` with only dual dimensions:\n\n>>> dual('undef', points=2)\n(~undef\u1d48=None, ~points\u1d48=2)\n\nHere, the dimension `undef` is created with an undefined size of `None`.\nUndefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.\n\nTo create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.\n\nSee Also:\n    `channel`, `batch`, `spatial`\n\nArgs:\n    *args: Either\n\n        * `Shape` or `Tensor` to filter or\n        * Names of dimensions with undefined sizes as `str`.\n\n    **dims: Dimension sizes and names. Must be empty when used as a filter operation.\n\nReturns:\n    `Shape` containing only dimensions of type dual.",
                    "signature": "(*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('Tensor')]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "eigenvalues",
                    "qualname": "eigenvalues",
                    "docstring": "Computes the eigenvalues of a square matrix.\nThe matrix columns are listed along dual dimensions and the rows are listed along the corresponding non-dual dimensions.\nRow dims are matched by name if possible, else all primal dims are used.\n\nArgs:\n    matrix: Square matrix. Must have at least one dual dim and corresponding non-dual dim.\n    eigen_dim: Dimension along which eigenvalues should be listed.\n\nReturns:\n    `Tensor` listing the eigenvalues along `eigen_dim`.",
                    "signature": "(matrix: phiml.math._tensors.Tensor, eigen_dim=(eigenvalues\u1d9c=None))"
                },
                {
                    "name": "enable_debug_checks",
                    "qualname": "enable_debug_checks",
                    "docstring": "Once called, additional type checks are enabled.\nThis may result in a noticeable drop in performance.",
                    "signature": "()"
                },
                {
                    "name": "equal",
                    "qualname": "equal",
                    "docstring": "Checks whether all objects are equal.\n\nSee Also:\n    `close()`, `always_close()`.\n\nArgs:\n    *objects: Objects to compare. Can be tensors or other objects or `None`\n    equal_nan: If all objects are tensor-like, whether to count `NaN` values as equal.\n\nReturns:\n    `bool`, whether all given objects are equal to the first one.",
                    "signature": "(*objects, equal_nan=False) -> bool"
                },
                {
                    "name": "erf",
                    "qualname": "erf",
                    "docstring": "Computes the error function *erf(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "exp",
                    "qualname": "exp",
                    "docstring": "Computes *exp(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "expand",
                    "qualname": "expand",
                    "docstring": "Adds dims to a `Tensor` or tensor-like object by implicitly repeating the tensor values along the new dimensions.\nIf `value` already contains any of the new dimensions, a size and type check is performed for these instead.\n\nIf any of `dims` varies along a dimension that is present neither in `value` nor on `dims`, it will also be added to `value`.\n\nThis function replaces the usual `tile` / `repeat` functions of\n[NumPy](https://numpy.org/doc/stable/reference/generated/numpy.tile.html),\n[PyTorch](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat),\n[TensorFlow](https://www.tensorflow.org/api_docs/python/tf/tile) and\n[Jax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html).\n\nAdditionally, it replaces the traditional `unsqueeze` / `expand_dims` functions.\n\nArgs:\n    value: `phiml.math.magic.Shapable`, such as `phiml.math.Tensor`\n        For tree nodes, expands all value attributes by `dims` or the first variable attribute if no value attributes are set.\n    *dims: Dimensions to be added as `Shape`\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.",
                    "signature": "(value, *dims: Union[phiml.math._shape.Shape, str], **kwargs)"
                },
                {
                    "name": "factor_ilu",
                    "qualname": "factor_ilu",
                    "docstring": "Incomplete LU factorization for dense or sparse matrices.\n\nFor sparse matrices, keeps the sparsity pattern of `matrix`.\nL and U will be trimmed to the respective areas, i.e. stored upper elements in L will be dropped,\n unless this would lead to varying numbers of stored elements along a batch dimension.\n\nArgs:\n    matrix: Dense or sparse matrix to factor.\n        Currently, compressed sparse matrices are decompressed before running the ILU algorithm.\n    iterations: (Optional) Number of fixed-point iterations to perform.\n        If not given, will be automatically determined from matrix size and sparsity.\n    safe: If `False` (default), only matrices with a rank deficiency of up to 1 can be factored as all values of L and U are uniquely determined.\n        For matrices with higher rank deficiencies, the result includes `NaN` values.\n        If `True`, the algorithm runs slightly slower but can factor highly rank-deficient matrices as well.\n        However, then L is undeterdetermined and unused values of L are set to 0.\n        Rank deficiencies of 1 occur frequently in periodic settings but higher ones are rare.\n\nReturns:\n    L: Lower-triangular matrix as `Tensor` with all diagonal elements equal to 1.\n    U: Upper-triangular matrix as `Tensor`.\n\nExamples:\n    >>> matrix = wrap([[-2, 1, 0],\n    >>>                [1, -2, 1],\n    >>>                [0, 1, -2]], channel('row'), dual('col'))\n    >>> L, U = math.factor_ilu(matrix)\n    >>> math.print(L)\n    row=0      1.          0.          0.         along ~col\n    row=1     -0.5         1.          0.         along ~col\n    row=2      0.         -0.6666667   1.         along ~col\n    >>> math.print(L @ U, \"L @ U\")\n                L @ U\n    row=0     -2.   1.   0.  along ~col\n    row=1      1.  -2.   1.  along ~col\n    row=2      0.   1.  -2.  along ~col",
                    "signature": "(matrix: phiml.math._tensors.Tensor, iterations: int, safe=False)"
                },
                {
                    "name": "factorial",
                    "qualname": "factorial",
                    "docstring": "Computes *factorial(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.\nFor floating-point numbers computes the continuous factorial using the gamma function.\nFor integer numbers computes the exact factorial and returns the same integer type.\nHowever, this results in integer overflow for inputs larger than 12 (int32) or 19 (int64).",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "fft",
                    "qualname": "fft",
                    "docstring": "Performs a fast Fourier transform (FFT) on all spatial dimensions of x.\n\nThe inverse operation is `ifft()`.\n\nImplementations:\n\n* NumPy: [`np.fft.fft`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html),\n  [`numpy.fft.fft2`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html),\n  [`numpy.fft.fftn`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html)\n* PyTorch: [`torch.fft.fft`](https://pytorch.org/docs/stable/fft.html)\n* TensorFlow: [`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft),\n  [`tf.signal.fft2d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft2d),\n  [`tf.signal.fft3d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft3d)\n* Jax: [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html),\n  [`jax.numpy.fft.fft2`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html)\n  [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html)\n\nArgs:\n    x: Uniform complex or float `Tensor` with at least one spatial dimension.\n    dims: Dimensions along which to perform the FFT.\n        If `None`, performs the FFT along all spatial dimensions of `x`.\n\nReturns:\n    *\u0191(x)* as complex `Tensor`",
                    "signature": "(x: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "fftfreq",
                    "qualname": "fftfreq",
                    "docstring": "Returns the discrete Fourier transform sample frequencies.\nThese are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.\n\nArgs:\n    resolution: Grid resolution measured in cells\n    dx: Distance between sampling points in real space.\n    dtype: Data type of the returned tensor (Default value = None)\n\nReturns:\n    `Tensor` holding the frequencies of the corresponding values computed by math.fft",
                    "signature": "(resolution: phiml.math._shape.Shape, dx: Union[phiml.math._tensors.Tensor, float] = 1, dtype: phiml.backend._dtype.DType = None)"
                },
                {
                    "name": "find_closest",
                    "qualname": "find_closest",
                    "docstring": "Finds the closest vector to `query` from `vectors`.\nThis is implemented using a k-d tree built from `vectors`.\n\n\nArgs:\n    vectors: Points to find.\n    query: (Optional) Target locations. If not specified, returns a function (query) -> index which caches the acceleration structure. Otherwise, returns the index tensor.\n    method: One of the following:\n\n        * `'dense'`: compute the pair-wise distances between all vectors and query points, then return the index of the smallest distance for each query point.\n        * `'kd'` (default): Build a k-d tree from `vectors` and use it to query all points in `query`. The tree will be cached if this call is jit-compiled and `vectors` is constant.\n    index_dim: Dimension along which components should be listed as `Shape`.\n        Pass `None` to get 1D indices as scalars.\n\nReturns:\n    Index tensor `idx` so that the closest points to `query` are `vectors[idx]`.",
                    "signature": "(vectors: phiml.math._tensors.Tensor, query: phiml.math._tensors.Tensor = None, /, method='kd', index_dim=(index\u1d9c=None))"
                },
                {
                    "name": "find_differences",
                    "qualname": "find_differences",
                    "docstring": "Compares `tree1` and `tree2` and returns all differences in the form `(difference_description: str, variable_identifier: str, value1, value2)`.\n\nArgs:\n    tree1: Nested tree or leaf\n    tree2: Nested tree or leaf\n    compare_tensors_by_id: Whether `phiml.math.Tensor` objects should be compared by identity or values.\n    attr_type: What attributes to compare, either `value_attributes` or `variable_attributes`.\n    tensor_equality: Function that compares two tensors for equality. `None` defaults to `equal`.\n\nReturns:\n    List of differences, each represented as a `tuple`.",
                    "signature": "(tree1, tree2, compare_tensors_by_id=False, attr_type=<function value_attributes at 0x0000024F476153A0>, tensor_equality=None) -> Sequence[Tuple[str, str, Any, Any]]"
                },
                {
                    "name": "finite_fill",
                    "qualname": "finite_fill",
                    "docstring": "Fills non-finite (NaN, inf, -inf) values from nearby finite values.\nExtrapolates the finite values of `values` for `distance` steps along `dims`.\nWhere multiple finite values could fill an invalid value, the average is computed.\n\nArgs:\n    values: Floating-point `Tensor`. All non-numeric values (`NaN`, `inf`, `-inf`) are interpreted as invalid.\n    dims: Dimensions along which to fill invalid values from finite ones.\n    distance: Number of extrapolation steps, each extrapolating one cell out.\n    diagonal: Whether to extrapolate values to their diagonal neighbors per step.\n    padding: Extrapolation of `values`. Determines whether to extrapolate from the edges as well.\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n    `Tensor` of same shape as `values`.",
                    "signature": "(values: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, distance: int = 1, diagonal: bool = True, padding=zero-gradient, padding_kwargs: dict = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "finite_max",
                    "qualname": "finite_max",
                    "docstring": "Finds the maximum along `dim` ignoring all non-finite values.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    default: Value to use where no finite value was encountered.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, default: Union[complex, float] = nan)"
                },
                {
                    "name": "finite_mean",
                    "qualname": "finite_mean",
                    "docstring": "Computes the mean value of all finite values in `value` along `dim`.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    default: Value to use where no finite value was encountered.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, default: Union[complex, float] = nan)"
                },
                {
                    "name": "finite_min",
                    "qualname": "finite_min",
                    "docstring": "Finds the minimum along `dim` ignoring all non-finite values.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    default: Value to use where no finite value was encountered.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, default: Union[complex, float] = nan)"
                },
                {
                    "name": "finite_sum",
                    "qualname": "finite_sum",
                    "docstring": "Sums all finite values in `value` along `dim`.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    default: Value to use where no finite value was encountered.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, default: Union[complex, float] = nan)"
                },
                {
                    "name": "flatten",
                    "qualname": "flatten",
                    "docstring": "Returns a `Tensor` with the same values as `value` but only a single dimension `flat_dim`.\nThe order of the values in memory is not changed.\n\nArgs:\n    value: `phiml.math.magic.Shapable`, such as `Tensor`.\n        If a non-`phiml.math.magic.Shaped` object or one with an empty `Shape` is passed, it is returned without alteration.\n    flat_dim: Dimension name and type as `Shape` object. The size is ignored.\n    flatten_batch: Whether to flatten batch dims as well.\n        If `False`, batch dims are kept, only onn-batch dims are flattened.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.\n\nExamples:\n    >>> flatten(math.zeros(spatial(x=4, y=3)))\n    (flat\u2071=12) const 0.0",
                    "signature": "(value, flat_dim: phiml.math._shape.Shape = (flat\u2071=None), flatten_batch=False, **kwargs)"
                },
                {
                    "name": "floor",
                    "qualname": "floor",
                    "docstring": "Computes *\u230ax\u230b* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "fourier_laplace",
                    "qualname": "fourier_laplace",
                    "docstring": "Applies the spatial laplace operator to the given tensor with periodic boundary conditions.\n\n*Note:* The results of `fourier_laplace` and `laplace` are close but not identical.\n\nThis implementation computes the laplace operator in Fourier space.\nThe result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.\n\nArgs:\n  grid: tensor, assumed to have periodic boundary conditions\n  dx: distance between grid points, tensor-like, scalar or vector\n  times: number of times the laplace operator is applied. The computational cost is independent of this parameter.\n  grid: Tensor:\n  dx: Tensor or Shape or float or list or tuple:\n  times: int:  (Default value = 1)\n\nReturns:\n  tensor of same shape as `tensor`",
                    "signature": "(grid: phiml.math._tensors.Tensor, dx: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape, float, list, tuple], times: int = 1)"
                },
                {
                    "name": "fourier_poisson",
                    "qualname": "fourier_poisson",
                    "docstring": "Inverse operation to `fourier_laplace`.\n\nArgs:\n  grid: Tensor: \n  dx: Tensor or Shape or float or list or tuple: \n  times: int:  (Default value = 1)\n\nReturns:",
                    "signature": "(grid: phiml.math._tensors.Tensor, dx: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape, float, list, tuple], times: int = 1)"
                },
                {
                    "name": "frequency_loss",
                    "qualname": "frequency_loss",
                    "docstring": "Penalizes the squared `values` in frequency (Fourier) space.\nLower frequencies are weighted more strongly then higher frequencies, depending on `frequency_falloff`.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` Values to penalize, typically `actual - target`.\n    frequency_falloff: Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.\n        *Note*: The total loss is not normalized. Varying the value will result in losses of different magnitudes.\n    threshold: Frequency amplitudes below this value are ignored.\n        Setting this to zero may cause infinities or NaN values during backpropagation.\n    ignore_mean: If `True`, does not penalize the mean value (frequency=0 component).\n\nReturns:\n  Scalar loss value",
                    "signature": "(x, frequency_falloff: float = 100, threshold=1e-05, ignore_mean=False, n=2) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "from_dict",
                    "qualname": "from_dict",
                    "docstring": "Loads a `Tensor` or `Shape` from a serialized form.\n\nSee Also:\n    `to_dict()`.\n\nArgs:\n    dict_: Serialized tensor properties.\n    convert: Whether to convert the data to the current backend format or keep it as a Numpy array.\n\nReturns:\n    `Tensor` or `Shape`.",
                    "signature": "(dict_: dict, convert=False)"
                },
                {
                    "name": "gather",
                    "qualname": "gather",
                    "docstring": "Gathers the entries of `values` at positions described by `indices`.\nAll non-channel dimensions of `indices` that are part of `values` but not indexed are treated as batch dimensions.\n\nSee Also:\n    `scatter()`.\n\nArgs:\n    values: `Tensor` or `phiml.math.matic.PhiTreeNode` containing values to gather.\n    indices: `int` `Tensor`. Multidimensional position references in `values`.\n        Must contain a single channel dimension for the index vector matching the number of dimensions to index.\n        This channel dimension should list the dimension names to index as labels unless explicitly specified as `dims`.\n    dims: (Optional) Dimensions indexed by `indices`.\n        Alternatively, the dimensions can be specified as the labels of the channel dimension of `indices`.\n        If `None` and no index labels are specified, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).\n    pref_index_dim: In case `indices` has multiple channel dims, use this dim as the index, treating the others as batch.\n        Has no effect if `indices` only has one channel dim.\n\nReturns:\n    `Tensor` with combined batch dimensions, channel dimensions of `values` and spatial/instance dimensions of `indices`.",
                    "signature": "(values, indices: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = None, pref_index_dim='index')"
                },
                {
                    "name": "get_format",
                    "qualname": "get_format",
                    "docstring": "Returns the sparse storage format of a tensor.\n\nArgs:\n    x: `Tensor`\n\nReturns:\n    One of `'coo'`, `'csr'`, `'csc'`, `'dense'`.",
                    "signature": "(x: phiml.math._tensors.Tensor) -> str"
                },
                {
                    "name": "get_precision",
                    "qualname": "get_precision",
                    "docstring": "Gets the current target floating point precision in bits.\nThe precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.\n\nAny Backend method may convert floating point values to this precision, even if the input had a different precision.\n\nReturns:\n    16 for half, 32 for single, 64 for double",
                    "signature": "() -> int"
                },
                {
                    "name": "get_sparsity",
                    "qualname": "get_sparsity",
                    "docstring": "Fraction of values currently stored on disk for the given `Tensor` `x`.\nFor sparse tensors, this is `nnz / shape`.\n\nThis is a lower limit on the number of values that will need to be processed for operations involving `x`.\nThe actual number is often higher since many operations require data be laid out in a certain format.\nIn these cases, missing values, such as zeros, are filled in before the operation.\n\nThe following operations may return tensors whose values are only partially stored:\n\n* `phiml.math.expand()`\n* `phiml.math.pairwise_distance()` with `max_distance` set.\n* Tracers used in `phiml.math.jit_compile_linear()`\n* Stacking any of the above.\n\nArgs:\n    x: `Tensor`\n\nReturns:\n    The number of values that are actually stored on disk.\n    This does not include additional information, such as position information / indices.\n    For sparse matrices, this is equal to the number of nonzero values.",
                    "signature": "(x: phiml.math._tensors.Tensor)"
                },
                {
                    "name": "gradient",
                    "qualname": "gradient",
                    "docstring": "Creates a function which computes the gradient of `f`.\n\nExample:\n```python\ndef loss_function(x, y):\n    prediction = f(x)\n    loss = math.l2_loss(prediction - y)\n    return loss, prediction\n\ndx = gradient(loss_function, 'x', get_output=False)(x, y)\n\n(loss, prediction), (dx, dy) = gradient(loss_function,\n                                        'x,y', get_output=True)(x, y)\n```\n\nFunctional gradients are implemented for the following backends:\n\n* PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)\n* TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n* Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)\n\nWhen the gradient function is invoked, `f` is called with tensors that track the gradient.\nFor PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.\n\nArgs:\n    f: Function to be differentiated.\n        `f` must return a floating point `Tensor` with rank zero.\n        It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.\n        All arguments for which the gradient is computed must be of dtype float or complex.\n    get_output: Whether the gradient function should also return the return values of `f`.\n    wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.\n        If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).\n\nReturns:\n    Function with the same arguments as `f` that returns the value of `f`, auxiliary data and gradient of `f` if `get_output=True`, else just the gradient of `f`.",
                    "signature": "(f: Callable, wrt: str = None, get_output=True) -> Callable"
                },
                {
                    "name": "grid_sample",
                    "qualname": "grid_sample",
                    "docstring": "Samples values of `grid` at the locations referenced by `coordinates`.\nValues lying in between sample points are determined via linear interpolation.\n\nIf `coordinates` has a channel dimension, its labels are used to determine the grid dimensions of `grid`.\nOtherwise, the spatial dims of `grid` will be used.\n\nFor values outside the valid bounds of `grid` (`coord < 0 or coord > grid.shape - 1`), `extrap` is used to determine the neighboring grid values.\nIf the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.\nIn that case, values lying further outside will not be sampled according to the extrapolation.\n\nArgs:\n    grid: Grid with at least one spatial dimension and no instance dimensions.\n    coordinates: Coordinates with a single channel dimension called `'vector'`.\n        The size of the `vector` dimension must match the number of spatial dimensions of `grid`.\n    extrap: Extrapolation used to determine the values of `grid` outside its valid bounds.\n    kwargs: Additional information for the extrapolation.\n\nReturns:\n    `Tensor` with channel dimensions of `grid`, spatial and instance dimensions of `coordinates` and combined batch dimensions.",
                    "signature": "(grid: phiml.math._tensors.Tensor, coordinates: phiml.math._tensors.Tensor, extrap: Union[ForwardRef('e_.Extrapolation'), float, str], **kwargs)"
                },
                {
                    "name": "histogram",
                    "qualname": "histogram",
                    "docstring": "Compute a histogram of a distribution of values.\n\n*Important Note:* In its current implementation, values outside the range of bins may or may not be added to the outermost bins.\n\nArgs:\n    values: `Tensor` listing the values to be binned along spatial or instance dimensions.\n        `values\u00b4 may not contain channel or dual dimensions.\n    bins: Either `Shape` specifying the number of equally-spaced bins to use or bin edge positions as `Tensor` with a spatial or instance dimension.\n    weights: `Tensor` assigning a weight to every value in `values` that will be added to the bin, default 1.\n    same_bins: Only used if `bins` is given as a `Shape`.\n        Use the same bin sizes and positions across these batch dimensions.\n        By default, bins will be chosen independently for each example.\n\nReturns:\n    hist: `Tensor` containing all batch dimensions and the `bins` dimension with dtype matching `weights`.\n    bin_edges: `Tensor`\n    bin_center: `Tensor`",
                    "signature": "(values: phiml.math._tensors.Tensor, bins: phiml.math._shape.Shape = (bins\u02e2=30), weights=1, same_bins: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = None, eps=1e-05)"
                },
                {
                    "name": "i2b",
                    "qualname": "i2b",
                    "docstring": "Change the type of all *instance* dims of `value` to *batch* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "identity",
                    "qualname": "identity",
                    "docstring": "Identity function for one argument.\nVararg functions cannot be transformed as the argument names are unknown.\n\nArgs:\n    x: Positional argument.\n\nReturns:\n    `x`",
                    "signature": "(x)"
                },
                {
                    "name": "ifft",
                    "qualname": "ifft",
                    "docstring": "Inverse of `fft()`.\n\nArgs:\n    k: Complex or float `Tensor` with at least one spatial dimension.\n    dims: Dimensions along which to perform the inverse FFT.\n        If `None`, performs the inverse FFT along all spatial dimensions of `k`.\n\nReturns:\n    *\u0191<sup>-1</sup>(k)* as complex `Tensor`",
                    "signature": "(k: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>)"
                },
                {
                    "name": "imag",
                    "qualname": "imag",
                    "docstring": "Returns the imaginary part of `x`.\nIf `x` does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.\n\nSee Also:\n    `real()`, `conjugate()`.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` or native tensor.\n\nReturns:\n    Imaginary component of `x` if `x` is complex, zeros otherwise.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "incomplete_gamma",
                    "qualname": "incomplete_gamma",
                    "docstring": "Computes the incomplete gamma function.\n\nArgs:\n    a: Positive parameter, `Tensor` or tree.\n    x: Non-negative argument, `Tensor` or tree.\n    upper: Whether to complete the upper integral (x to infinity) or the lower integral (0 to x).\n    regularized: Whether the integral is divided by \u0393(a).",
                    "signature": "(a: ~TensorOrTree, x: ~TensorOrTree, upper=False, regularized=True) -> ~TensorOrTree"
                },
                {
                    "name": "index_shift",
                    "qualname": "index_shift",
                    "docstring": "Returns shifted versions of `x` according to `offsets` where each offset is an `int` vector indexing some dimensions of `x`.\n\nSee Also:\n    `shift`, `neighbor_reduce`.\n\nArgs:\n    x: Input grid-like `Tensor`.\n    offsets: Sequence of offset vectors. Each offset is an `int` vector indexing some dimensions of `x`.\n        Offsets can have different subsets of the dimensions of `x`. Missing dimensions count as 0.\n        The value `0` can also be passed as a zero-shift.\n    padding: Padding to be performed at the boundary so that the shifted versions have the same size as `x`.\n        Must be one of the following: `Extrapolation`, `Tensor` or number for constant extrapolation, name of extrapolation as `str`.\n        Can be set to `None` to disable padding. Then the result tensors will be smaller than `x`.\n\nReturns:\n    `list` of shifted tensors. The number of return tensors is equal to the number of `offsets`.",
                    "signature": "(x: phiml.math._tensors.Tensor, offsets: Sequence[Union[int, phiml.math._tensors.Tensor]], padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None) -> List[phiml.math._tensors.Tensor]"
                },
                {
                    "name": "instance",
                    "qualname": "instance",
                    "docstring": "Returns the instance dimensions of an existing `Shape` or creates a new `Shape` with only instance dimensions.\n\nUsage for filtering instance dimensions:\n>>> instance_dims = instance(shape)\n>>> instance_dims = instance(tensor)\n\nUsage for creating a `Shape` with only instance dimensions:\n>>> instance_shape = instance('undef', points=2)\n(points=2, undef=None)\n\nHere, the dimension `undef` is created with an undefined size of `None`.\nUndefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.\n\nTo create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.\n\nSee Also:\n    `channel`, `batch`, `spatial`\n\nArgs:\n    *args: Either\n\n        * `Shape` or `Tensor` to filter or\n        * Names of dimensions with undefined sizes as `str`.\n\n    **dims: Dimension sizes and names. Must be empty when used as a filter operation.\n\nReturns:\n    `Shape` containing only dimensions of type instance.",
                    "signature": "(*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('Tensor')]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "ipack",
                    "qualname": "ipack",
                    "docstring": "Short for `pack_dims(..., dims=instance)",
                    "signature": "(value, packed_dim: Union[phiml.math._shape.Shape, str], pos: Optional[int] = None, **kwargs)"
                },
                {
                    "name": "irange",
                    "qualname": "irange",
                    "docstring": "Construct a range `Tensor` along one instance dim.",
                    "signature": "(start: int = 0, **stop: int) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "is_composite",
                    "qualname": "is_composite",
                    "docstring": "Args:\n    x: Object to check.\n\nReturns:\n    `True` if `x` is a composite type / container, e.g. a dataclass or pytree.\n    Sparse tensors are treated as non-composite.",
                    "signature": "(x: Any) -> bool"
                },
                {
                    "name": "is_finite",
                    "qualname": "is_finite",
                    "docstring": "Returns a `Tensor` or `phiml.math.magic.PhiTreeNode` matching `x` with values `True` where `x` has a finite value and `False` otherwise.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "is_inf",
                    "qualname": "is_inf",
                    "docstring": "Returns a `Tensor` or `phiml.math.magic.PhiTreeNode` matching `x` with values `True` where `x` is `+inf` or `-inf` and `False` otherwise.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "is_nan",
                    "qualname": "is_nan",
                    "docstring": "Returns a `Tensor` or `phiml.math.magic.PhiTreeNode` matching `x` with values `True` where `x` is `NaN` and `False` otherwise.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "is_none",
                    "qualname": "is_none",
                    "docstring": "Returns `True` if `x is None or x == wrap(None)`.",
                    "signature": "(x: Optional[phiml.math._tensors.Tensor]) -> bool"
                },
                {
                    "name": "is_numeric",
                    "qualname": "is_numeric",
                    "docstring": "Args:\n    x: Object to test.\n\nReturns:\n    `True` if `x` is a primitive number, native number tensor or numeric `Tensor`.",
                    "signature": "(x: Any) -> bool"
                },
                {
                    "name": "is_scalar",
                    "qualname": "is_scalar",
                    "docstring": "Checks whether `value` has no dimensions.\n\nArgs:\n    value: `Tensor` or Python primitive or native tensor.\n\nReturns:\n    `bool`",
                    "signature": "(value) -> bool"
                },
                {
                    "name": "is_sparse",
                    "qualname": "is_sparse",
                    "docstring": "Checks whether a tensor is represented in COO, CSR or CSC format.\nIf the tensor is neither sparse nor dense, this function raises an error.\n\nArgs:\n    x: `Tensor` to test.\n\nReturns:\n    `True` if `x` is sparse, `False` if `x` is dense.\n\nRaises:\n    `AssertionError` if `x` is neither sparse nor fully dense.",
                    "signature": "(x: phiml.math._tensors.Tensor)"
                },
                {
                    "name": "isize",
                    "qualname": "isize",
                    "docstring": "Returns the total number of elements listed along instance dims of an object, equal to the product of the sizes of all instance dims.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    Size as `int`. If `obj` is an undefined `Shape`, returns `None`.",
                    "signature": "(obj) -> Optional[int]"
                },
                {
                    "name": "iterate",
                    "qualname": "iterate",
                    "docstring": "Repeatedly call `function`, passing the previous output as the next input.\n\nIf the function outputs more values than the number of arguments in `x0`, only the first `len(x0)` ones are passed to `map_function`.\nHowever, all outputs will be returned by `iterate`.\n\nArgs:\n    map_function: Function to call. Must be callable as `f(x0, **f_kwargs)` and `f(f(x0, **f_kwargs), **f_kwargs)`.\n    iterations: Number of iterations as `int` or single-dimension `Shape`.\n        If `int`, returns the final output of `map_function`.\n        If `Shape`, returns the trajectory (`x0` and all outputs of `map_function`), stacking the values along this dimension.\n    x0: Initial positional arguments for `map_function`.\n        Values that are initially `None` are not stacked with the other values if `iterations` is a `Shape`.\n    range: Range function. Can be used to generate tqdm output by passing `trange`.\n    measure: Function without arguments to call at the start and end (and in between if `isinstance(iterations, Shape)`) calls to `map_function`.\n        The measure of each call to `map_function` is `measure()` after minus `measure()` before the call.\n    substeps: If > 1, iterates the function multiple times for each recorded step.\n        The returned trajectories as well as measurements only record the large steps, not the sub-steps.\n        The `range` is also only used on large steps, not sub-steps.\n    f_kwargs: Additional keyword arguments to be passed to `map_function`.\n        These arguments can be of any type.\n    f_kwargs_: More keyword arguments.\n\nReturns:\n    final_or_trajectory: Stacked trajectory or final output of `map_function`, depending on `iterations`.\n    measured: Only if `measure` was specified, returns the measured value or trajectory tensor.",
                    "signature": "(map_function: Callable, iterations: Union[int, phiml.math._shape.Shape], *x0, f_kwargs: dict = None, range: Callable = <class 'range'>, measure: Callable = None, substeps: int = 1, **f_kwargs_)"
                },
                {
                    "name": "jacobian",
                    "qualname": "jacobian",
                    "docstring": "Creates a function which computes the Jacobian matrix of `f`.\nFor scalar functions, consider using `gradient()` instead.\n\nExample:\n```python\ndef f(x, y):\n    prediction = f(x)\n    loss = math.l2_loss(prediction - y)\n    return loss, prediction\n\ndx = jacobian(loss_function, wrt='x', get_output=False)(x, y)\n\n(loss, prediction), (dx, dy) = jacobian(loss_function,\n                                    wrt='x,y', get_output=True)(x, y)\n```\n\nFunctional gradients are implemented for the following backends:\n\n* PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)\n* TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n* Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)\n\nWhen the gradient function is invoked, `f` is called with tensors that track the gradient.\nFor PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.\n\nArgs:\n    f: Function to be differentiated.\n        `f` must return a floating point `Tensor` with rank zero.\n        It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.\n        All arguments for which the gradient is computed must be of dtype float or complex.\n    get_output: Whether the gradient function should also return the return values of `f`.\n    wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.\n        If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).\n\nReturns:\n    Function with the same arguments as `f` that returns the value of `f`, auxiliary data and Jacobian of `f` if `get_output=True`, else just the Jacobian of `f`.",
                    "signature": "(f: Callable, wrt: str = None, get_output=True) -> Callable"
                },
                {
                    "name": "jit_compile",
                    "qualname": "jit_compile",
                    "docstring": "Compiles a graph based on the function `f`.\nThe graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.\n\nThe traced function will compute the same result as `f` but may run much faster.\nSome checks may be disabled in the compiled function.\n\nCan be used as a decorator:\n```python\n@math.jit_compile\ndef my_function(x: math.Tensor) -> math.Tensor:\n```\n\nInvoking the returned function may invoke re-tracing / re-compiling `f` after the first call if either\n\n* it is called with a different number of arguments,\n* the tensor arguments have different dimension names or types (the dimension order also counts),\n* any `Tensor` arguments require a different backend than previous invocations,\n* `phiml.math.magic.PhiTreeNode` positional arguments do not match in non-variable properties.\n\nCompilation is implemented for the following backends:\n\n* PyTorch: [`torch.jit.trace`](https://pytorch.org/docs/stable/jit.html)\n* TensorFlow: [`tf.function`](https://www.tensorflow.org/guide/function)\n* Jax: [`jax.jit`](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)\n\nJit-compilations cannot be nested, i.e. you cannot call `jit_compile()` while another function is being compiled.\nAn exception to this is `jit_compile_linear()` which can be called from within a jit-compiled function.\n\nSee Also:\n    `jit_compile_linear()`\n\nArgs:\n    f: Function to be traced.\n        All positional arguments must be of type `Tensor` or `phiml.math.magic.PhiTreeNode` returning a single `Tensor` or `phiml.math.magic.PhiTreeNode`.\n    auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.\n    forget_traces: If `True`, only remembers the most recent compiled instance of this function.\n        Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.\n\nReturns:\n    Function with similar signature and return values as `f`.",
                    "signature": "(f: Callable = None, auxiliary_args: str = '', forget_traces: bool = None) -> Callable"
                },
                {
                    "name": "jit_compile_linear",
                    "qualname": "jit_compile_linear",
                    "docstring": "Compile an optimized representation of the linear function `f`.\nFor backends that support sparse tensors, a sparse matrix will be constructed for `f`.\n\nCan be used as a decorator:\n```python\n@math.jit_compile_linear\ndef my_linear_function(x: math.Tensor) -> math.Tensor:\n```\n\nUnlike `jit_compile()`, `jit_compile_linear()` can be called during a regular jit compilation.\n\nSee Also:\n    `jit_compile()`\n\nArgs:\n    f: Function that is linear in its positional arguments.\n        All positional arguments must be of type `Tensor` and `f` must return a `Tensor`.\n    auxiliary_args: Which parameters `f` is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.\n    forget_traces: If `True`, only remembers the most recent compiled instance of this function.\n        Upon tracing with new instance (due to changed shapes or auxiliary args), deletes the previous traces.\n\nReturns:\n    `LinearFunction` with similar signature and return values as `f`.",
                    "signature": "(f: Callable[[~X], ~Y] = None, auxiliary_args: str = None, forget_traces: bool = None) -> 'LinearFunction[X, Y]'"
                },
                {
                    "name": "l1_loss",
                    "qualname": "l1_loss",
                    "docstring": "Computes *\u2211<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub>*, summing over all non-batch dimensions.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` or 0D or 1D native tensor.\n        For `phiml.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.\n    reduce: Dimensions to reduce as `DimFilter`.\n\nReturns:\n    loss: `Tensor`",
                    "signature": "(x, reduce: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "l2_loss",
                    "qualname": "l2_loss",
                    "docstring": "Computes *\u2211<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2*, summing over all non-batch dimensions.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` or 0D or 1D native tensor.\n        For `phiml.math.magic.PhiTreeNode` objects, only value the sum over all value attributes is computed.\n    reduce: Dimensions to reduce as `DimFilter`.\n\nReturns:\n    loss: `Tensor`",
                    "signature": "(x, reduce: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "laplace",
                    "qualname": "laplace",
                    "docstring": "Spatial Laplace operator as defined for scalar fields.\nIf a vector field is passed, the laplace is computed component-wise.\n\nArgs:\n    x: n-dimensional field of shape (batch, spacial dimensions..., components)\n    dx: scalar or 1d tensor\n    padding: Padding mode.\n        Must be one of the following: `Extrapolation`, `Tensor` or number for constant extrapolation, name of extrapolation as `str`.\n    dims: The second derivative along these dimensions is summed over\n    weights: (Optional) Multiply the axis terms by these factors before summation.\n        Must be a Tensor with a single channel dimension that lists all laplace dims by name.\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n    `phiml.math.Tensor` of same shape as `x`",
                    "signature": "(x: phiml.math._tensors.Tensor, dx: Union[phiml.math._tensors.Tensor, float] = 1, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = zero-gradient, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, weights: phiml.math._tensors.Tensor = None, padding_kwargs: dict = None)"
                },
                {
                    "name": "layout",
                    "qualname": "layout",
                    "docstring": "Wraps a Python tree in a `Tensor`, allowing elements to be accessed via dimensions.\nA python tree is a structure of nested `tuple`, `list`, `dict` and *leaf* objects where leaves can be any Python object.\n\nAll keys of `dict` containers must be of type `str`.\nThe keys are automatically assigned as labels along that dimension unless conflicting with other elements.\n\nStrings may also be used as containers.\n\nExample:\n>>> t = layout({'a': 'text', 'b': [0, 1]}, channel('dict,inner'))\n>>> t.inner[1].dict['a'].native()\n'e'\n\nSee Also:\n    `tensor()`, `wrap()`.\n\nArgs:\n    objects: PyTree of `list` or `tuple`.\n    *shape: Tensor dimensions\n\nReturns:\n    `Tensor`.\n    Calling `Tensor.native()` on the returned tensor will return `objects`.",
                    "signature": "(objects: Union[Sequence[~T], ~T], *shape: Union[phiml.math._shape.Shape, str]) -> phiml.math._tensors.Tensor[~T]"
                },
                {
                    "name": "length",
                    "qualname": "length",
                    "docstring": "Deprecated. Use `norm` instead.",
                    "signature": "(*args, **kwargs)"
                },
                {
                    "name": "linspace",
                    "qualname": "linspace",
                    "docstring": "Returns `number` evenly spaced numbers between `start` and `stop` along `dim`.\n\nIf `dim` contains multiple dimensions, evenly spaces values along each dimension, then stacks the result along a new channel dimension called `vector`.\n\nSee Also:\n    `arange()`, `meshgrid()`.\n\nArgs:\n    start: First value, `int` or `Tensor`.\n    stop: Last value, `int` or `Tensor`.\n    dim: Linspace dimension of integer size.\n        The size determines how many values to linearly space between `start` and `stop`.\n        The values will be laid out along `dim`.\n\nReturns:\n    `Tensor`\n\nExamples:\n    >>> math.linspace(0, 1, spatial(x=5))\n    (0.000, 0.250, 0.500, 0.750, 1.000) along x\u02e2\n\n    >>> math.linspace(0, (-1, 1), spatial(x=3))\n    (0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (x\u02e2=3, vector\u1d9c=2)",
                    "signature": "(start: Union[float, phiml.math._tensors.Tensor, tuple, list], stop: Union[float, phiml.math._tensors.Tensor, tuple, list], dim: phiml.math._shape.Shape) -> phiml.math._tensors.Tensor[float]"
                },
                {
                    "name": "load",
                    "qualname": "load",
                    "docstring": "Loads a `Tensor` or tree from one or multiple files previously written using `save`.\n\nAll tensors are restored as NumPy arrays, not the backend-specific tensors they may have been written as.\nUse `convert()` to convert all or some of the tensors to a different backend.\n\nExamples:\n\n    >>> B = batch(b=3)\n    >>> files = -f-f\"data/test_{arange(B)}.npz\"\n    >>> data = randn(B, spatial(x=10))\n    >>> save(files, data)  # store 10 values per file\n    >>> assert_close(data, load(files))\n\nSee Also:\n    `save()`.\n\nArgs:\n    file: Either single file to read as `str` or a batch of files as a string `Tensor`.\n        When a batch of paths is provided, each file is loaded and the results are stacked according to the dims of `file`.\n        For obtaining a batch of files, see `wrap()`, `phiml.os.listdir()`, `phiml.math.f`.\n\nReturns:\n    Same type as what was written.",
                    "signature": "(file: Union[str, phiml.math._tensors.Tensor])"
                },
                {
                    "name": "log",
                    "qualname": "log",
                    "docstring": "Computes the natural logarithm of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "log10",
                    "qualname": "log10",
                    "docstring": "Computes *log(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x` with base 10.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "log2",
                    "qualname": "log2",
                    "docstring": "Computes *log(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x` with base 2.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "log_gamma",
                    "qualname": "log_gamma",
                    "docstring": "Computes *log(gamma(x))* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "map",
                    "qualname": "map_",
                    "docstring": "Calls `function` on slices of the arguments and returns the stacked result.\n\nArgs:\n    function: Function to be called on slices of `args` and `kwargs`.\n        Must return one or multiple values that can be stacked.\n        `None` may be returned but if any return value is `None`, all calls to `function` must return `None` in that position.\n    *args: Positional arguments for `function`.\n        Values that are `phiml.math.magic.Sliceable` will be sliced along `dims`.\n    **kwargs: Keyword arguments for `function`.\n        Values that are `phiml.math.magic.Sliceable` will be sliced along `dims`.\n    dims: Dimensions which should be sliced.\n        `function` is called once for each element in `dims`, i.e. `dims.volume` times.\n        If `dims` is not specified, all dimensions from the `phiml.math.magic.Sliceable` values in `args` and `kwargs` will be mapped.\n        Pass `object` to map only objects, not tensors of primitives (`dtype.kind == object`). This will select only `layout`-type dimensions.\n    range: Optional range function. Can be used to generate `tqdm` output by passing `trange`.\n    unwrap_scalars: If `True`, passes the contents of scalar `Tensor`s instead of the tensor objects.\n    simplify: If `True`, reduces constant dims of output tensors that don't vary across mapped slices.\n\nReturns:\n    `Tensor` of same shape as `value`.",
                    "signature": "(function: Callable[..., ~Y], *args, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function shape at 0x0000024F475F3380>, range=<class 'range'>, unwrap_scalars=True, expand_results=False, simplify=False, map_name=None, **kwargs) -> Union[NoneType, phiml.math._tensors.Tensor, ~Y]"
                },
                {
                    "name": "map_pairs",
                    "qualname": "map_pairs",
                    "docstring": "Evaluates `map_function` on all pairs of elements present in the sparsity pattern of `connections`.\n\nArgs:\n    map_function: Function with signature `(Tensor, Tensor) -> Tensor`.\n    values: Values to evaluate `map_function` on.\n        Needs to have a spatial or instance dimension but must not have a dual dimension.\n    connections: Sparse tensor.\n\nReturns:\n    `Tensor` with the sparse dimensions of `connections` and all non-instance dimensions returned by `map_function`.",
                    "signature": "(map_function: Callable, values: phiml.math._tensors.Tensor, connections: phiml.math._tensors.Tensor)"
                },
                {
                    "name": "map_types",
                    "qualname": "map_types",
                    "docstring": "Wraps a function to change the dimension types of its `Tensor` and `phiml.math.magic.PhiTreeNode` arguments.\n\nArgs:\n    f: Function to wrap.\n    dims: Concrete dimensions or dimension type, such as `spatial` or `batch`.\n        These dimensions will be mapped to `dim_type` for all positional function arguments.\n    dim_type: Dimension type, such as `spatial` or `batch`.\n        `f` will be called with dimensions remapped to this type.\n\nReturns:\n    Function with signature matching `f`.",
                    "signature": "(f: Callable, dims: Union[phiml.math._shape.Shape, tuple, list, str, Callable], dim_type: Union[Callable, str]) -> Callable"
                },
                {
                    "name": "masked_fill",
                    "qualname": "masked_fill",
                    "docstring": "Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance` steps in all spatial directions.\nOverlapping extrapolated values get averaged. Extrapolation also includes diagonals.\n\nArgs:\n    values: Tensor which holds the values for extrapolation\n    valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values\n    distance: Number of extrapolation steps\n\nReturns:\n    values: Extrapolation result\n    valid: mask marking all valid values after extrapolation",
                    "signature": "(values: phiml.math._tensors.Tensor, valid: phiml.math._tensors.Tensor, distance: int = 1) -> Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]"
                },
                {
                    "name": "matrix_from_function",
                    "qualname": "matrix_from_function",
                    "docstring": "Trace a linear function and construct a matrix.\nDepending on the functional form of `f`, the returned matrix may be dense or sparse.\n\nArgs:\n    f: Function to trace.\n    *args: Arguments for `f`.\n    auxiliary_args: Arguments in which the function is not linear.\n        These parameters are not traced but passed on as given in `args` and `kwargs`.\n    auto_compress: If `True`, returns a compressed matrix if supported by the backend.\n    sparsify_batch: If `False`, the matrix will be batched.\n        If `True`, will create dual dimensions for the involved batch dimensions.\n        This will result in one large matrix instead of a batch of matrices.\n    **kwargs: Keyword arguments for `f`.\n\nReturns:\n    matrix: Matrix representing the linear dependency of the output `f` on the input of `f`.\n        Input dimensions will be `dual` dimensions of the matrix while output dimensions will be regular.\n    bias: Bias for affine functions or zero-vector if the function is purely linear.",
                    "signature": "(f: Callable, *args, auxiliary_args=None, auto_compress=False, sparsify_batch=None, separate_independent=False, _return_raw_output=False, **kwargs) -> Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]"
                },
                {
                    "name": "matrix_rank",
                    "qualname": "matrix_rank",
                    "docstring": "Approximates the rank of a matrix.\nThe tolerances used depend on the current precision.\n\nArgs:\n    matrix: Sparse or dense matrix, i.e. `Tensor` with primal and dual dims.\n\nReturns:\n    Matrix rank.",
                    "signature": "(matrix: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "max",
                    "qualname": "max_",
                    "docstring": "Determines the maximum value of `values` along the specified dimensions.\n\nArgs:\n    value: (Sparse) `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    key: Optional comparison values. If specified, returns the value where `key` is maximal, see `at_max()`.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value: ~TensorOrTree, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, key: phiml.math._tensors.Tensor = None) -> ~TensorOrTree"
                },
                {
                    "name": "maximum",
                    "qualname": "maximum",
                    "docstring": "Computes the element-wise maximum of `x` and `y`.",
                    "signature": "(x: Union[phiml.math._tensors.Tensor, float], y: Union[phiml.math._tensors.Tensor, float], allow_none=False)"
                },
                {
                    "name": "mean",
                    "qualname": "mean",
                    "docstring": "Computes the mean over `values` along the specified dimensions.\n\nArgs:\n    value: (Sparse) `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    weight: Optionally perform a weighted mean operation. Must broadcast to `value`.\n    where_no_weight: Value to use when the sum of all weights are smaller than `epsilon`.\n    epsilon: Only if `where_no_weight`. Threshold for using `where_no_weight`.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, weight: Union[phiml.math._tensors.Tensor, list, tuple] = None, where_no_weight=nan, epsilon=1e-10) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "median",
                    "qualname": "median",
                    "docstring": "Reduces `dim` of `value` by picking the median value.\nFor odd dimension sizes (ambigous choice), the linear average of the two median values is computed.\n\nCurrently implemented via `quantile()`.\n\nArgs:\n    value: `Tensor`\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor`",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>)"
                },
                {
                    "name": "merge_shapes",
                    "qualname": "merge_shapes",
                    "docstring": "Combines `shapes` into a single `Shape`, grouping dimensions by type.\nIf dimensions with equal names are present in multiple shapes, their types and sizes must match.\n\nThe shorthand `shape1 & shape2` merges shapes with `check_exact=[spatial]`.\n\nSee Also:\n    `concat_shapes()`.\n\nArgs:\n    *objs: `Shape` or `Shaped` objects to combine.\n    allow_varying_sizes: If `True`, merges incompatible dims by setting their size to `None` and erasing their labels.\n        If `False`, raises an error for incompatible dims.\n\nReturns:\n    Merged `Shape`\n\nRaises:\n    IncompatibleShapes if the shapes are not compatible",
                    "signature": "(*objs: Union[phiml.math._shape.Shape, Any], allow_varying_sizes=False) -> phiml.math._shape.Shape"
                },
                {
                    "name": "meshgrid",
                    "qualname": "meshgrid",
                    "docstring": "Generate a mesh-grid `Tensor` from keyword dimensions.\n\nArgs:\n    **dimensions: Mesh-grid dimensions, mapping names to values.\n        Values may be `int`, 1D `Tensor` or 1D native tensor.\n    dims: Dimension type of mesh-grid dimensions, one of `spatial`, `channel`, `batch`, `instance`.\n    stack_dim: Channel dim along which grids are stacked.\n        This is optional for 1D mesh-grids. In that case returns a `Tensor` without a stack dim if `None` or an empty `Shape` is passed.\n\nReturns:\n    Mesh-grid `Tensor` with the dimensions of `dims` / `dimensions` and `stack_dim`.\n\nExamples:\n    >>> math.meshgrid(x=2, y=2)\n    (x\u02e2=2, y\u02e2=2, vector\u1d9c=x,y) 0.500 \u00b1 0.500 (0e+00...1e+00)\n\n    >>> math.meshgrid(x=2, y=(-1, 1))\n    (x\u02e2=2, y\u02e2=2, vector\u1d9c=x,y) 0.250 \u00b1 0.829 (-1e+00...1e+00)\n\n    >>> math.meshgrid(x=2, stack_dim=None)\n    (0, 1) along x\u02e2",
                    "signature": "(dims: Union[Callable, phiml.math._shape.Shape] = <function spatial at 0x0000024F475F3420>, stack_dim: Union[phiml.math._shape.Shape, str, NoneType] = (vector\u1d9c=None), **dimensions: Union[int, phiml.math._tensors.Tensor, tuple, list, Any]) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "min",
                    "qualname": "min_",
                    "docstring": "Determines the minimum value of `values` along the specified dimensions.\n\nArgs:\n    value: (Sparse) `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\n    key: Optional comparison values. If specified, returns the value where `key` is minimal, see `at_min()`.\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, key: phiml.math._tensors.Tensor = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "minimize",
                    "qualname": "minimize",
                    "docstring": "Finds a minimum of the scalar function *f(x)*.\nThe `method` argument of `solve` determines which optimizer is used.\nAll optimizers supported by `scipy.optimize.minimize` are supported,\nsee https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html .\nAdditionally a gradient descent solver with adaptive step size can be used with `method='GD'`.\n\n`math.minimize()` is limited to backends that support `jacobian()`, i.e. PyTorch, TensorFlow and Jax.\n\nTo obtain additional information about the performed solve, use a `SolveTape`.\n\nSee Also:\n    `solve_nonlinear()`.\n\nArgs:\n    f: Function whose output is subject to minimization.\n        All positional arguments of `f` are optimized and must be `Tensor` or `phiml.math.magic.PhiTreeNode`.\n        If `solve.x0` is a `tuple` or `list`, it will be passed to *f* as varargs, `f(*x0)`.\n        To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.\n        The first return value of `f` must be a scalar float `Tensor` or `phiml.math.magic.PhiTreeNode`.\n    solve: `Solve` object to specify method type, parameters and initial guess for `x`.\n\nReturns:\n    x: solution, the minimum point `x`.\n\nRaises:\n    NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.\n    Diverged: If the optimization failed prematurely.",
                    "signature": "(f: Callable[[~X], ~Y], solve: phiml.math._optimize.Solve[~X, ~Y]) -> ~X"
                },
                {
                    "name": "minimum",
                    "qualname": "minimum",
                    "docstring": "Computes the element-wise minimum of `x` and `y`.",
                    "signature": "(x: Union[phiml.math._tensors.Tensor, float], y: Union[phiml.math._tensors.Tensor, float], allow_none=False)"
                },
                {
                    "name": "nan_to_0",
                    "qualname": "nan_to_0",
                    "docstring": "Replaces all NaN values in `x` with `0`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "native",
                    "qualname": "native",
                    "docstring": "Returns the native tensor representation of `value`.\nIf `value` is a `phiml.math.Tensor`, this is equal to calling `phiml.math.Tensor.native()`.\nOtherwise, checks that `value` is a valid tensor object and returns it.\n\nArgs:\n    value: `Tensor` or native tensor or tensor-like.\n\nReturns:\n    Native tensor representation\n\nRaises:\n    ValueError if the tensor cannot be transposed to match target_shape",
                    "signature": "(value: Union[phiml.math._tensors.Tensor, numbers.Number, tuple, list, Any])"
                },
                {
                    "name": "native_call",
                    "qualname": "native_call",
                    "docstring": "Calls `f` with the native representations of the `inputs` tensors in standard layout and returns the result as a `Tensor`.\n\nAll inputs are converted to native tensors (including precision cast) depending on `channels_last`:\n\n* `channels_last=True`: Dimension layout `(total_batch_size, spatial_dims..., total_channel_size)`\n* `channels_last=False`: Dimension layout `(total_batch_size, total_channel_size, spatial_dims...)`\n\nAll batch dimensions are compressed into a single dimension with `total_batch_size = input.shape.batch.volume`.\nThe same is done for all channel dimensions.\n\nAdditionally, missing batch and spatial dimensions are added so that all `inputs` have the same batch and spatial shape.\n\nArgs:\n    f: Function to be called on native tensors of `inputs`.\n        The function output must have the same dimension layout as the inputs, unless overridden by `spatial_dim`,\n        and the batch size must be identical.\n    *inputs: Uniform `Tensor` arguments\n    channels_last: (Optional) Whether to put channels as the last dimension of the native representation.\n        If `None`, the channels are put in the default position associated with the current backend,\n        see `phiml.math.backend.Backend.prefers_channels_last()`.\n    channel_dim: Name of the channel dimension of the result.\n    spatial_dim: Name of the spatial dimension of the result.\n\nReturns:\n    `Tensor` with batch and spatial dimensions of `inputs`, unless overridden by `spatial_dim`,\n    and single channel dimension `channel_dim`.",
                    "signature": "(f: Callable, *inputs: phiml.math._tensors.Tensor, channels_last=None, channel_dim='vector', spatial_dim=None, **f_kwargs)"
                },
                {
                    "name": "ncat",
                    "qualname": "ncat",
                    "docstring": "Concatenate named components along `dim`.\n\nArgs:\n    values: Each value can contain multiple components of `dim` if `dim` is present in its shape.\n        Else, it is interpreted as a single component whose name will be determined from the leftover labels of `dim`.\n    dim: Single dimension that has labels matching components of `values`.\n    expand_values: If `True`, will add all missing dims to values, not just batch dimensions.\n        This allows tensors with different dims to be concatenated.\n        The resulting tensor will have all dims that are present in `values`.\n        If `False`, this may return a non-numeric object instead.\n\nReturns:\n    Same type as any value from `values`.",
                    "signature": "(values: Sequence[~PhiTreeNodeType], dim: phiml.math._shape.Shape, expand_values=False) -> ~PhiTreeNodeType"
                },
                {
                    "name": "neighbor_max",
                    "qualname": "neighbor_max",
                    "docstring": "`neighbor_reduce` with `reduce_fun` set to `phiml.math.max`.",
                    "signature": "(grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, extend_bounds=0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "neighbor_mean",
                    "qualname": "neighbor_mean",
                    "docstring": "`neighbor_reduce` with `reduce_fun` set to `phiml.math.mean`.",
                    "signature": "(grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, extend_bounds=0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "neighbor_min",
                    "qualname": "neighbor_min",
                    "docstring": "`neighbor_reduce` with `reduce_fun` set to `phiml.math.min`.",
                    "signature": "(grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, extend_bounds=0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "neighbor_reduce",
                    "qualname": "neighbor_reduce",
                    "docstring": "Computes the sum/mean/min/max/prod/etc. of two neighboring values along each dimension in `dim`.\nThe result tensor has one entry less than `grid` in each averaged dimension unless `padding` is specified.\n\nWith two `dims`, computes the mean of 4 values, in 3D, the mean of 8 values.\n\nArgs:\n    reduce_fun: Reduction function, such as `sum`, `mean`, `max`, `min`, `prod`.\n    grid: Values to reduce.\n    dims: Dimensions along which neighbors should be reduced.\n    padding: Padding at the upper edges of `grid` along `dims'. If not `None`, the result tensor will have the same shape as `grid`.\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n    `Tensor`",
                    "signature": "(reduce_fun: Callable, grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, padding_kwargs: dict = None, extend_bounds=0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "neighbor_sum",
                    "qualname": "neighbor_sum",
                    "docstring": "`neighbor_reduce` with `reduce_fun` set to `phiml.math.sum`.",
                    "signature": "(grid: phiml.math._tensors.Tensor, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = None, extend_bounds=0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "non_batch",
                    "qualname": "non_batch",
                    "docstring": "Returns the non-batch dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "non_channel",
                    "qualname": "non_channel",
                    "docstring": "Returns the non-channel dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "non_dual",
                    "qualname": "non_dual",
                    "docstring": "Returns the non-dual dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "non_instance",
                    "qualname": "non_instance",
                    "docstring": "Returns the non-instance dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "non_primal",
                    "qualname": "non_primal",
                    "docstring": "Returns the batch and dual dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "non_spatial",
                    "qualname": "non_spatial",
                    "docstring": "Returns the non-spatial dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "nonzero",
                    "qualname": "nonzero",
                    "docstring": "Get spatial indices of non-zero / True values.\n\nBatch dimensions are preserved by this operation.\nIf channel dimensions are present, this method returns the indices where any component is nonzero.\n\nImplementations:\n\n* NumPy: [`numpy.argwhere`](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html)\n* PyTorch: [`torch.nonzero`](https://pytorch.org/docs/stable/generated/torch.nonzero.html)\n* TensorFlow: [`tf.where(tf.not_equal(values, 0))`](https://www.tensorflow.org/api_docs/python/tf/where)\n* Jax: [`jax.numpy.nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html)\n\nArgs:\n    value: spatial tensor to find non-zero / True values in.\n    list_dim: Dimension listing non-zero values. If size specified, lists only the first `size` non-zero values.\n        Special case: For retrieving only the first non-zero value, you may pass `1` instead of a `Shape` of size 1.\n    index_dim: Index dimension.\n    element_dims: Dims listing components of one value. A value is only considered `zero` if all components are 0.\n    list_dims: Dims in which non-zero elements are searched. These will be stored in the labels of `index_dim`.\n\nReturns:\n    `Tensor` of shape (batch dims..., `list_dim`=#non-zero, `index_dim`=value.shape.spatial_rank)",
                    "signature": "(value: Union[phiml.math._tensors.Tensor, bool], list_dim: Union[phiml.math._shape.Shape, str, int] = (nonzero\u2071=None), index_dim: phiml.math._shape.Shape = (vector\u1d9c=None), element_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, list_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, preserve_names=False)"
                },
                {
                    "name": "norm",
                    "qualname": "norm",
                    "docstring": "Computes the vector norm (L2 norm) of `vec` defined as \u221a\u2211v\u00b2.\n\nArgs:\n    eps: Minimum valid vector length. Use to avoid `inf` gradients for zero-norm vectors.\n        Lengths shorter than `eps` are set to 0.",
                    "signature": "(vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, eps: Union[float, phiml.math._tensors.Tensor] = None)"
                },
                {
                    "name": "normalize",
                    "qualname": "normalize",
                    "docstring": "Normalizes the vectors in `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector.\n\nArgs:\n    vec: `Tensor` to normalize.\n    vec_dim: Dimensions to normalize over. By default, all channel dimensions are used to compute the vector length.\n    epsilon: (Optional) Zero-length threshold. Vectors shorter than this length yield the unit vector (1, 0, 0, ...).\n        If not specified, the zero-vector yields `NaN` as it cannot be normalized.\n    allow_infinite: Allow infinite components in vectors. These vectors will then only points towards the infinite components.\n    allow_zero: Whether to return zero vectors for inputs smaller `epsilon` instead of a unit vector.",
                    "signature": "(vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, epsilon=None, allow_infinite=False, allow_zero=False)"
                },
                {
                    "name": "normalize_to",
                    "qualname": "normalize_to",
                    "docstring": "Multiplies the target so that its sum matches the source.\n\nArgs:\n    target: `Tensor`\n    source: `Tensor` or constant\n    epsilon: Small number to prevent division by zero.\n\nReturns:\n    Normalized tensor of the same shape as target",
                    "signature": "(target: phiml.math._tensors.Tensor, source: Union[float, phiml.math._tensors.Tensor], epsilon=1e-05)"
                },
                {
                    "name": "numpy",
                    "qualname": "numpy_",
                    "docstring": "Converts `value` to a `numpy.ndarray` where value must be a `Tensor`, backend tensor or tensor-like.\nIf `value` is a `phiml.math.Tensor`, this is equal to calling `phiml.math.Tensor.numpy()`.\n\n*Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.\nTo get a differentiable tensor, use `Tensor.native()` instead.\n\nTransposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.\nIf a dimension of the tensor is not listed in `order`, a `ValueError` is raised.\n\nIf `value` is a NumPy array, it may be returned directly.\n\nReturns:\n    NumPy representation of `value`\n\nRaises:\n    ValueError if the tensor cannot be transposed to match target_shape",
                    "signature": "(value: Union[phiml.math._tensors.Tensor, numbers.Number, tuple, list, Any])"
                },
                {
                    "name": "ones",
                    "qualname": "ones",
                    "docstring": "Define a tensor with specified shape with value `1.0`/ `1` / `True` everywhere.\n\nThis method may not immediately allocate the memory to store the values.\n\nSee Also:\n    `ones_like()`, `zeros()`.\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.\n\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "ones_like",
                    "qualname": "ones_like",
                    "docstring": "Create a `Tensor` containing only `1.0` / `1` / `True` with the same shape and dtype as `obj`.",
                    "signature": "(value: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "p2d",
                    "qualname": "p2d",
                    "docstring": "Change the type of all *primal* dims (instance, spatial, channel) of `value` to *dual* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "pack_dims",
                    "qualname": "pack_dims",
                    "docstring": "Compresses multiple dims into a single dimension by concatenating the elements.\nElements along the new dims are laid out according to the order of `dims`.\nIf the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.\nThis function replaces the traditional `reshape` for these cases.\n\nThe type of the new dimension will be equal to the types of `dims`.\nIf `dims` have varying types, the new dimension will be a batch dimension.\n\nIf none of `dims` exist on `value`, `packed_dim` will be added only if it is given with a definite size and `value` is not a primitive type.\n\nSee Also:\n    `unpack_dim()`\n\nArgs:\n    value: `phiml.math.magic.Shapable`, such as `phiml.math.Tensor`.\n    dims: Dimensions to be compressed in the specified order.\n    packed_dim: Single-dimension `Shape`.\n    pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.\n\nExamples:\n    >>> pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance('points'))\n    (points\u2071=12) const 0.0",
                    "signature": "(value, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], packed_dim: Union[phiml.math._shape.Shape, str], pos: Optional[int] = None, **kwargs)"
                },
                {
                    "name": "pad",
                    "qualname": "pad",
                    "docstring": "Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.\nUnlike `Extrapolation.pad()`, this function can handle negative widths which slice off outer values.\n\nArgs:\n    value: `Tensor` to be padded\n    widths: Number of values to add at the edge of `value`. Negative values can be used to slice off edge values. Must be one of the following:\n\n        * `tuple` containing `(lower: int, upper: int)`. This will pad all non-batch dimensions by `lower` and `upper` at the lower and upper edge, respectively.\n        * `dict` mapping `dim: str -> (lower: int, upper: int)`\n        * Sequence of slicing `dict`s. This will add all values specified by the slicing dicts and is the inverse operation to `slice_off`. Exactly one value in each slicing dict must be a `slice` object.\n\n    mode: Padding mode used to determine values added from positive `widths`.\n        Must be one of the following: `Extrapolation`, `Tensor` or number for constant extrapolation, name of extrapolation as `str`.\n    kwargs: Additional padding arguments.\n        These are ignored by the standard extrapolations defined in `phiml.math.extrapolation` but can be used to pass additional contextual information to custom extrapolations.\n\nReturns:\n    Padded `Tensor`\n\nExamples:\n    >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, 1), 'y': (2, 1)}, 0)\n    (x\u02e2=12, y\u02e2=13) 0.641 \u00b1 0.480 (0e+00...1e+00)\n\n    >>> math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, -1)}, 0)\n    (x\u02e2=10, y\u02e2=10) 0.900 \u00b1 0.300 (0e+00...1e+00)",
                    "signature": "(value: phiml.math._tensors.Tensor, widths: Union[dict, tuple, list], mode: Union[ForwardRef('e_.Extrapolation'), phiml.math._tensors.Tensor, numbers.Number, str, dict] = 0, **kwargs) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "pad_to_uniform",
                    "qualname": "pad_to_uniform",
                    "docstring": "Pads a possibly non-uniform to the smallest uniform shape that fits all elements.\n\nArgs:\n    x: Value to pad. Can be uniform or non-uniform.\n    target_shape: Override size along any dims in order to add more padding.\n\nReturns:\n    Uniform `Tensor`",
                    "signature": "(x: phiml.math._tensors.Tensor, target_shape: phiml.math._shape.Shape = ())"
                },
                {
                    "name": "pairwise_differences",
                    "qualname": "pairwise_differences",
                    "docstring": "Computes the distance matrix containing the pairwise position differences between each pair of points.\nThe matrix will consist of the channel and batch dimension of `positions` and the primal dimensions plus their dual counterparts, spanning the matrix.\nPoints that are further apart than `max_distance` (if specified) are assigned an invalid value given by `default`.\nThe diagonal of the matrix (self-distance) consists purely of zero-vectors and is always stored explicitly.\nThe neighbors of the positions are listed along the dual dimension(s) of the matrix, and vectors point *towards* the neighbors.\n\nThis function can operate in *dense* mode or *sparse* mode, depending on `format`.\nIf `format=='dense'` or a dense `Tensor`, all possible pair-wise distances are considered and a full-rank tensor is returned.\nThe value of `method` is ignored in that case.\n\nOtherwise, if `format` is a sparse format identifier or sparse `Tensor`, only a subset of distances is considered, depending on `method`.\nIn this case, the result is a sparse matrix with the same dimensionos as the dense tensor would have had.\n\n**JIT behavior:** This function can be JIT compiled with all backends.\nHowever, as the exact number of neighbors is unknown beforehand, all sparse methods rely on a variable-size buffer.\nPyTorch and TensorFlow allow variable shapes and behave the same way with JIT compilation as without.\nJAX, however, requires all tensor shapes to be known beforehand.\nThis function will guess the required buffer size based on `avg_neighbors` and track the actually required sizes.\nWhen using `phiml.math.jit_compile`, this will automatically trigger a re-tracing when a buffer overflow is detected.\nUser calling `jax.jit` manually must retrieve these sizes from the buffer API and implement buffer overflow handling.\n\nArgs:\n    positions: `Tensor`.\n        Channel dimensions are interpreted as position components.\n        Instance and spatial dimensions list nodes.\n    max_distance: Scalar or `Tensor` specifying a max_radius for each point separately.\n        Can contain additional batch dimensions but spatial/instance dimensions must match `positions` if present.\n        If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.\n    format: Matrix format as `str` or concrete sparsity pattern as `Tensor`.\n        Allowed strings are `'dense'', `'sparse'`, `'csr'`, `'coo'`, `'csc'`.\n        When a `Tensor` is passed, it needs to have all instance and spatial dims as `positions` as well as corresponding dual dimensions.\n        The distances will be evaluated at all stored entries of the `format` tensor.\n    domain: Lower and upper corner of the bounding box. All positions must lie within this box.\n        This must be specified to use with periodic boundaries.\n    periodic: Which domain boundaries should be treated as periodic, i.e. particles on opposite sides are neighbors.\n        Can be specified as a `bool` for all sides or as a vector-valued boolean `Tensor` to specify periodicity by direction.\n    default: Value for distances greater than `max_distance`. Only for dense distance matrices.\n    method: Neighbor search algorithm; only used if `format` is a sparse format or `Tensor`.\n        The default, `'auto'` lets the runtime decide on the best method. Supported methods:\n\n        * `'sparse'`: GPU-supported hash grid implementation with fully sparse connectivity.\n        * `'scipy-kd'`: SciPy's [kd-tree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query_ball_point.html#scipy.spatial.KDTree.query_ball_point) implementation.\n\n    avg_neighbors: Expected average number of neighbors. This is only relevant for hash grid searches, where it influences the default buffer sizes.\n\nReturns:\n    Distance matrix as sparse or dense `Tensor`, depending on `format`.\n    For each spatial/instance dimension in `positions`, the matrix also contains a dual dimension of the same name and size.\n    The matrix also contains all batch dimensions of `positions` and the channel dimension of `positions`.\n\nExamples:\n    >>> pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))\n    >>> dx = pairwise_differences(pos, format='dense', max_distance=2)\n    >>> dx.particles[0]\n    (x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (~particles\u1d48=3, vector\u1d9c=x,y)",
                    "signature": "(positions: phiml.math._tensors.Tensor, max_distance: Union[float, phiml.math._tensors.Tensor] = None, format: Union[str, phiml.math._tensors.Tensor] = 'dense', domain: Optional[Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]] = None, periodic: Union[bool, phiml.math._tensors.Tensor] = False, method: str = 'auto', default: float = nan, avg_neighbors=8.0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "pairwise_distances",
                    "qualname": "pairwise_differences",
                    "docstring": "Computes the distance matrix containing the pairwise position differences between each pair of points.\nThe matrix will consist of the channel and batch dimension of `positions` and the primal dimensions plus their dual counterparts, spanning the matrix.\nPoints that are further apart than `max_distance` (if specified) are assigned an invalid value given by `default`.\nThe diagonal of the matrix (self-distance) consists purely of zero-vectors and is always stored explicitly.\nThe neighbors of the positions are listed along the dual dimension(s) of the matrix, and vectors point *towards* the neighbors.\n\nThis function can operate in *dense* mode or *sparse* mode, depending on `format`.\nIf `format=='dense'` or a dense `Tensor`, all possible pair-wise distances are considered and a full-rank tensor is returned.\nThe value of `method` is ignored in that case.\n\nOtherwise, if `format` is a sparse format identifier or sparse `Tensor`, only a subset of distances is considered, depending on `method`.\nIn this case, the result is a sparse matrix with the same dimensionos as the dense tensor would have had.\n\n**JIT behavior:** This function can be JIT compiled with all backends.\nHowever, as the exact number of neighbors is unknown beforehand, all sparse methods rely on a variable-size buffer.\nPyTorch and TensorFlow allow variable shapes and behave the same way with JIT compilation as without.\nJAX, however, requires all tensor shapes to be known beforehand.\nThis function will guess the required buffer size based on `avg_neighbors` and track the actually required sizes.\nWhen using `phiml.math.jit_compile`, this will automatically trigger a re-tracing when a buffer overflow is detected.\nUser calling `jax.jit` manually must retrieve these sizes from the buffer API and implement buffer overflow handling.\n\nArgs:\n    positions: `Tensor`.\n        Channel dimensions are interpreted as position components.\n        Instance and spatial dimensions list nodes.\n    max_distance: Scalar or `Tensor` specifying a max_radius for each point separately.\n        Can contain additional batch dimensions but spatial/instance dimensions must match `positions` if present.\n        If not specified, uses an infinite cutoff radius, i.e. all points will be considered neighbors.\n    format: Matrix format as `str` or concrete sparsity pattern as `Tensor`.\n        Allowed strings are `'dense'', `'sparse'`, `'csr'`, `'coo'`, `'csc'`.\n        When a `Tensor` is passed, it needs to have all instance and spatial dims as `positions` as well as corresponding dual dimensions.\n        The distances will be evaluated at all stored entries of the `format` tensor.\n    domain: Lower and upper corner of the bounding box. All positions must lie within this box.\n        This must be specified to use with periodic boundaries.\n    periodic: Which domain boundaries should be treated as periodic, i.e. particles on opposite sides are neighbors.\n        Can be specified as a `bool` for all sides or as a vector-valued boolean `Tensor` to specify periodicity by direction.\n    default: Value for distances greater than `max_distance`. Only for dense distance matrices.\n    method: Neighbor search algorithm; only used if `format` is a sparse format or `Tensor`.\n        The default, `'auto'` lets the runtime decide on the best method. Supported methods:\n\n        * `'sparse'`: GPU-supported hash grid implementation with fully sparse connectivity.\n        * `'scipy-kd'`: SciPy's [kd-tree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query_ball_point.html#scipy.spatial.KDTree.query_ball_point) implementation.\n\n    avg_neighbors: Expected average number of neighbors. This is only relevant for hash grid searches, where it influences the default buffer sizes.\n\nReturns:\n    Distance matrix as sparse or dense `Tensor`, depending on `format`.\n    For each spatial/instance dimension in `positions`, the matrix also contains a dual dimension of the same name and size.\n    The matrix also contains all batch dimensions of `positions` and the channel dimension of `positions`.\n\nExamples:\n    >>> pos = vec(x=0, y=tensor([0, 1, 2.5], instance('particles')))\n    >>> dx = pairwise_differences(pos, format='dense', max_distance=2)\n    >>> dx.particles[0]\n    (x=0.000, y=0.000); (x=0.000, y=1.000); (x=0.000, y=0.000) (~particles\u1d48=3, vector\u1d9c=x,y)",
                    "signature": "(positions: phiml.math._tensors.Tensor, max_distance: Union[float, phiml.math._tensors.Tensor] = None, format: Union[str, phiml.math._tensors.Tensor] = 'dense', domain: Optional[Tuple[phiml.math._tensors.Tensor, phiml.math._tensors.Tensor]] = None, periodic: Union[bool, phiml.math._tensors.Tensor] = False, method: str = 'auto', default: float = nan, avg_neighbors=8.0) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "perf_counter",
                    "qualname": "perf_counter",
                    "docstring": "Get the time (`time.perf_counter()`) at which all `wait_for_tensors` are computed.\nIf all tensors are already available, returns the current `time.perf_counter()`.\n\nArgs:\n    wait_for_tensor: `Tensor` that need to be computed before the time is measured.\n    *wait_for_tensors: Additional tensors that need to be computed before the time is measured.\n\nReturns:\n    Time at which all `wait_for_tensors` are ready as a scalar `Tensor`.",
                    "signature": "(wait_for_tensor, *wait_for_tensors: phiml.math._tensors.Tensor) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "precision",
                    "qualname": "precision",
                    "docstring": "Sets the floating point precision for the local context.\n\nUsage: `with precision(p):`\n\nThis overrides the global setting, see `set_global_precision()`.\n\nArgs:\n    floating_point_bits: 16 for half, 32 for single, 64 for double",
                    "signature": "(floating_point_bits: int)"
                },
                {
                    "name": "primal",
                    "qualname": "primal",
                    "docstring": "Returns the instance, spatial and channel dimensions of an object.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    `Shape`",
                    "signature": "(obj) -> phiml.math._shape.Shape"
                },
                {
                    "name": "print",
                    "qualname": "print_",
                    "docstring": "Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.\n\nUnlike NumPy's array printing, the dimensions are sorted.\nElements along the alphabetically first dimension is printed to the right, the second dimension upward.\nTypically, this means x right, y up.\n\nArgs:\n    obj: tensor-like\n    name: name of the tensor\n\nReturns:",
                    "signature": "(obj: Union[phiml.math._tensors.Tensor, phiml.math.magic.PhiTreeNode, numbers.Number, tuple, list, NoneType] = None, name: str = '')"
                },
                {
                    "name": "print_gradient",
                    "qualname": "print_gradient",
                    "docstring": "Prints the gradient vector of `value` when computed.\nThe gradient at `value` is the vector-Jacobian product of all operations between the output of this function and the loss value.\n\nThe gradient is not printed in jit mode, see `jit_compile()`.\n\nExample:\n    ```python\n    def f(x):\n        x = math.print_gradient(x, 'dx')\n        return math.l1_loss(x)\n\n    math.jacobian(f)(math.ones(x=6))\n    ```\n\nArgs:\n    value: `Tensor` for which the gradient may be computed later.\n    name: (Optional) Name to print along with the gradient values\n    detailed: If `False`, prints a short summary of the gradient tensor.\n\nReturns:\n    `identity(value)` which when differentiated, prints the gradient vector.",
                    "signature": "(value: phiml.math._tensors.Tensor, name='', detailed=False) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "prod",
                    "qualname": "prod",
                    "docstring": "Multiplies `values` along the specified dimensions.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "quantile",
                    "qualname": "quantile",
                    "docstring": "Compute the q-th quantile of `value` along `dim` for each q in `quantiles`.\n\nImplementations:\n\n* NumPy: [`quantile`](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)\n* PyTorch: [`quantile`](https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile)\n* TensorFlow: [`tfp.stats.percentile`](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile)\n* Jax: [`quantile`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html)\n\nArgs:\n    value: `Tensor`\n    quantiles: Single quantile or tensor of quantiles to compute.\n        Must be of type `float`, `tuple`, `list` or `Tensor`.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to reduce the sequence of Tensors\n\nReturns:\n    `Tensor` with dimensions of `quantiles` and non-reduced dimensions of `value`.",
                    "signature": "(value: phiml.math._tensors.Tensor, quantiles: Union[float, tuple, list, phiml.math._tensors.Tensor], dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>)"
                },
                {
                    "name": "radians_to_degrees",
                    "qualname": "radians_to_degrees",
                    "docstring": "Convert degrees to radians.",
                    "signature": "(rad: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "rand",
                    "qualname": "random_uniform",
                    "docstring": "Creates a `Tensor` with the specified shape, filled with random values sampled from a uniform distribution.\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: (optional) `DType` or `(kind, bits)`.\n        The dtype kind must be one of `float`, `int`, `complex`.\n        If not specified, a `float` tensor with the current default precision is created, see `get_precision()`.\n    low: Minimum value, included.\n    high: Maximum value, excluded.\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, low: Union[phiml.math._tensors.Tensor, float] = 0, high: Union[phiml.math._tensors.Tensor, float] = 1, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "randn",
                    "qualname": "random_normal",
                    "docstring": "Creates a `Tensor` with the specified shape, filled with random values sampled from a normal / Gaussian distribution.\n\nImplementations:\n\n* NumPy: [`numpy.random.standard_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html)\n* PyTorch: [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)\n* TensorFlow: [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)\n* Jax: [`jax.random.normal`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: (optional) floating point `DType`. If `None`, a float tensor with the current default precision is created, see `get_precision()`.\n\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "random_normal",
                    "qualname": "random_normal",
                    "docstring": "Creates a `Tensor` with the specified shape, filled with random values sampled from a normal / Gaussian distribution.\n\nImplementations:\n\n* NumPy: [`numpy.random.standard_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html)\n* PyTorch: [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)\n* TensorFlow: [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)\n* Jax: [`jax.random.normal`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: (optional) floating point `DType`. If `None`, a float tensor with the current default precision is created, see `get_precision()`.\n\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "random_uniform",
                    "qualname": "random_uniform",
                    "docstring": "Creates a `Tensor` with the specified shape, filled with random values sampled from a uniform distribution.\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: (optional) `DType` or `(kind, bits)`.\n        The dtype kind must be one of `float`, `int`, `complex`.\n        If not specified, a `float` tensor with the current default precision is created, see `get_precision()`.\n    low: Minimum value, included.\n    high: Maximum value, excluded.\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, low: Union[phiml.math._tensors.Tensor, float] = 0, high: Union[phiml.math._tensors.Tensor, float] = 1, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "range",
                    "qualname": "arange",
                    "docstring": "Returns evenly spaced values between `start` and `stop`.\nIf only one limit is given, `0` is used for the start.\n\nSee Also:\n    `range_tensor()`, `linspace()`, `meshgrid()`.\n\nArgs:\n    dim: Dimension name and type as `Shape` object.\n        The `size` of `dim` is interpreted as `stop` unless `start_or_stop` is specified.\n    start_or_stop: (Optional) `int`. Interpreted as `start` if `stop` is specified as well. Otherwise this is `stop`.\n    stop: (Optional) `int`. `stop` value.\n    step: Distance between values.\n    backend: Backend to use for creating the tensor. If unspecified, uses the current default.\n\nReturns:\n    `Tensor`",
                    "signature": "(dim: phiml.math._shape.Shape, start_or_stop: Optional[int] = None, stop: Optional[int] = None, step=1, backend=None) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "range_tensor",
                    "qualname": "range_tensor",
                    "docstring": "Returns a `Tensor` with given `shape` containing the linear indices of each element.\nFor 1D tensors, this equivalent to `arange()` with `step=1`.\n\nSee Also:\n    `arange()`, `meshgrid()`.\n\nArgs:\n    shape: Tensor shape.\n\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "ravel_index",
                    "qualname": "ravel_index",
                    "docstring": "Computes a scalar index from a vector index.\n\nArgs:\n    index: `Tensor` with one channel dim.\n    resolution: `Shape`\n    mode: `'undefined'`, `'periodic'`, `'clamp'` or an `int` to use for all invalid indices.\n\nReturns:\n    `Tensor`",
                    "signature": "(index: phiml.math._tensors.Tensor, resolution: phiml.math._shape.Shape, dim=<function channel at 0x0000024F475F3560>, mode='undefined') -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "real",
                    "qualname": "real",
                    "docstring": "See Also:\n    `imag()`, `conjugate()`.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` or native tensor.\n\nReturns:\n    Real component of `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "rename_dims",
                    "qualname": "rename_dims",
                    "docstring": "Change the name and optionally the type of some dims of `value`.\n\nDimensions that are not present on value will be ignored. The corresponding new dims given by `names` will not be added.\n\nArgs:\n    value: `Shape` or `Tensor` or `Shapable`.\n    dims: Existing dims of `value` as comma-separated `str`, `tuple`, `list`, `Shape` or filter function.\n    names: Either\n\n        * Sequence of names matching `dims` as `tuple`, `list` or `str`. This replaces only the dimension names but leaves the types untouched.\n        * `Shape` matching `dims` to replace names and types.\n        * Dimension type function to replace only types.\n\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.",
                    "signature": "(value: ~PhiTreeNodeType, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], names: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], **kwargs) -> ~PhiTreeNodeType"
                },
                {
                    "name": "replace",
                    "qualname": "replace",
                    "docstring": "Creates a copy of the given `phiml.math.magic.PhiTreeNode` with updated values as specified in `updates`.\n\nIf `obj` overrides `__with_attrs__`, the copy will be created via that specific implementation.\nOtherwise, the `copy` module and `setattr` will be used.\n\nArgs:\n    obj: `phiml.math.magic.PhiTreeNode`\n    **updates: Values to be replaced.\n\nReturns:\n    Copy of `obj` with updated values.",
                    "signature": "(obj: ~PhiTreeNodeType, **updates) -> ~PhiTreeNodeType"
                },
                {
                    "name": "replace_dims",
                    "qualname": "rename_dims",
                    "docstring": "Change the name and optionally the type of some dims of `value`.\n\nDimensions that are not present on value will be ignored. The corresponding new dims given by `names` will not be added.\n\nArgs:\n    value: `Shape` or `Tensor` or `Shapable`.\n    dims: Existing dims of `value` as comma-separated `str`, `tuple`, `list`, `Shape` or filter function.\n    names: Either\n\n        * Sequence of names matching `dims` as `tuple`, `list` or `str`. This replaces only the dimension names but leaves the types untouched.\n        * `Shape` matching `dims` to replace names and types.\n        * Dimension type function to replace only types.\n\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.",
                    "signature": "(value: ~PhiTreeNodeType, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], names: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], **kwargs) -> ~PhiTreeNodeType"
                },
                {
                    "name": "reshaped_native",
                    "qualname": "reshaped_native",
                    "docstring": "Returns a native representation of `value` where dimensions are laid out according to `groups`.\n\nSee Also:\n    `native()`, `pack_dims()`.\n\nArgs:\n    value: `Tensor`\n    groups: `tuple` or `list` of dimensions to be packed into one native dimension. Each entry must be one of the following:\n\n        * `str`: the name of one dimension that is present on `value`.\n        * `Shape`: Dimensions to be packed. If `force_expand`, missing dimensions are first added, otherwise they are ignored.\n        * Filter function: Packs all dimensions of this type that are present on `value`.\n        * Ellipsis `...`: Packs all remaining dimensions into this slot. Can only be passed once.\n        * `None` or `()`: Adds a singleton dimension.\n\n        Collections of or comma-separated dims may also be used but only if all dims are present on `value`.\n\n    force_expand: `bool` or sequence of dimensions.\n        If `True`, repeats the tensor along missing dimensions.\n        If `False`, puts singleton dimensions where possible.\n        If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.\n    to_numpy: If True, converts the native tensor to a `numpy.ndarray`.\n\nReturns:\n    Native tensor with dimensions matching `groups`.",
                    "signature": "(value: phiml.math._tensors.Tensor, groups: Union[tuple, list], force_expand: Any = True, to_numpy=False)"
                },
                {
                    "name": "reshaped_numpy",
                    "qualname": "reshaped_numpy",
                    "docstring": "Returns the NumPy representation of `value` where dimensions are laid out according to `groups`.\n\nSee Also:\n    `numpy()`, `reshaped_native()`, `pack_dims()`, `reshaped_tensor()`.\n\nArgs:\n    value: `Tensor`\n    groups: Sequence of dimension names as `str` or groups of dimensions to be packed_dim as `Shape`.\n    force_expand: `bool` or sequence of dimensions.\n        If `True`, repeats the tensor along missing dimensions.\n        If `False`, puts singleton dimensions where possible.\n        If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.\n\nReturns:\n    NumPy `ndarray` with dimensions matching `groups`.",
                    "signature": "(value: phiml.math._tensors.Tensor, groups: Union[tuple, list], force_expand: Any = True) -> numpy.ndarray"
                },
                {
                    "name": "reshaped_tensor",
                    "qualname": "reshaped_tensor",
                    "docstring": "Creates a `Tensor` from a native tensor or tensor-like whereby the dimensions of `value` are split according to `groups`.\n\nSee Also:\n    `phiml.math.tensor()`, `reshaped_native()`, `unpack_dim()`.\n\nArgs:\n    value: Native tensor or tensor-like.\n    groups: Sequence of dimension groups to be packed_dim as `tuple[Shape]` or `list[Shape]`.\n    check_sizes: If True, group sizes must match the sizes of `value` exactly. Otherwise, allows singleton dimensions.\n    convert: If True, converts the data to the native format of the current default backend.\n        If False, wraps the data in a `Tensor` but keeps the given data reference if possible.\n\nReturns:\n    `Tensor` with all dimensions from `groups`",
                    "signature": "(value: Any, groups: Sequence[Union[phiml.math._shape.Shape, str]], check_sizes=False, convert=True)"
                },
                {
                    "name": "rotate_vector",
                    "qualname": "rotate_vector",
                    "docstring": "Rotates `vector` around the origin.\n\nArgs:\n    vector: n-dimensional vector with exactly one channel dimension\n    angle: Euler angle(s) or rotation matrix.\n        `None` is interpreted as no rotation.\n    invert: Whether to apply the inverse rotation.\n\nReturns:\n    Rotated vector as `Tensor`",
                    "signature": "(vector: phiml.math._tensors.Tensor, angle: Union[float, phiml.math._tensors.Tensor, NoneType], invert=False, dim='vector') -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "rotation_matrix",
                    "qualname": "rotation_matrix",
                    "docstring": "Create a 2D or 3D rotation matrix from the corresponding angle(s).\n\nArgs:\n    x:\n        2D: scalar angle\n        3D: Either vector pointing along the rotation axis with rotation angle as length or Euler angles.\n        Euler angles need to be laid out along a `angle` channel dimension with dimension names listing the spatial dimensions.\n        E.g. a 90\u00b0 rotation about the z-axis is represented by `vec('angles', x=0, y=0, z=PI/2)`.\n        If a rotation matrix is passed for `angle`, it is returned without modification.\n    matrix_dim: Matrix dimension for 2D rotations. In 3D, the channel dimension of angle is used.\n\nReturns:\n    Matrix containing `matrix_dim` in primal and dual form as well as all non-channel dimensions of `x`.",
                    "signature": "(x: Union[float, phiml.math._tensors.Tensor, NoneType], matrix_dim=(vector\u1d9c=None)) -> Optional[phiml.math._tensors.Tensor]"
                },
                {
                    "name": "round",
                    "qualname": "round_",
                    "docstring": "Rounds the `Tensor` or `phiml.math.magic.PhiTreeNode` `x` to the closest integer.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "s2b",
                    "qualname": "s2b",
                    "docstring": "Change the type of all *spatial* dims of `value` to *batch* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "safe_div",
                    "qualname": "safe_div",
                    "docstring": "Computes *x/y* with the `Tensor`s `x` and `y` but returns 0 where *y=0*.",
                    "signature": "(x: Union[numbers.Number, phiml.math._tensors.Tensor], y: Union[numbers.Number, phiml.math._tensors.Tensor])"
                },
                {
                    "name": "safe_mul",
                    "qualname": "safe_mul",
                    "docstring": "Multiplication for tensors with non-finite values.\nComputes *x\u00b7y* in the forward pass but drops gradient contributions from infinite and `NaN` values.",
                    "signature": "(x: Union[numbers.Number, phiml.math._tensors.Tensor], y: Union[numbers.Number, phiml.math._tensors.Tensor])"
                },
                {
                    "name": "sample_subgrid",
                    "qualname": "sample_subgrid",
                    "docstring": "Samples a sub-grid from `grid` with equal distance between sampling points.\nThe values at the new sample points are determined via linear interpolation.\n\nArgs:\n    grid: `Tensor` to be resampled. Values are assumed to be sampled at cell centers.\n    start: Origin point of sub-grid within `grid`, measured in number of cells.\n        Must have a single dimension called `vector`.\n        Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.\n        The order of dims must be equal to `size` and `grid.shape.spatial`.\n    size: Resolution of the sub-grid. Must not be larger than the resolution of `grid`.\n        The order of dims must be equal to `start` and `grid.shape.spatial`.\n\nReturns:\n  Sub-grid as `Tensor`",
                    "signature": "(grid: phiml.math._tensors.Tensor, start: phiml.math._tensors.Tensor, size: phiml.math._shape.Shape) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "save",
                    "qualname": "save",
                    "docstring": "Saves a `Tensor` or tree using NumPy.\nThis function converts all tensors contained in `obj` to NumPy tensors before storing.\nEach tensor is given a name corresponding to its path within `obj`, allowing reading only specific arrays from the file later on.\nPickle is used for structures, but no reference to `Tensor` or its sub-classes is included.\n\nExamples:\n\n    >>> B = batch(b=3)\n    >>> files = -f-f\"data/test_{arange(B)}.npz\"\n    >>> data = randn(B, spatial(x=10))\n    >>> save(files, data)  # store 10 values per file\n    >>> assert_close(data, load(files))\n\nSee Also:\n    `load()`.\n\nArgs:\n    file: Either single file to read as `str` or a batch of files as a string `Tensor`. The file ending will be completed to `.npz`.\n        When a batch of paths is provided, the data `obj` is sliced along the dims of `file` and broken up to be stored among the multiple files.\n        For obtaining a batch of files, see `wrap()`, `phiml.os.listdir()`, `phiml.math.f`.\n    obj: `Tensor` or tree to store.\n    mkdir: Whether to create the file's directory if it doesn't exist.",
                    "signature": "(file: Union[phiml.math._tensors.Tensor, str], obj: ~TensorOrTree, mkdir=True)"
                },
                {
                    "name": "scatter",
                    "qualname": "scatter",
                    "docstring": "Scatters `values` into `base_grid` at `indices`.\ninstance dimensions of `indices` and/or `values` are reduced during scattering.\nDepending on `mode`, this method has one of the following effects:\n\n* `mode='update'`: Replaces the values of `base_grid` at `indices` by `values`. The result is undefined if `indices` contains duplicates.\n* `mode='add'`: Adds `values` to `base_grid` at `indices`. The values corresponding to duplicate indices are accumulated.\n* `mode='mean'`: Replaces the values of `base_grid` at `indices` by the mean of all `values` with the same index.\n\nImplementations:\n\n* NumPy: Slice assignment / `numpy.add.at`\n* PyTorch: [`torch.scatter`](https://pytorch.org/docs/stable/generated/torch.scatter.html), [`torch.scatter_add`](https://pytorch.org/docs/stable/generated/torch.scatter_add.html)\n* TensorFlow: [`tf.tensor_scatter_nd_add`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add), [`tf.tensor_scatter_nd_update`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update)\n* Jax: [`jax.lax.scatter_add`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html), [`jax.lax.scatter`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html)\n\nSee Also:\n    `gather()`.\n\nArgs:\n    base_grid: `Tensor` into which `values` are scattered.\n    indices: `Tensor` of n-dimensional indices at which to place `values`.\n        Must have a single channel dimension with size matching the number of spatial dimensions of `base_grid`.\n        This dimension is optional if the spatial rank is 1.\n        Must also contain all `scatter_dims`.\n    values: `Tensor` of values to scatter at `indices`.\n    mode: Scatter mode as `str` or function.\n        Supported modes are 'add', 'mean', 'update', 'max', 'min', 'prod', 'any', 'all'.\n        The corresponding functions are the built-in `sum`, `max\u00b4, `min`, as well as the reduce functions in `phiml.math`.\n    outside_handling: Defines how indices lying outside the bounds of `base_grid` are handled.\n\n        * `'check'`: Raise an error if any index is out of bounds.\n        * `'discard'`: Outside indices are ignored.\n        * `'clamp'`: Outside indices are projected onto the closest point inside the grid.\n        * `'undefined'`: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.\n    indices_gradient: Whether to allow the gradient of this operation to be backpropagated through `indices`.\n    default: Default value to use for bins into which no value is scattered.\n        By default, `NaN` is used for the modes `update` and `mean`, `0` for `sum`, `inf` for min and `-inf` for max.\n        This will upgrade the data type to `float` if necessary.\n    treat_as_batch: Dimensions which should be treated like dims by this operation.\n        This can be used for scattering vectors along instance dims into a grid.\n        Normally, instance dims on `values` and `indices` would not be matched to `base_grid` but when treated as batch, they will be.\n\nReturns:\n    Copy of `base_grid` with updated values at `indices`.",
                    "signature": "(base_grid: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape], indices: Union[phiml.math._tensors.Tensor, dict], values: Union[phiml.math._tensors.Tensor, float], mode: Union[str, Callable] = 'update', outside_handling: str = 'check', indices_gradient=False, default=None, treat_as_batch=None)"
                },
                {
                    "name": "seed",
                    "qualname": "seed",
                    "docstring": "Sets the current seed of all backends and the built-in `random` package.\n\nCalling this function with a fixed value at the start of an application yields reproducible results\nas long as the same backend is used.\n\nArgs:\n    seed: Seed to use.",
                    "signature": "(seed: int)"
                },
                {
                    "name": "set_global_precision",
                    "qualname": "set_global_precision",
                    "docstring": "Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.\n\nIf `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.\nOperations may also convert floating point values to this precision, even if the input had a different precision.\n\nIf `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.\nThe output of math operations has the same precision as its inputs.\n\nArgs:\n  floating_point_bits: one of (16, 32, 64, None)",
                    "signature": "(floating_point_bits: int)"
                },
                {
                    "name": "shape",
                    "qualname": "shape",
                    "docstring": "If `obj` is a `Tensor` or `phiml.math.magic.Shaped`, returns its shape.\nIf `obj` is a `Shape`, returns `obj`.\n\nThis function can be passed as a `dim` argument to an operation to specify that it should act upon all dimensions.\n\nArgs:\n    obj: `Tensor` or `Shape` or `Shaped`\n    allow_unshaped: If `True`, returns an empty shape for unsupported objects, else raises a `ValueError`.\n\nReturns:\n    `Shape`",
                    "signature": "(obj, allow_unshaped=False) -> phiml.math._shape.Shape"
                },
                {
                    "name": "shift",
                    "qualname": "shift",
                    "docstring": "Shift the tensor `x` by a fixed offset, using `padding` for edge values.\n\nThis is similar to `numpy.roll()` but with major differences:\n\n* Values shifted in from the boundary are defined by `padding`.\n* Positive offsets represent negative shifts.\n* Support for multi-dimensional shifts\n\nSee Also:\n    `index_shift`, `neighbor_reduce`.\n\nArgs:\n    x: Input grid-like `Tensor`.\n    offsets: `tuple` listing shifts to compute, each must be an `int`. One `Tensor` will be returned for each entry.\n    dims: Dimensions along which to shift, defaults to all *spatial* dims of `x`.\n    padding: Padding to be performed at the boundary so that the shifted versions have the same size as `x`.\n        Must be one of the following: `Extrapolation`, `Tensor` or number for constant extrapolation, name of extrapolation as `str`.\n        Can be set to `None` to disable padding. Then the result tensors will be smaller than `x`.\n    stack_dim: Dimension along which the components corresponding to each dim in `dims` should be stacked.\n        This can be set to `None` only if `dims` is a single dimension.\n    extend_bounds: Number of cells by which to pad the tensors in addition to the number required to maintain the size of `x`.\n        Can only be used with a valid `padding`.\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n    `list` of shifted tensors. The number of return tensors is equal to the number of `offsets`.",
                    "signature": "(x: phiml.math._tensors.Tensor, offsets: Sequence[int], dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = zero-gradient, stack_dim: Union[phiml.math._shape.Shape, str, NoneType] = (shift\u1d9c=None), extend_bounds: Union[tuple, int] = 0, padding_kwargs: dict = None) -> List[phiml.math._tensors.Tensor]"
                },
                {
                    "name": "si2d",
                    "qualname": "si2d",
                    "docstring": "Change the type of all *spatial* and *instance* dims of `value` to *dual* dimensions. See `rename_dims`.",
                    "signature": "(value: ~PhiTreeNodeType) -> ~PhiTreeNodeType"
                },
                {
                    "name": "sigmoid",
                    "qualname": "sigmoid",
                    "docstring": "Computes the sigmoid function of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "sign",
                    "qualname": "sign",
                    "docstring": "The sign of positive numbers is 1 and -1 for negative numbers.\nThe sign of 0 is undefined.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode`\n\nReturns:\n    `Tensor` or `phiml.math.magic.PhiTreeNode` matching `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "sin",
                    "qualname": "sin",
                    "docstring": "Computes *sin(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "sinh",
                    "qualname": "sinh",
                    "docstring": "Computes *sinh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "slice",
                    "qualname": "slice_",
                    "docstring": "Slices a `Tensor` or `phiml.math.magic.PhiTreeNode` along named dimensions.\n\nSee Also:\n    `unstack`.\n\nArgs:\n    value: `Tensor` or `phiml.math.magic.PhiTreeNode` or `Number` or `None`.\n    slices: `dict` mapping dimension names to slices. A slice can be one of the following:\n\n        * An index (`int`)\n        * A range (`slice`)\n        * An item name (`str`)\n        * Multiple labels (comma-separated `str`)\n        * Multiple indices or labels (`tuple` or `list`)\n\nReturns:\n    `Tensor` or `phiml.math.magic.PhiTreeNode` of the same type as `value`.\n\nExamples:\n    >>> math.slice([vec(x=0, y=1), vec(x=2, y=3)], {'vector': 'y'})\n    [1, 3]",
                    "signature": "(value: ~PhiTreeNodeType, slices: Union[Dict[str, Union[int, slice, str, tuple, list, Any]], Any]) -> ~PhiTreeNodeType"
                },
                {
                    "name": "slice_off",
                    "qualname": "slice_off",
                    "docstring": "Args:\n    x: Any instance of `phiml.math.magic.Shapable`\n    *slices:\n\nReturns:",
                    "signature": "(x, *slices: Dict[str, Union[slice, int, str]])"
                },
                {
                    "name": "soft_plus",
                    "qualname": "soft_plus",
                    "docstring": "Computes *softplus(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "softmax",
                    "qualname": "softmax",
                    "docstring": "Compute the softmax of `x` over any dimension. The softmax is e^x / \u2211 e^x .",
                    "signature": "(x, reduce: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType])"
                },
                {
                    "name": "solve_linear",
                    "qualname": "solve_linear",
                    "docstring": "Solves the system of linear equations *f(x) = y* and returns *x*.\nThis method will use the solver specified in `solve`.\nThe following method identifiers are supported by all backends:\n\n* `'auto'`: Automatically choose a solver\n* `'CG'`: Conjugate gradient, only for symmetric and positive definite matrices.\n* `'CG-adaptive'`: Conjugate gradient with adaptive step size, only for symmetric and positive definite matrices.\n* `'biCG'` or `'biCG-stab(0)'`: Biconjugate gradient\n* `'biCG-stab'` or `'biCG-stab(1)'`: Biconjugate gradient stabilized, first order\n* `'biCG-stab(2)'`, `'biCG-stab(4)'`, ...: Biconjugate gradient stabilized, second or higher order\n* `'scipy-direct'`: SciPy direct solve always run oh the CPU using `scipy.sparse.linalg.spsolve`.\n* `'scipy-CG'`, `'scipy-GMres'`, `'scipy-biCG'`, `'scipy-biCG-stab'`, `'scipy-CGS'`, `'scipy-QMR'`, `'scipy-GCrotMK'`, `'scipy-lsqr'`: SciPy iterative solvers always run oh the CPU, both in eager execution and JIT mode.\n\nFor maximum performance, compile `f` using `jit_compile_linear()` beforehand.\nThen, an optimized representation of `f` (such as a sparse matrix) will be used to solve the linear system.\n\n**Caution:** The matrix construction may potentially be performed each time `solve_linear` is called if auxiliary arguments change.\nTo prevent this, jit-compile the function that makes the call to `solve_linear`.\n\nTo obtain additional information about the performed solve, perform the solve within a `SolveTape` context.\nThe used implementation can be obtained as `SolveInfo.method`.\n\nThe gradient of this operation will perform another linear solve with the parameters specified by `Solve.gradient_solve`.\n\nSee Also:\n    `solve_nonlinear()`, `jit_compile_linear()`.\n\nArgs:\n    f: One of the following:\n\n        * Linear function with `Tensor` or `phiml.math.magic.PhiTreeNode` first parameter and return value. `f` can have additional auxiliary arguments and return auxiliary values.\n        * Dense matrix (`Tensor` with at least one dual dimension)\n        * Sparse matrix (Sparse `Tensor` with at least one dual dimension)\n        * Native tensor (not yet supported)\n\n    y: Desired output of `f(x)` as `Tensor` or `phiml.math.magic.PhiTreeNode`.\n    solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.\n    *f_args: Positional arguments to be passed to `f` after `solve.x0`. These arguments will not be solved for.\n        Supports vararg mode or pass all arguments as a `tuple`.\n    f_kwargs: Additional keyword arguments to be passed to `f`.\n        These arguments are treated as auxiliary arguments and can be of any type.\n\nReturns:\n    x: solution of the linear system of equations `f(x) = y` as `Tensor` or `phiml.math.magic.PhiTreeNode`.\n\nRaises:\n    NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.\n    Diverged: If the solve failed prematurely.",
                    "signature": "(f: Union[Callable[[~X], ~Y], phiml.math._tensors.Tensor], y: ~Y, solve: phiml.math._optimize.Solve[~X, ~Y], *f_args, grad_for_f=False, f_kwargs: dict = None, **f_kwargs_) -> ~X"
                },
                {
                    "name": "solve_nonlinear",
                    "qualname": "solve_nonlinear",
                    "docstring": "Solves the non-linear equation *f(x) = y* by minimizing the norm of the residual.\n\nThis method is limited to backends that support `jacobian()`, currently PyTorch, TensorFlow and Jax.\n\nTo obtain additional information about the performed solve, use a `SolveTape`.\n\nSee Also:\n    `minimize()`, `solve_linear()`.\n\nArgs:\n    f: Function whose output is optimized to match `y`.\n        All positional arguments of `f` are optimized and must be `Tensor` or `phiml.math.magic.PhiTreeNode`.\n        The output of `f` must match `y`.\n    y: Desired output of `f(x)` as `Tensor` or `phiml.math.magic.PhiTreeNode`.\n    solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.\n\nReturns:\n    x: Solution fulfilling `f(x) = y` within specified tolerance as `Tensor` or `phiml.math.magic.PhiTreeNode`.\n\nRaises:\n    NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.\n    Diverged: If the solve failed prematurely.",
                    "signature": "(f: Callable, y, solve: phiml.math._optimize.Solve) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "sort",
                    "qualname": "sort",
                    "docstring": "Sort the values of `x` along `dim`.\nIf `key` is specified, sorts `x` according to the corresponding values in the `key` tensor.\nWhen sorting by key, you can pass pytrees and dataclasses for `x`. The value `range` for `x` returns the sorting permutation.\n\nIn order to sort a flattened array, use `pack_dims` first.\n\nExamples:\n\n>>> x = tensor([1, 3, 2, -1], spatial('x'))\n>>> math.sort(x)\n>>> # Out: (-1, 1, 2, 3) along x\u02e2\n\n>>> math.sort(range, 'x', key=x)\n>>> # Out: (3, 0, 2, 1) along x\u02e2 int64\n\n>>> result, perm = math.sort((x, range), key=x)\n\nArgs:\n    x: `Tensor` to sort. If `key` is specified, can be a tree as well.\n    dim: Dimension to sort. If not present, sorting will be skipped. Defaults to non-batch dim.\n    key: `Tensor` holding values to compare during sorting.\n\nReturns:\n    Sorted `Tensor` or `x` if `x` is constant along `dims`.",
                    "signature": "(x: ~TensorOrTree, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, key: phiml.math._tensors.Tensor = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "spack",
                    "qualname": "spack",
                    "docstring": "Short for `pack_dims(..., dims=spatial)",
                    "signature": "(value, packed_dim: Union[phiml.math._shape.Shape, str], pos: Optional[int] = None, **kwargs)"
                },
                {
                    "name": "sparse_tensor",
                    "qualname": "sparse_tensor",
                    "docstring": "Construct a sparse tensor that stores `values` at the corresponding `indices` and is 0 everywhere else.\nDuplicate entries (entries with the same indices) are identical to one entry with the sum of the corresponding values.\nThis can be performed explicitly using `sum_equal_entries()`.\n\nIn addition to the sparse dimensions indexed by `indices`, the tensor inherits all batch and channel dimensions from `values`.\n\nSparse tensors can be used to implement `bincount`, i.e. `bincount = dense(sparse_tensor(indices, weights, dims))`.\n\nArgs:\n    indices: `Tensor` encoding the positions of stored values. It can either list the individual stored indices (COO format) or encode only part of the index while containing other dimensions directly (compact format).\n\n        For COO, it has the following dimensions:\n\n        * One instance dimension exactly matching the instance dimension on `values`.\n          It enumerates the positions of stored entries.\n        * One channel dimension.\n          Its labels must match the dimension names of `dense_shape` but the order can be arbitrary.\n        * Any number of batch dimensions\n\n        You may pass `None` to create a sparse tensor with no entries.\n\n    values: `Tensor` containing the stored values at positions given by `indices`. It has the following dimensions:\n\n        * One instance dimension exactly matching the instance dimension on `indices`.\n          It enumerates the values of stored entries.\n        * Any number of channel dimensions if multiple values are stored at each index.\n        * Any number of batch dimensions\n\n    dense_shape: Dimensions listed in `indices`.\n        The order can differ from the labels of `indices`.\n    can_contain_double_entries: Whether some indices might occur more than once.\n        If so, values at the same index will be summed.\n    indices_sorted: Whether the indices are sorted in ascending order given the dimension order of the labels of `indices`.\n    indices_constant: Whether the positions of the non-zero values are fixed.\n        If `True`, JIT compilation will not create a placeholder for `indices`.\n    format: Sparse format in which to store the data, such as `'coo'` or `'csr'`. See `phiml.math.get_format`.\n        If `None`, uses the format in which the indices were given.\n\nReturns:\n    Sparse `Tensor` with the specified `format`.",
                    "signature": "(indices: Optional[phiml.math._tensors.Tensor], values: Union[phiml.math._tensors.Tensor, numbers.Number], dense_shape: phiml.math._shape.Shape, can_contain_double_entries=True, indices_sorted=False, format=None, indices_constant: bool = True) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "spatial",
                    "qualname": "spatial",
                    "docstring": "Returns the spatial dimensions of an existing `Shape` or creates a new `Shape` with only spatial dimensions.\n\nUsage for filtering spatial dimensions:\n>>> spatial_dims = spatial(shape)\n>>> spatial_dims = spatial(tensor)\n\nUsage for creating a `Shape` with only spatial dimensions:\n>>> spatial_shape = spatial('undef', x=2, y=3)\n(x=2, y=3, undef=None)\n\nHere, the dimension `undef` is created with an undefined size of `None`.\nUndefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.\n\nTo create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 & shape2`.\n\nSee Also:\n    `channel`, `batch`, `instance`\n\nArgs:\n    *args: Either\n\n        * `Shape` or `Tensor` to filter or\n        * Names of dimensions with undefined sizes as `str`.\n\n    **dims: Dimension sizes and names. Must be empty when used as a filter operation.\n\nReturns:\n    `Shape` containing only dimensions of type spatial.",
                    "signature": "(*args, **dims: Union[int, str, tuple, list, phiml.math._shape.Shape, ForwardRef('Tensor')]) -> phiml.math._shape.Shape"
                },
                {
                    "name": "spatial_gradient",
                    "qualname": "spatial_gradient",
                    "docstring": "Calculates the spatial_gradient of a scalar channel from finite differences.\nThe spatial_gradient vectors are in reverse order, lowest dimension first.\n\nArgs:\n    grid: grid values\n    dims: (Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names\n    dx: Physical distance between grid points, `float` or `Tensor`.\n        When passing a vector-valued `Tensor`, the dx values should be listed along `stack_dim`, matching `dims`.\n    difference: type of difference, one of ('forward', 'backward', 'central') (default 'forward')\n    padding: Padding mode.\n        Must be one of the following: `Extrapolation`, `Tensor` or number for constant extrapolation, name of extrapolation as `str`.\n    stack_dim: name of the new vector dimension listing the spatial_gradient w.r.t. the various axes\n    pad: How many cells to extend the result compared to `grid`.\n        This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n    `Tensor`",
                    "signature": "(grid: phiml.math._tensors.Tensor, dx: Union[float, phiml.math._tensors.Tensor] = 1, difference: str = 'central', padding: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor, str, NoneType] = zero-gradient, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, stack_dim: Union[phiml.math._shape.Shape, str, NoneType] = (gradient\u1d9c=None), pad=0, padding_kwargs: dict = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "sqrt",
                    "qualname": "sqrt",
                    "docstring": "Computes *sqrt(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "squared_norm",
                    "qualname": "squared_norm",
                    "docstring": "Computes the squared norm of `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector.",
                    "signature": "(vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>)"
                },
                {
                    "name": "squeeze",
                    "qualname": "squeeze",
                    "docstring": "Remove specific singleton (volume=1) dims from `x`.\n\nArgs:\n    x: Tensor or composite type / tree.\n    dims: Singleton dims to remove.\n\nReturns:\n    Same type as `x`.",
                    "signature": "(x: ~PhiTreeNodeType, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType]) -> ~PhiTreeNodeType"
                },
                {
                    "name": "srange",
                    "qualname": "srange",
                    "docstring": "Construct a range `Tensor` along one spatial dim.",
                    "signature": "(start: int = 0, **stop: int) -> phiml.math._tensors.Tensor[int]"
                },
                {
                    "name": "ssize",
                    "qualname": "ssize",
                    "docstring": "Returns the total number of elements listed along spatial dims of an object, equal to the product of the sizes of all spatial dims.\n\nArgs:\n    obj: `Shape` or object with a valid `shape` property.\n\nReturns:\n    Size as `int`. If `obj` is an undefined `Shape`, returns `None`.",
                    "signature": "(obj) -> Optional[int]"
                },
                {
                    "name": "stack",
                    "qualname": "stack",
                    "docstring": "Stacks `values` along the new dimension `dim`.\nAll values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.\nBatch dims will be added as needed.\n\nStacking tensors is performed lazily, i.e. the memory is allocated only when needed.\nThis makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.\n\nArgs:\n    values: Collection of `phiml.math.magic.Shapable`, such as `phiml.math.Tensor`\n        If a `dict`, keys must be of type `str` and are used as labels along `dim`.\n    dim: `Shape` with a least one dimension. None of these dims can be present with any of the `values`.\n        If `dim` is a single-dimension shape, its size is determined from `len(values)` and can be left undefined (`None`).\n        If `dim` is a multi-dimension shape, its volume must be equal to `len(values)`.\n    expand_values: If `True`, will first add missing dims to all values, not just batch dimensions.\n        This allows tensors with different dims to be stacked.\n        The resulting tensor will have all dims that are present in `values`.\n        If `False`, this may return a non-numeric object instead.\n    simplify: If `True` and all values are equal, returns one value without adding the dimension.\n    layout_non_matching: If non-matching values should be stacked using a Layout object, i.e. should be put into a named list instead.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    `Tensor` containing `values` stacked along `dim`.\n\nExamples:\n    >>> stack({'x': 0, 'y': 1}, channel('vector'))\n    (x=0, y=1)\n\n    >>> stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c='x,y'))\n    (x=0.000, y=1.000); (x=0.000, y=1.000) (b\u1d47=2, c\u1d9c=x,y)\n\n    >>> stack([vec(x=1, y=0), vec(x=2, y=3.)], batch('b'))\n    (x=1.000, y=0.000); (x=2.000, y=3.000) (b\u1d47=2, vector\u1d9c=x,y)",
                    "signature": "(values: Union[Sequence[~PhiTreeNodeType], Dict[str, ~PhiTreeNodeType]], dim: Union[phiml.math._shape.Shape, str], expand_values=False, simplify=False, layout_non_matching=False, **kwargs) -> ~PhiTreeNodeType"
                },
                {
                    "name": "std",
                    "qualname": "std",
                    "docstring": "Computes the standard deviation over `values` along the specified dimensions.\n\n*Warning*: The standard deviation of non-uniform tensors along the stack dimension is undefined.\n\nArgs:\n    value: `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "stop_gradient",
                    "qualname": "stop_gradient",
                    "docstring": "Disables gradients for the given tensor.\nThis may switch off the gradients for `x` itself or create a copy of `x` with disabled gradients.\n\nImplementations:\n\n* PyTorch: [`x.detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)\n* TensorFlow: [`tf.stop_gradient`](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)\n* Jax: [`jax.lax.stop_gradient`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html)\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` for which gradients should be disabled.\n\nReturns:\n    Copy of `x`.",
                    "signature": "(x)"
                },
                {
                    "name": "stored_indices",
                    "qualname": "stored_indices",
                    "docstring": "Returns the indices of the stored values for a given `Tensor``.\nFor sparse tensors, this will return the stored indices tensor.\nFor collapsed tensors, only the stored dimensions will be returned.\n\nArgs:\n    x: `Tensor`\n    list_dim: Dimension along which stored indices should be laid out.\n    invalid: One of `'discard'`, `'clamp'`, `'keep'` Filter result by valid indices.\n        Internally, invalid indices may be stored for performance reasons.\n\nReturns:\n    `Tensor` representing all indices of stored values.",
                    "signature": "(x: phiml.math._tensors.Tensor, list_dim=(entries\u2071=None), index_dim=(index\u1d9c=None), invalid='discard') -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "stored_values",
                    "qualname": "stored_values",
                    "docstring": "Returns the stored values for a given `Tensor``.\n\nFor sparse tensors, this will return only the stored entries.\n\nDense tensors are reshaped so that all non-batch dimensions are packed into `list_dim`. Batch dimensions are preserved.\n\nArgs:\n    x: `Tensor`\n    list_dim: Dimension along which stored values should be laid out.\n    invalid: One of `'discard'`, `'clamp'`, `'keep'` Filter result by valid indices.\n        Internally, invalid indices may be stored for performance reasons.\n\nReturns:\n    `Tensor` representing all values stored to represent `x`.",
                    "signature": "(x: phiml.math._tensors.Tensor, list_dim=(entries\u2071=None), invalid='discard') -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "sum",
                    "qualname": "sum_",
                    "docstring": "Sums `values` along the specified dimensions.\n\nArgs:\n    value: (Sparse) `Tensor` or `list` / `tuple` of Tensors.\n    dim: Dimension or dimensions to be reduced. One of\n\n        * `None` to reduce all non-batch dimensions\n        * `str` containing single dimension or comma-separated list of dimensions\n        * `Tuple[str]` or `List[str]`\n        * `Shape`\n        * `batch`, `instance`, `spatial`, `channel` to select dimensions by type\n        * `'0'` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors\n\nReturns:\n    `Tensor` without the reduced dimensions.",
                    "signature": "(value: ~TensorOrTree, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>) -> ~TensorOrTree"
                },
                {
                    "name": "svd",
                    "qualname": "svd",
                    "docstring": "Singular value decomposition.\n\nThe original matrix is approximated by `(latent_to_value * singular.T) @ latents` or `latent_to_value @ (singular * latents)`.\n\n**Warning:** Even for well-defined SVDs, different backend use different sign conventions, causing results to differ.\n\nArgs:\n    x: Matrix containing `feature_dim` and `list_dim`.\n    feature_dim: Dimensions that list the features (columns).\n    list_dim: Dimensions that list the data points (rows).\n    latent_dim: Latent dimension. If a size is specified, truncates the SVD to this size.\n    full_matrices: If `True`, return full-sized (square) matrices for latent_by_example and latent_to_value. These may not match the singular values.\n\nReturns:\n    latents: Latent vectors of each item listed. `Tensor` with `list_dim` and `latent_dim`.\n    singular: List of singular values. `Tensor` with `latent_dim`.\n    features: Stacked normalized features / trends. This matrix can be used to compute the original value from a latent vector. `Tensor` with `latent_dim` and `feature_dim`.",
                    "signature": "(x: phiml.math._tensors.Tensor, feature_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, list_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = None, latent_dim=(singular\u1d9c=None), full_matrices=False)"
                },
                {
                    "name": "swap_axes",
                    "qualname": "swap_axes",
                    "docstring": "Swap the dimension order of `x`.\nThis operation is generally not necessary for `Tensor`s because tensors will be reshaped under the hood or when getting the native/numpy representations.\nIt can be used to transpose native tensors.\n\nImplementations:\n\n* NumPy: [`numpy.transpose`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)\n* PyTorch: [`x.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute)\n* TensorFlow: [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)\n* Jax: [`jax.numpy.transpose`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)\n\nArgs:\n    x: `Tensor` or native tensor or `phiml.math.magic.Shapable`.\n    axes: `tuple` or `list`\n\nReturns:\n    `Tensor` or native tensor, depending on `x`.",
                    "signature": "(x, axes)"
                },
                {
                    "name": "tan",
                    "qualname": "tan",
                    "docstring": "Computes *tan(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "tanh",
                    "qualname": "tanh",
                    "docstring": "Computes *tanh(x)* of the `Tensor` or `phiml.math.magic.PhiTreeNode` `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "tcat",
                    "qualname": "tcat",
                    "docstring": "Concatenate values by dim type.\nThis function first packs all dims of `dim_type` into one dim, then concatenates all `values`.\nValues that do not have a dim of `dim_type` are considered a size-1 slice.\n\nThe name of the first matching dim of `dim_type` is used as the concatenated output dim name.\nIf no value has a matching dim, `default_name` is used instead.\n\nArgs:\n    values: Values to be concatenated.\n    dim_type: Dimension type along which to concatenate.\n    expand_values: Whether to add missing other non-batch dims to values as needed.\n    default_name: Concatenation dim name if none of the values have a matching dim.\n\nReturns:\n    Same type as any value.",
                    "signature": "(values: Sequence[~PhiTreeNodeType], dim_type: Callable, expand_values=False, default_name='tcat') -> ~PhiTreeNodeType"
                },
                {
                    "name": "tensor",
                    "qualname": "tensor",
                    "docstring": "Create a Tensor from the specified `data`.\nIf `convert=True`, converts `data` to the preferred format of the default backend.\n\n`data` must be one of the following:\n\n* Number: returns a dimensionless Tensor.\n* Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.\n* `tuple` or `list` of numbers: backs the Tensor with native tensor.\n* `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.\n* Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.\n* Shape: creates a 1D tensor listing the dimension sizes.\n\nWhile specifying `names` is optional in some cases, it is recommended to always specify them.\n\nDimension types are always inferred from the dimension names if specified.\n\nImplementations:\n\n* NumPy: [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n* PyTorch: [`torch.tensor`](https://pytorch.org/docs/stable/generated/torch.tensor.html), [`torch.from_numpy`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html)\n* TensorFlow: [`tf.convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor)\n* Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)\n\nSee Also:\n    `phiml.math.wrap()` which uses `convert=False`, `layout()`.\n\nArgs:\n    data: native tensor, sparse COO / CSR / CSC matrix, scalar, sequence, `Shape` or `Tensor`\n    shape: Ordered dimensions and types. If sizes are defined, they will be checked against `data`.`\n        When passing multiple shapes, they will be concatenated. Duplicate names are not allowed.\n        Instead of `Shape` instances, you may pass strings specifying dims in the format `name:t` or `name:t=(labels)` where `t` refers to the type letter, one of s,i,c,d,b.\n        Alternatively, you can pass a single `list` of shapes which will call `reshaped_tensor`. This allows for unpacking native dims into multiple dims.\n    convert: If True, converts the data to the native format of the current default backend.\n        If False, wraps the data in a `Tensor` but keeps the given data reference if possible.\n\nRaises:\n    AssertionError: if dimension names are not provided and cannot automatically be inferred\n    ValueError: if `data` is not tensor-like\n\nReturns:\n    Tensor containing same values as data\n\nExamples:\n    >>> tensor([1, 2, 3], channel(vector='x,y,z'))\n    (x=1, y=2, z=3)\n\n    >>> tensor([1., 2, 3], channel(vector='x,y,z'))\n    (x=1.000, y=2.000, z=3.000) float64\n\n    >>> tensor(numpy.zeros([10, 8, 6, 2]), batch('batch'), spatial('x,y'), channel(vector='x,y'))\n    (batch\u1d47=10, x\u02e2=8, y\u02e2=6, vector\u1d9c=x,y) float64 const 0.0\n\n    >>> tensor([(0, 1), (0, 2), (1, 3)], instance('particles'), channel(vector='x,y'))\n    (x=0, y=1); (x=0, y=2); (x=1, y=3) (particles\u2071=3, vector\u1d9c=x,y)\n\n    >>> tensor(numpy.random.randn(10))\n    (vector\u1d9c=10) float64 -0.128 \u00b1 1.197 (-2e+00...2e+00)",
                    "signature": "(data: Union[Sequence[~T], ~T], *shape: Union[phiml.math._shape.Shape, str, list], convert: bool = True, default_list_dim=(vector\u1d9c=None)) -> phiml.math._tensors.Tensor[~T]"
                },
                {
                    "name": "tensor_like",
                    "qualname": "tensor_like",
                    "docstring": "Creates a tensor with the same format and shape as `existing_tensor`.\n\nArgs:\n    existing_tensor: Any `Tensor`, sparse or dense.\n    values: New values to replace the existing values by.\n        If `existing_tensor` is sparse, `values` must broadcast to the instance dimension listing the stored indices.\n    value_order: Order of `values` compared to `existing_tensor`, only relevant if `existing_tensor` is sparse.\n        If `'original'`, the values are ordered like the values that was used to create the first tensor with this sparsity pattern.\n        If `'as existing'`, the values match the current order of `existing_tensor`.\n        Note that the order of values may be changed upon creating a sparse tensor.\n\nReturns:\n    `Tensor`",
                    "signature": "(existing_tensor: phiml.math._tensors.Tensor, values: Union[phiml.math._tensors.Tensor, numbers.Number, bool], value_order: str = None)"
                },
                {
                    "name": "to_complex",
                    "qualname": "to_complex",
                    "docstring": "Converts the given tensor to complex floating point format with the currently specified precision.\n\nThe precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.\n\nSee the documentation at https://tum-pbs.github.io/PhiML/Data_Types.html\n\nSee Also:\n    `cast()`.\n\nArgs:\n    x: values to convert\n\nReturns:\n    `Tensor` of same shape as `x`",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "to_device",
                    "qualname": "to_device",
                    "docstring": "Allocates the tensors of `value` on `device`.\nIf the value already exists on that device, this function may either create a copy of `value` or return `value` directly.\n\nSee Also:\n    `to_cpu()`.\n\nArgs:\n    value: `Tensor` or `phiml.math.magic.PhiTreeNode` or native tensor.\n    device: Device to allocate value on.\n        Either `ComputeDevice` or category `str`, such as `'CPU'` or `'GPU'`.\n    convert: Whether to convert tensors that do not belong to the corresponding backend to compatible native tensors.\n        If `False`, this function has no effect on numpy tensors.\n    use_dlpack: Only if `convert==True`.\n        Whether to use the DLPack library to convert from one GPU-enabled backend to another.\n\nReturns:\n    Same type as `value`.",
                    "signature": "(value, device: phiml.backend._backend.ComputeDevice, convert=True, use_dlpack=True)"
                },
                {
                    "name": "to_dict",
                    "qualname": "to_dict",
                    "docstring": "Returns a serializable form of a `Tensor` or `Shape`.\nThe result can be written to a JSON file, for example.\n\nSee Also:\n    `from_dict()`.\n\nArgs:\n    value: `Tensor` or `Shape`\n\nReturns:\n    Serializable Python tree of primitives",
                    "signature": "(value: Union[phiml.math._tensors.Tensor, phiml.math._shape.Shape])"
                },
                {
                    "name": "to_float",
                    "qualname": "to_float",
                    "docstring": "Converts the given tensor to floating point format with the currently specified precision.\n\nThe precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.\n\nSee the documentation at https://tum-pbs.github.io/PhiML/Data_Types.html\n\nSee Also:\n    `cast()`.\n\nArgs:\n    x: `Tensor` or `phiml.math.magic.PhiTreeNode` to convert\n\nReturns:\n    `Tensor` or `phiml.math.magic.PhiTreeNode` matching `x`.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "to_format",
                    "qualname": "to_format",
                    "docstring": "Converts a `Tensor` to the specified sparse format or to a dense tensor.\n\nArgs:\n    x: Sparse or dense `Tensor`\n    format: Target format. One of `'dense'`, `'coo'`, `'csr'`, or `'csc'`.\n        Additionally, `'sparse'` can be passed to convert dense matrices to a sparse format, decided based on the backend for `x`.\n\nReturns:\n    `Tensor` of the specified format.",
                    "signature": "(x: phiml.math._tensors.Tensor, format: str)"
                },
                {
                    "name": "to_int32",
                    "qualname": "to_int32",
                    "docstring": "Converts the `Tensor` or `phiml.math.magic.PhiTreeNode` `x` to 32-bit integer.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "to_int64",
                    "qualname": "to_int64",
                    "docstring": "Converts the `Tensor` or `phiml.math.magic.PhiTreeNode` `x` to 64-bit integer.",
                    "signature": "(x: ~TensorOrTree) -> ~TensorOrTree"
                },
                {
                    "name": "trace_check",
                    "qualname": "trace_check",
                    "docstring": "Tests if `f(*args, **kwargs)` has already been traced for arguments compatible with `args` and `kwargs`.\nIf true, jit-compiled functions are very fast since the Python function is not actually called anymore.\n\nArgs:\n    traced_function: Transformed Function, e.g. jit-compiled or linear function.\n    *args: Hypothetical arguments to be passed to `f`\n    **kwargs: Hypothetical keyword arguments to be passed to `f`\n\nReturns:\n    result: `True` if there is an existing trace that can be used, `False` if `f` would have to be re-traced.\n    message: A `str` that, if `result == False`, gives hints as to why `f` needs to be re-traced given `args` and `kwargs`.",
                    "signature": "(traced_function, *args, **kwargs) -> Tuple[bool, str]"
                },
                {
                    "name": "unpack_dim",
                    "qualname": "unpack_dim",
                    "docstring": "Decompresses a dimension by unstacking the elements along it.\nThis function replaces the traditional `reshape` for these cases.\nThe compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.\n\nIf `dim` does not exist on `value`, this function will return `value` as-is. This includes primitive types.\n\nSee Also:\n    `pack_dims()`\n\nArgs:\n    value: `phiml.math.magic.Shapable`, such as `Tensor`, for which one dimension should be split.\n    dim: Single dimension to be decompressed.\n    *unpacked_dims: Either vararg `Shape`, ordered dims to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.\n        This results in a single tensor output.\n        Alternatively, pass a `tuple` or `list` of shapes to unpack a dim into multiple tensors whose combined volumes match `dim.size`.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dims to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dims must always work without keyword arguments.\n\nReturns:\n    Same type as `value`.\n\nExamples:\n    >>> unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))\n    (x\u02e2=4, y\u02e2=3) const 0.0",
                    "signature": "(value, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], *unpacked_dims: Union[phiml.math._shape.Shape, Sequence[phiml.math._shape.Shape]], **kwargs)"
                },
                {
                    "name": "unravel_index",
                    "qualname": "unravel_index",
                    "docstring": "Computes a vector index from a scalar index.\n\nArgs:\n    index: Scalar index. May have a channel dimension of size 1.\n    resolution: `Shape`\n\nReturns:\n    `Tensor` like `index` but with `index_dim` listing the dims in `resolution`.",
                    "signature": "(index: phiml.math._tensors.Tensor, resolution: phiml.math._shape.Shape, index_dim=(index\u1d9c=None))"
                },
                {
                    "name": "unstack",
                    "qualname": "unstack",
                    "docstring": "Un-stacks a `Sliceable` along one or multiple dimensions.\n\nIf multiple dims are given, the order of elements will be according to the dimension order in `dim`, i.e. elements along the last dimension will be neighbors in the returned `tuple`.\nIf no dimension is given or none of the given dims exists on `value`, returns a list containing only `value`.\n\nSee Also:\n    `phiml.math.slice`.\n\nArgs:\n    value: `phiml.math.magic.Shapable`, such as `phiml.math.Tensor`\n    dim: Dimensions as `Shape` or comma-separated `str` or dimension type, i.e. `channel`, `spatial`, `instance`, `batch`.\n    expand: If `True`, `dim` must be a `Shape` and the returned tuple will have length `dim.volume`. Otherwise, only existing dims are unstacked.\n\nReturns:\n    `tuple` of objects matching the type of `value`.\n\nExamples:\n    >>> unstack(expand(0, spatial(x=5)), 'x')\n    (0.0, 0.0, 0.0, 0.0, 0.0)",
                    "signature": "(value: ~MagicType, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], expand=False) -> Tuple[~MagicType, ...]"
                },
                {
                    "name": "upsample2x",
                    "qualname": "upsample2x",
                    "docstring": "Resamples a regular grid to double the number of spatial sample points per dimension.\nThe grid values at the new points are determined via linear interpolation.\n\nArgs:\n    grid: half-size grid\n    padding: grid extrapolation\n    dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.\n    grid: Tensor:\n    padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)\n    dims: tuple or None:  (Default value = None)\n    padding_kwargs: Additional keyword arguments to be passed to `phiml.math.pad()`.\n\nReturns:\n  double-size grid",
                    "signature": "(grid: phiml.math._tensors.Tensor, padding: phiml.math.extrapolation.Extrapolation = zero-gradient, dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function spatial at 0x0000024F475F3420>, padding_kwargs: dict = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "use",
                    "qualname": "set_global_default_backend",
                    "docstring": "Sets the given backend as default.\nThis setting can be overridden using `with backend:`.\n\nSee `default_backend()`, `choose_backend()`.\n\nArgs:\n    backend: `Backend` or backend name to set as default.\n        Possible names are `'torch'`, `'tensorflow'`, `'jax'`, `'numpy'`.\n\nReturns:\n    The chosen backend as a `Backend\u00b4 instance.",
                    "signature": "(backend: Union[str, phiml.backend._backend.Backend]) -> phiml.backend._backend.Backend"
                },
                {
                    "name": "vec",
                    "qualname": "vec",
                    "docstring": "Lay out the given values along a channel dimension without converting them to the current backend.\n\nArgs:\n    name: Dimension name.\n    *sequence: Component values that will also be used as labels.\n        If specified, `components` must be empty.\n    **components: Values by component name.\n        If specified, no additional positional arguments must be given.\n    tuple_dim: Dimension for `tuple` values passed as components, e.g. `vec(x=(0, 1), ...)`\n    list_dim: Dimension for `list` values passed as components, e.g. `vec(x=[0, 1], ...)`\n\nReturns:\n    `Tensor`\n\nExamples:\n    >>> vec(x=1, y=0, z=-1)\n    (x=1, y=0, z=-1)\n\n    >>> vec(x=1., z=0)\n    (x=1.000, z=0.000)\n\n    >>> vec(x=tensor([1, 2, 3], instance('particles')), y=0)\n    (x=1, y=0); (x=2, y=0); (x=3, y=0) (particles\u2071=3, vector\u1d9c=x,y)\n\n    >>> vec(x=0, y=[0, 1])\n    (x=0, y=0); (x=0, y=1) (vector\u1d9c=x,y, sequence\u2071=2)\n\n    >>> vec(x=0, y=(0, 1))\n    (x=0, y=0); (x=0, y=1) (sequence\u02e2=2, vector\u1d9c=x,y)",
                    "signature": "(name: Union[str, phiml.math._shape.Shape] = 'vector', *sequence, tuple_dim=(sequence\u02e2=None), list_dim=(sequence\u2071=None), **components) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "vec_length",
                    "qualname": "length",
                    "docstring": "Deprecated. Use `norm` instead.",
                    "signature": "(*args, **kwargs)"
                },
                {
                    "name": "vec_normalize",
                    "qualname": "normalize",
                    "docstring": "Normalizes the vectors in `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector.\n\nArgs:\n    vec: `Tensor` to normalize.\n    vec_dim: Dimensions to normalize over. By default, all channel dimensions are used to compute the vector length.\n    epsilon: (Optional) Zero-length threshold. Vectors shorter than this length yield the unit vector (1, 0, 0, ...).\n        If not specified, the zero-vector yields `NaN` as it cannot be normalized.\n    allow_infinite: Allow infinite components in vectors. These vectors will then only points towards the infinite components.\n    allow_zero: Whether to return zero vectors for inputs smaller `epsilon` instead of a unit vector.",
                    "signature": "(vec: phiml.math._tensors.Tensor, vec_dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function channel at 0x0000024F475F3560>, epsilon=None, allow_infinite=False, allow_zero=False)"
                },
                {
                    "name": "vec_squared",
                    "qualname": "vec_squared",
                    "docstring": "Deprecated. Use `squared_norm` instead.",
                    "signature": "(*args, **kwargs)"
                },
                {
                    "name": "when_available",
                    "qualname": "when_available",
                    "docstring": "Calls `runnable(*tensor_args)` once the concrete values of all tensors are available.\nIn eager mode, `runnable` is called immediately.\nWhen jit-compiled, `runnable` is called after the jit-compiled function has returned.\n\nArgs:\n    runnable: Function to call as `runnable(*tensor_args)`. This can be a `lambda` function.\n    *tensor_args: `Tensor` values to pass to `runnable` with concrete values.",
                    "signature": "(runnable: Callable, *tensor_args: phiml.math._tensors.Tensor)"
                },
                {
                    "name": "where",
                    "qualname": "where",
                    "docstring": "Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.\nIf `condition` is not of type boolean, non-zero values are interpreted as True.\n\nThis function requires non-None values for `value_true` and `value_false`.\nTo get the indices of True / non-zero values, use :func:`nonzero`.\n\nArgs:\n  condition: determines where to choose values from value_true or from value_false\n  value_true: Values to pick where `condition != 0 / True`\n  value_false: Values to pick where `condition == 0 / False`\n\nReturns:\n    `Tensor` containing dimensions of all inputs.",
                    "signature": "(condition: Union[phiml.math._tensors.Tensor, bool], value_true: Union[phiml.math._tensors.Tensor, float, int, Any] = None, value_false: Union[phiml.math._tensors.Tensor, float, int, Any] = None)"
                },
                {
                    "name": "with_diagonal",
                    "qualname": "with_diagonal",
                    "docstring": "Create a copy of `matrix`, replacing the diagonal elements.\nIf `matrix` is sparse, diagonal zeros (and possibly other explicitly stored zeros) will be dropped from the sparse matrix.\n\nThis function currently only supports sparse COO,CSR,CSC SciPy matrices.\n\nArgs:\n    matrix: `Tensor` with at least one dual dim.\n    values: Diagonal values\n    check_square: If `True` allow this function only for square matrices.\n\nReturns:\n    `Tensor`",
                    "signature": "(matrix: phiml.math._tensors.Tensor, values: Union[float, phiml.math._tensors.Tensor], check_square=True)"
                },
                {
                    "name": "wrap",
                    "qualname": "wrap",
                    "docstring": "Short for `phiml.math.tensor()` with `convert=False`.",
                    "signature": "(data: Union[Sequence[~T], ~T], *shape: Union[phiml.math._shape.Shape, str, list], default_list_dim=(vector\u1d9c=None)) -> phiml.math._tensors.Tensor[~T]"
                },
                {
                    "name": "zeros",
                    "qualname": "zeros",
                    "docstring": "Define a tensor with specified shape with value `0.0` / `0` / `False` everywhere.\n\nThis method may not immediately allocate the memory to store the values.\n\nSee Also:\n    `zeros_like()`, `ones()`.\n\nArgs:\n    *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.\n    dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.\n\nReturns:\n    `Tensor`",
                    "signature": "(*shape: phiml.math._shape.Shape, dtype: Union[phiml.backend._dtype.DType, tuple, type] = None) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "zeros_like",
                    "qualname": "zeros_like",
                    "docstring": "Create a `Tensor` containing only `0.0` / `0` / `False` with the same shape and dtype as `obj`.",
                    "signature": "(obj: Union[phiml.math._tensors.Tensor, phiml.math.magic.PhiTreeNode]) -> Union[phiml.math._tensors.Tensor, phiml.math.magic.PhiTreeNode]"
                }
            ],
            "classes": [
                {
                    "name": "ConvergenceException",
                    "qualname": "ConvergenceException",
                    "docstring": "Base class for exceptions raised when a solve does not converge.\n\nSee Also:\n    `Diverged`, `NotConverged`.",
                    "signature": "(result: phiml.math._optimize.SolveInfo)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "DType",
                    "qualname": "DType",
                    "docstring": "Instances of `DType` represent the kind and size of data elements.\nThe data type of tensors can be obtained via `Tensor.dtype`.\n\nThe following kinds of data types are supported:\n\n* `float` with 32 / 64 bits\n* `complex` with 64 / 128 bits\n* `int` with 8 / 16 / 32 / 64 bits\n* `bool` with 8 bits\n* `str` with 8*n* bits\n\nUnlike with many computing libraries, there are no global variables corresponding to the available types.\nInstead, data types can simply be instantiated as needed.",
                    "signature": "(*args, **kwargs)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "Dict",
                    "qualname": "Dict",
                    "docstring": "Dictionary of `Tensor` or `phiml.math.magic.PhiTreeNode` values.\nDicts are not themselves tensors and do not have a shape.\nUse `layout()` to treat `dict` instances like tensors.\n\nIn addition to dictionary functions, supports mathematical operators with other `Dict`s and lookup via `.key` syntax.\n`Dict` implements `phiml.math.magic.PhiTreeNode` so instances can be passed to math operations like `sin`.",
                    "signature": "Error getting signature: ValueError",
                    "type": "class",
                    "methods": [
                        {
                            "name": "copy",
                            "qualname": "Dict.copy",
                            "docstring": "Return a shallow copy of the dict.",
                            "signature": "(self)"
                        }
                    ]
                },
                {
                    "name": "Diverged",
                    "qualname": "Diverged",
                    "docstring": "Raised if the optimization was stopped prematurely and cannot continue.\nThis may indicate that no solution exists.\n\nThe values of the last estimate `x` may or may not be finite.\n\nThis exception inherits from `ConvergenceException`.\n\nSee Also:\n    `NotConverged`.",
                    "signature": "(result: phiml.math._optimize.SolveInfo)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "IncompatibleShapes",
                    "qualname": "IncompatibleShapes",
                    "docstring": "Raised when the shape of a tensor does not match the other arguments.",
                    "signature": "(message, *shapes: phiml.math._shape.Shape)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "LinearFunction",
                    "qualname": "LinearFunction",
                    "docstring": "Just-in-time compiled linear function of `Tensor` arguments and return values.\n\nUse `jit_compile_linear()` to create a linear function representation.",
                    "signature": "(f, auxiliary_args: Set[str], forget_traces: bool)",
                    "type": "class",
                    "methods": [
                        {
                            "name": "sparse_matrix",
                            "qualname": "LinearFunction.sparse_matrix",
                            "docstring": "Create an explicit representation of this linear function as a sparse matrix.\n\nSee Also:\n    `sparse_matrix_and_bias()`.\n\nArgs:\n    *args: Function arguments. This determines the size of the matrix.\n    **kwargs: Additional keyword arguments for the linear function.\n\nReturns:\n    Sparse matrix representation with `values` property and `native()` method.",
                            "signature": "(self, *args, **kwargs)"
                        },
                        {
                            "name": "sparse_matrix_and_bias",
                            "qualname": "LinearFunction.sparse_matrix_and_bias",
                            "docstring": "Create an explicit representation of this affine function as a sparse matrix and a bias vector.\n\nArgs:\n    *args: Positional arguments to the linear function.\n        This determines the size of the matrix.\n    **kwargs: Additional keyword arguments for the linear function.\n\nReturns:\n    matrix: Sparse matrix representation with `values` property and `native()` method.\n    bias: `Tensor`",
                            "signature": "(self, *args, **kwargs)"
                        }
                    ]
                },
                {
                    "name": "NotConverged",
                    "qualname": "NotConverged",
                    "docstring": "Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.\n\nThis exception inherits from `ConvergenceException`.\n\nSee Also:\n    `Diverged`.",
                    "signature": "(result: phiml.math._optimize.SolveInfo)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "Shape",
                    "qualname": "Shape",
                    "docstring": "Base class for protocol classes.\n\nProtocol classes are defined as::\n\n    class Proto(Protocol):\n        def meth(self) -> int:\n            ...\n\nSuch classes are primarily used with static type checkers that recognize\nstructural subtyping (static duck-typing).\n\nFor example::\n\n    class C:\n        def meth(self) -> int:\n            return 0\n\n    def func(x: Proto) -> int:\n        return x.meth()\n\n    func(C())  # Passes static type check\n\nSee PEP 544 for details. Protocol classes decorated with\n@typing.runtime_checkable act as simple-minded runtime protocols that check\nonly the presence of given attributes, ignoring their type signatures.\nProtocol classes can be generic, they are defined as::\n\n    class GenProto[T](Protocol):\n        def meth(self) -> T:\n            ...",
                    "signature": "(*args, **kwargs)",
                    "type": "class",
                    "methods": [
                        {
                            "name": "after_gather",
                            "qualname": "Shape.after_gather",
                            "docstring": "",
                            "signature": "(self, selection: dict) -> 'Shape'"
                        },
                        {
                            "name": "as_batch",
                            "qualname": "Shape.as_batch",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of type *batch*.",
                            "signature": "(self)"
                        },
                        {
                            "name": "as_channel",
                            "qualname": "Shape.as_channel",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of type *channel*.",
                            "signature": "(self)"
                        },
                        {
                            "name": "as_dual",
                            "qualname": "Shape.as_dual",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of type *dual*.",
                            "signature": "(self)"
                        },
                        {
                            "name": "as_instance",
                            "qualname": "Shape.as_instance",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of type *instance*.",
                            "signature": "(self)"
                        },
                        {
                            "name": "as_spatial",
                            "qualname": "Shape.as_spatial",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of type *spatial*.",
                            "signature": "(self)"
                        },
                        {
                            "name": "as_type",
                            "qualname": "Shape.as_type",
                            "docstring": "Returns a copy of this `Shape` with all dimensions of the given type, either `batch`, `dual`, `spatial`, `instance`, or `channel` .",
                            "signature": "(self, new_type: Callable)"
                        },
                        {
                            "name": "flipped",
                            "qualname": "Shape.flipped",
                            "docstring": "",
                            "signature": "(self, dims: Union[List[str], Tuple[str]])"
                        },
                        {
                            "name": "get_dim_type",
                            "qualname": "Shape.get_dim_type",
                            "docstring": "Args:\n    dim: Dimension, either as name `str` or single-dimension `Shape`.\n\nReturns:\n    Dimension type, one of `batch`, `spatial`, `instance`, `channel`.",
                            "signature": "(self, dim: str) -> str"
                        },
                        {
                            "name": "get_labels",
                            "qualname": "Shape.get_labels",
                            "docstring": "Args:\n    fallback_spatial: If `True` and no labels are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as labels along `dim` in the order they are listed in this `Shape`.\n    dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.\n\nReturns:\n    Item names as `tuple` or `None` if not defined.",
                            "signature": "(self, dim: Union[str, ForwardRef('Shape'), int], fallback_spatial=False) -> Optional[tuple]"
                        },
                        {
                            "name": "get_size",
                            "qualname": "Shape.get_size",
                            "docstring": "Args:\n    dim: Dimension, either as name `str` or single-dimension `Shape` or index `int`.\n    default: (Optional) If the dim does not exist, return this value instead of raising an error.\n\nReturns:\n    Size associated with `dim` as `int` or `Tensor`.",
                            "signature": "(self, dim: Union[str, ForwardRef('Shape'), int], default=None)"
                        },
                        {
                            "name": "index",
                            "qualname": "Shape.index",
                            "docstring": "Finds the index of the dimension within this `Shape`.\n\nSee Also:\n    `Shape.indices()`.\n\nArgs:\n    dim: Dimension name or single-dimension `Shape`.\n\nReturns:\n    Index as `int`.",
                            "signature": "(self, dim: Union[str, ForwardRef('Shape'), NoneType]) -> int"
                        },
                        {
                            "name": "indices",
                            "qualname": "Shape.indices",
                            "docstring": "Finds the indices of the given dimensions within this `Shape`.\n\nSee Also:\n    `Shape.index()`.\n\nArgs:\n    names: Sequence of dim names as `tuple` or `list`. No name can occur in `names` more than once.\n\nReturns:\n    Indices as `tuple[int]`.",
                            "signature": "(self, names: Sequence[str]) -> Tuple[int, ...]"
                        },
                        {
                            "name": "is_compatible",
                            "qualname": "Shape.is_compatible",
                            "docstring": "Checks if this shape and the others can be broadcast.\n\nArgs:\n    others: Other shapes.\n\nReturns:\n    `True` only if all shapes are compatible.",
                            "signature": "(self, *others: 'Shape') -> bool"
                        },
                        {
                            "name": "isdisjoint",
                            "qualname": "Shape.isdisjoint",
                            "docstring": "Shapes are disjoint if all dimension names of one shape do not occur in the other shape.",
                            "signature": "(self, other: Union[ForwardRef('Shape'), tuple, list, str])"
                        },
                        {
                            "name": "mask",
                            "qualname": "Shape.mask",
                            "docstring": "Returns a binary sequence corresponding to the names of this Shape.\nA value of 1 means that a dimension of this Shape is contained in `names`.\n\nArgs:\n  names: instance of dimension\n  names: tuple or list or set:\n\nReturns:\n  binary sequence",
                            "signature": "(self, names: Union[tuple, list, set, ForwardRef('Shape')])"
                        },
                        {
                            "name": "meshgrid",
                            "qualname": "Shape.meshgrid",
                            "docstring": "Builds a sequence containing all multi-indices within a tensor of this shape.\nAll indices are returned as `dict` mapping dimension names to `int` indices.\n\nThe corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.\n\nThis function currently only supports uniform tensors.\n\nArgs:\n    names: If `True`, replace indices by their labels if available.\n\nReturns:\n    `dict` iterator.",
                            "signature": "(self, names=False)"
                        },
                        {
                            "name": "only",
                            "qualname": "Shape.only",
                            "docstring": "Builds a new shape from this one that only contains the given dimensions.\nDimensions in `dims` that are not part of this Shape are ignored.\n\nThe complementary operation is :func:`Shape.without`.\n\nArgs:\n  dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.\n  reorder: If `False`, keeps the dimension order as defined in this shape.\n    If `True`, reorders the dimensions of this shape to match the order of `dims`.\n\nReturns:\n  Shape containing only specified dimensions",
                            "signature": "(self, dims: 'DimFilter', reorder=False) -> 'Shape'"
                        },
                        {
                            "name": "prepare_gather",
                            "qualname": "Shape.prepare_gather",
                            "docstring": "Parse a slice object for a specific dimension.\n\nArgs:\n    dim: Name of dimension to slice.\n    selection: Slice object.\n\nReturns:",
                            "signature": "(self, dim: str, selection: Union[slice, int, ForwardRef('Shape'), str, tuple, list]) -> Union[slice, List[int]]"
                        },
                        {
                            "name": "prepare_renaming_gather",
                            "qualname": "Shape.prepare_renaming_gather",
                            "docstring": "",
                            "signature": "(self, dim: str, selection: Union[slice, int, ForwardRef('Shape'), str, tuple, list])"
                        },
                        {
                            "name": "replace",
                            "qualname": "Shape.replace",
                            "docstring": "Returns a copy of `self` with `dims` replaced by `new`.\nDimensions that are not present in `self` are ignored.\n\nThe dimension order is preserved.\n\nArgs:\n    dims: Dimensions to replace.\n    new: New dimensions, must have same length as `dims` if `len(dims) > 1`.\n        If a `Shape` is given, replaces the dimension types and labels as well.\n\nReturns:\n    `Shape` with same rank and dimension order as `self`.",
                            "signature": "(self, dims: Union[ForwardRef('Shape'), str, tuple, list], new: 'Shape') -> 'Shape'"
                        },
                        {
                            "name": "replace_selection",
                            "qualname": "Shape.replace_selection",
                            "docstring": "Replace some of the dims of this shape.\n\nArgs:\n    names: Sequence of dim names.\n    new: Replacement dims, must have same length as `old`.\n\nReturns:\n    Copy of `self` with replaced dims.",
                            "signature": "(self, names: Sequence[str], new: 'Shape') -> 'Shape'"
                        },
                        {
                            "name": "transpose",
                            "qualname": "Shape.transpose",
                            "docstring": "",
                            "signature": "(self, dim_type: str)"
                        },
                        {
                            "name": "with_dim_size",
                            "qualname": "Shape.with_dim_size",
                            "docstring": "Returns a new `Shape` that has a different size for `dim`.\n\nArgs:\n    dim: Dimension for which to replace the size, `Shape` or `str`.\n    size: New size, `int` or `Tensor`\n\nReturns:\n    `Shape` with same names and types as `self`.",
                            "signature": "(self, dim: Union[str, ForwardRef('Shape')], size: Union[int, ForwardRef('math.Tensor'), str, tuple, list], keep_labels=True)"
                        },
                        {
                            "name": "with_size",
                            "qualname": "Shape.with_size",
                            "docstring": "Only for single-dimension shapes.\nReturns a `Shape` representing this dimension but with a different size.\n\nSee Also:\n    `Shape.with_sizes()`.\n\nArgs:\n    size: Replacement size for this dimension.\n\nReturns:\n    `Shape`",
                            "signature": "(self, size: Union[int, Sequence[str]])"
                        },
                        {
                            "name": "with_sizes",
                            "qualname": "Shape.with_sizes",
                            "docstring": "Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.\n\nSee Also:\n    `Shape.with_size()`.\n\nArgs:\n    sizes: One of\n\n        * `tuple` / `list` of same length as `self` containing replacement sizes or replacement labels.\n        * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.\n        * `int`: new size for all dimensions\n\n    keep_labels: If `False`, forgets all labels.\n        If `True`, keeps labels where the size does not change.\n\nReturns:\n    `Shape` with same names and types as `self`.",
                            "signature": "(self, sizes: Union[Sequence[int], Sequence[Tuple[str, ...]], ForwardRef('Shape'), int], keep_labels=True)"
                        },
                        {
                            "name": "without",
                            "qualname": "Shape.without",
                            "docstring": "Builds a new shape from this one that is missing all given dimensions.\nDimensions in `dims` that are not part of this Shape are ignored.\n\nThe complementary operation is `Shape.only()`.\n\nArgs:\n  dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)\n  dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.\n\nReturns:\n  Shape without specified dimensions",
                            "signature": "(self, dims: 'DimFilter') -> 'Shape'"
                        },
                        {
                            "name": "without_sizes",
                            "qualname": "Shape.without_sizes",
                            "docstring": "Returns:\n    `Shape` with all sizes undefined (`None`)",
                            "signature": "(self)"
                        }
                    ]
                },
                {
                    "name": "Solve",
                    "qualname": "Solve",
                    "docstring": "Specifies parameters and stopping criteria for solving a minimization problem or system of equations.",
                    "signature": "(method: Optional[str] = 'auto', rel_tol: Union[float, phiml.math._tensors.Tensor] = None, abs_tol: Union[float, phiml.math._tensors.Tensor] = None, x0: Union[~X, Any] = None, max_iterations: Union[int, phiml.math._tensors.Tensor] = 1000, suppress: Union[tuple, list] = (), preprocess_y: Callable = None, preprocess_y_args: tuple = (), preconditioner: Optional[str] = None, rank_deficiency: int = None, gradient_solve: Optional[ForwardRef('Solve[Y, X]')] = None)",
                    "type": "class",
                    "methods": [
                        {
                            "name": "with_defaults",
                            "qualname": "Solve.with_defaults",
                            "docstring": "",
                            "signature": "(self, mode: str)"
                        },
                        {
                            "name": "with_preprocessing",
                            "qualname": "Solve.with_preprocessing",
                            "docstring": "Adds preprocessing to this `Solve` and all corresponding gradient solves.\n\nArgs:\n    preprocess_y: Preprocessing function.\n    *args: Arguments for the preprocessing function.\n\nReturns:\n    Copy of this `Solve` with given preprocessing.",
                            "signature": "(self, preprocess_y: Callable, *args) -> 'Solve'"
                        }
                    ]
                },
                {
                    "name": "SolveInfo",
                    "qualname": "SolveInfo",
                    "docstring": "Stores information about the solution or trajectory of a solve.\n\nWhen representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.",
                    "signature": "(solve: phiml.math._optimize.Solve, x: ~X, residual: Optional[~Y], iterations: Optional[phiml.math._tensors.Tensor], function_evaluations: Optional[phiml.math._tensors.Tensor], converged: phiml.math._tensors.Tensor, diverged: phiml.math._tensors.Tensor, method: str, msg: phiml.math._tensors.Tensor, solve_time: float)",
                    "type": "class",
                    "methods": [
                        {
                            "name": "convergence_check",
                            "qualname": "SolveInfo.convergence_check",
                            "docstring": "",
                            "signature": "(self, only_warn: bool)"
                        },
                        {
                            "name": "snapshot",
                            "qualname": "SolveInfo.snapshot",
                            "docstring": "",
                            "signature": "(self, index)"
                        }
                    ]
                },
                {
                    "name": "SolveTape",
                    "qualname": "SolveTape",
                    "docstring": "Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.\nWhile a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.\n\nTo access a `SolveInfo` of a recorded solve, use\n>>> solve = Solve(method, ...)\n>>> with SolveTape() as solves:\n>>>     x = math.solve_linear(f, y, solve)\n>>> result: SolveInfo = solves[solve]  # get by Solve\n>>> result: SolveInfo = solves[0]  # get by index\n\nArgs:\n    *solves: (Optional) Select specific `solves` to be recorded.\n        If none is given, records all solves that occur within the scope of this `SolveTape`.\n    record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.",
                    "signature": "(*solves: phiml.math._optimize.Solve, record_trajectories=False)",
                    "type": "class",
                    "methods": [
                        {
                            "name": "should_record_trajectory_for",
                            "qualname": "SolveTape.should_record_trajectory_for",
                            "docstring": "",
                            "signature": "(self, solve: phiml.math._optimize.Solve)"
                        }
                    ]
                },
                {
                    "name": "Tensor",
                    "qualname": "Tensor",
                    "docstring": "Abstract base class to represent structured data of one data type.\nThis class replaces the native tensor classes `numpy.ndarray`, `torch.Tensor`, `tensorflow.Tensor` or `jax.numpy.ndarray` as the main data container in \u03a6-ML.\n\n`Tensor` instances are different from native tensors in two important ways:\n\n* The dimensions of Tensors have *names* and *types*.\n* Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.\n\nTo check whether a value is a tensor, use `isinstance(value, Tensor)`.\n\nTo construct a Tensor, use `phiml.math.tensor()`, `phiml.math.wrap()` or one of the basic tensor creation functions,\nsee https://tum-pbs.github.io/PhiML/Tensors.html .\n\nTensors are not editable.\nWhen backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.",
                    "signature": "()",
                    "type": "class",
                    "methods": [
                        {
                            "name": "dimension",
                            "qualname": "Tensor.dimension",
                            "docstring": "Returns a reference to a specific dimension of this tensor.\nThis is equivalent to the syntax `tensor.<name>`.\n\nThe dimension need not be part of the `Tensor.shape` in which case its size is 1.\n\nArgs:\n    name: dimension name\n\nReturns:\n    `TensorDim` corresponding to a dimension of this tensor",
                            "signature": "(self, name: Union[str, phiml.math._shape.Shape]) -> 'TensorDim'"
                        },
                        {
                            "name": "item",
                            "qualname": "Tensor.item",
                            "docstring": "",
                            "signature": "(self) -> <property object at 0x0000024F4764D0D0>"
                        },
                        {
                            "name": "map",
                            "qualname": "Tensor.map",
                            "docstring": "",
                            "signature": "(self, function: Callable, dims=<function shape at 0x0000024F475F3380>, range=<class 'range'>, unwrap_scalars=True, **kwargs)"
                        },
                        {
                            "name": "native",
                            "qualname": "Tensor.native",
                            "docstring": "Returns a native tensor object with the dimensions ordered according to `order`.\n\nTransposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.\nIf a dimension of the tensor is not listed in `order`, a `ValueError` is raised.\n\nAdditionally, groups of dims can be specified for `order` to pack dims.\nTo do this, pass a `tuple` or `list` of dims to be packed into one native axis. Each entry must be one of the following:\n\n* `str`: the name of one dimension that is present on `value`.\n* `Shape`: Dimensions to be packed. If `force_expand`, missing dimensions are first added, otherwise they are ignored.\n* Filter function: Packs all dimensions of this type that are present on `value`.\n* Ellipsis `...`: Packs all remaining dimensions into this slot. Can only be passed once.\n* `None` or `()`: Adds a singleton dimension.\n\nCollections of or comma-separated dims may also be used but only if all dims are present on `value`.\n\nArgs:\n    order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.\n    force_expand: If `False`, dimensions along which values are guaranteed to be constant will not be expanded to their true size but returned as singleton dimensions.\n        If `True`, repeats the tensor along missing dimensions.\n        If `False`, puts singleton dimensions where possible.\n        If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.\n\nReturns:\n    Native tensor representation, such as PyTorch tensor or NumPy array.\n\nRaises:\n    `ValueError` if the tensor cannot be transposed to match target_shape",
                            "signature": "(self, order: Union[str, tuple, list, phiml.math._shape.Shape] = None, force_expand=True)"
                        },
                        {
                            "name": "numpy",
                            "qualname": "Tensor.numpy",
                            "docstring": "Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.\n\n*Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.\nTo get a differentiable tensor, use `Tensor.native()` instead.\n\nTransposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.\nIf a dimension of the tensor is not listed in `order`, a `ValueError` is raised.\n\nIf this `Tensor` is backed by a NumPy array, a reference to this array may be returned.\n\nSee Also:\n    `phiml.math.numpy()`\n\nArgs:\n    order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.\n    force_expand: If `False`, dimensions along which values are guaranteed to be constant will not be expanded to their true size but returned as singleton dimensions.\n\nReturns:\n    NumPy representation\n\nRaises:\n    ValueError if the tensor cannot be transposed to match target_shape",
                            "signature": "(self, order: Union[str, tuple, list, phiml.math._shape.Shape] = None, force_expand=True) -> numpy.ndarray"
                        },
                        {
                            "name": "pack",
                            "qualname": "Tensor.pack",
                            "docstring": "See `pack_dims()`",
                            "signature": "(self, dims, packed_dim) -> 'Tensor[T]'"
                        },
                        {
                            "name": "print",
                            "qualname": "Tensor.print",
                            "docstring": "",
                            "signature": "(self, layout='full', float_format=None, threshold=8, include_shape=None, include_dtype=None)"
                        },
                        {
                            "name": "unpack",
                            "qualname": "Tensor.unpack",
                            "docstring": "See `unpack_dim()`",
                            "signature": "(self, dim, unpacked_dims) -> 'Tensor[T]'"
                        }
                    ]
                }
            ],
            "submodules": [
                {
                    "type": "module",
                    "name": "phiml.math.extrapolation",
                    "docstring": "Extrapolations are used for padding tensors and sampling coordinates lying outside the tensor bounds.\nStandard extrapolations are listed as global variables in this module.\n\nExtrapolations are an important part of sampled fields such as grids.\nSee the documentation at https://tum-pbs.github.io/PhiML/Fields.html#extrapolations .",
                    "functions": [
                        {
                            "name": "as_extrapolation",
                            "qualname": "as_extrapolation",
                            "docstring": "Creates an `Extrapolation` from a descriptor object.\n\nArgs:\n    obj: Extrapolation specification, one of the following:\n\n        * `Extrapolation`\n        * Primitive name as `str`: periodic, zero, one, zero-gradient, symmetric, symmetric-gradient, antisymmetric, reflect, antireflect\n        * `dict` containing exactly the keys `'normal'` and `'tangential'`\n        * `dict` mapping spatial dimension names to extrapolations\n\nReturns:\n    `Extrapolation`",
                            "signature": "(obj) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "combine_by_direction",
                            "qualname": "combine_by_direction",
                            "docstring": "Use a different extrapolation for the normal component of vector-valued tensors.\n\nArgs:\n    normal: Extrapolation for the component that is orthogonal to the boundary.\n    tangential: Extrapolation for the component that is tangential to the boundary.\n\nReturns:\n    `Extrapolation`",
                            "signature": "(normal: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor], tangential: Union[phiml.math.extrapolation.Extrapolation, float, phiml.math._tensors.Tensor]) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "combine_sides",
                            "qualname": "combine_sides",
                            "docstring": "Specify extrapolations for each side / face of a box.\n\nArgs:\n    boundary_dict: Extrapolations by boundary names.\n    **extrapolations: map from dim: str -> `Extrapolation` or `tuple` (lower, upper)\n\nReturns:\n    `Extrapolation`",
                            "signature": "(boundary_dict: Dict[str, phiml.math.extrapolation.Extrapolation] = None, **extrapolations: Union[phiml.math.extrapolation.Extrapolation, tuple, numbers.Number]) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "domain_slice",
                            "qualname": "domain_slice",
                            "docstring": "Slices a domain, similar to `ext[item]` but with additional information about the domain.\nIn some cases, `ext[item]` will fail, e.g. slicing a normal/tangential extrapolation along `vector`.\n\nArgs:\n    ext: `Extrapolation`\n    item: slicing dict\n    domain_dims: All spatial dimensions.\n\nReturns:\n    `Extrapolation`",
                            "signature": "(ext: phiml.math.extrapolation.Extrapolation, item: dict, domain_dims: Union[phiml.math._shape.Shape, tuple, list, str]) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "from_dict",
                            "qualname": "from_dict",
                            "docstring": "Loads an `Extrapolation` object from a dictionary that was created using `Extrapolation.to_dict()`.\n\nArgs:\n    dictionary: serializable dictionary holding all extrapolation properties\n\nReturns:\n    Loaded extrapolation",
                            "signature": "(dictionary: dict) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "get_normal",
                            "qualname": "get_normal",
                            "docstring": "Returns only the extrapolation for the surface normal component.",
                            "signature": "(ext: phiml.math.extrapolation.Extrapolation) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "get_tangential",
                            "qualname": "get_tangential",
                            "docstring": "Returns only the extrapolation for components tangential to the boundary surface.",
                            "signature": "(ext: phiml.math.extrapolation.Extrapolation) -> phiml.math.extrapolation.Extrapolation"
                        },
                        {
                            "name": "map",
                            "qualname": "map",
                            "docstring": "Applies a function to all leaf extrapolations in `extrapolation`.\nNon-leaves are those created by `combine_sides()` and `combine_by_direction()`.\n\nThe tree will be collapsed if possible.\n\nArgs:\n    f: Function mapping a leaf `Extrapolation` to another `Extrapolation`.\n    extrapolation: Input tree for `f`.\n\nReturns:\n    `Extrapolation`",
                            "signature": "(f: Callable[[phiml.math.extrapolation.Extrapolation], phiml.math.extrapolation.Extrapolation], extrapolation)"
                        },
                        {
                            "name": "order_by_shape",
                            "qualname": "order_by_shape",
                            "docstring": "If sequence is a dict with dimension names as keys, orders its values according to this shape.\n\nOtherwise, the sequence is returned unchanged.\n\nArgs:\n  sequence: Sequence or dict to be ordered\n  default: default value used for dimensions not contained in sequence\n\nReturns:\n  ordered sequence of values",
                            "signature": "(names: Sequence[str], sequence, default=None) -> Union[tuple, list]"
                        },
                        {
                            "name": "remove_constant_offset",
                            "qualname": "remove_constant_offset",
                            "docstring": "Removes all constant offsets from an extrapolation.\nThis also includes `NaN` values in constants (unlike `ext - ext`).\n\nArgs:\n    extrapolation: `Extrapolation` object.\n\nReturns:\n    `Extrapolation` that has no constant offsets",
                            "signature": "(extrapolation)"
                        },
                        {
                            "name": "where",
                            "qualname": "where",
                            "docstring": "Uses `true_ext` where `mask` is true and `false_ext` where mask is false.\nIf the extrapolations are not consistent in which boundary faces are uniquely determined, the result cannot be used for boundary faces.\n\nYou may also call `math.where()` instead of this function.\n\nArgs:\n    mask: `Tensor` with dimensions matching the tensor that is being padded.\n    true_ext: Extrapolation to use where `mask==True`.\n    false_ext: Extrapolation to use where `mask==False`.\n\nReturns:\n    `Extrapolation`",
                            "signature": "(mask: phiml.math._tensors.Tensor, true_ext, false_ext) -> phiml.math.extrapolation.Extrapolation"
                        }
                    ],
                    "classes": [
                        {
                            "name": "ConstantExtrapolation",
                            "qualname": "ConstantExtrapolation",
                            "docstring": "Extrapolate with a constant value.\n\nArgs:\n    pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.\n        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.",
                            "signature": "(value: Union[phiml.math._tensors.Tensor, float])",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "determines_boundary_values",
                                    "qualname": "ConstantExtrapolation.determines_boundary_values",
                                    "docstring": "Tests whether this extrapolation fully determines the values at the boundary faces of the outermost cells or elements.\nIf so, the values need not be stored along with the inside values.\n\nOverride this function instead of `valid_outer_faces()`.\n\nArgs:\n    boundary_key: Boundary name as `str`.\n\nReturns:\n    Whether the value is fully determined by the boundary and need not be stored elsewhere.",
                                    "signature": "(self, boundary_key: Union[Tuple[str, bool], str]) -> bool"
                                },
                                {
                                    "name": "is_copy_pad",
                                    "qualname": "Extrapolation.is_copy_pad",
                                    "docstring": ":return: True if all pad values are copies of existing values in the tensor to be padded",
                                    "signature": "(self, dim: str, upper_edge: bool)"
                                },
                                {
                                    "name": "is_one",
                                    "qualname": "ConstantExtrapolation.is_one",
                                    "docstring": "",
                                    "signature": "(self)"
                                },
                                {
                                    "name": "is_zero",
                                    "qualname": "ConstantExtrapolation.is_zero",
                                    "docstring": "",
                                    "signature": "(self)"
                                },
                                {
                                    "name": "pad",
                                    "qualname": "ConstantExtrapolation.pad",
                                    "docstring": "Pads a tensor using constant values.",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, widths: dict, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "pad_values",
                                    "qualname": "ConstantExtrapolation.pad_values",
                                    "docstring": "Determines the values with which the given tensor would be padded at the specified using this extrapolation.\n\nArgs:\n    value: `Tensor` to be padded.\n    width: `int > 0`: Number of cells to pad along `dimension`.\n    dim: Dimension name as `str`.\n    upper_edge: `True` for upper edge, `False` for lower edge.\n    already_padded: Used when padding a tensor with multiple extrapolations.\n        Contains all widths that have already been padded prior to this call.\n        This causes the shape of `value` to be different from the original tensor passed to `math.pad()`.\n\nReturns:\n    `Tensor` that can be concatenated to `value` along `dimension`",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, width: int, dim: str, upper_edge: bool, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "shortest_distance",
                                    "qualname": "Extrapolation.shortest_distance",
                                    "docstring": "Computes the shortest distance between two points.\nBoth points are assumed to lie within the domain\n\nArgs:\n    start: Start position.\n    end: End position.\n    domain_size: Domain side lengths as vector.\n\nReturns:\n    Shortest distance from `start` to `end`.",
                                    "signature": "(self, start: phiml.math._tensors.Tensor, end: phiml.math._tensors.Tensor, domain_size: phiml.math._tensors.Tensor)"
                                },
                                {
                                    "name": "sparse_pad_values",
                                    "qualname": "ConstantExtrapolation.sparse_pad_values",
                                    "docstring": "Determines pad values for boundary nodes of a sparsely connected graph.\n\nArgs:\n    value: `Tensor` to pad. Dense tensor containing an entry for each non-boundary node of the graph, laid out along a dual dim.\n    connectivity: Sliced graph connectivity as sparse matrix. Only the relevant entries along the primal node dim are given.\n    boundary: Boundary name to pad.\n    **kwargs: Additional provided arguments, such as `mesh`.\n\nReturns:\n    Values with which to pad `value`, laid out along the dual dim of `value`.",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, connectivity: phiml.math._tensors.Tensor, dim: str, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "spatial_gradient",
                                    "qualname": "ConstantExtrapolation.spatial_gradient",
                                    "docstring": "Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.\n\nReturns:\n    `Extrapolation` or `NotImplemented`",
                                    "signature": "(self)"
                                },
                                {
                                    "name": "to_dict",
                                    "qualname": "ConstantExtrapolation.to_dict",
                                    "docstring": "Serialize this extrapolation to a dictionary that is serializable (JSON-writable).\n\nUse `from_dict()` to restore the Extrapolation object.",
                                    "signature": "(self) -> dict"
                                },
                                {
                                    "name": "transform_coordinates",
                                    "qualname": "Extrapolation.transform_coordinates",
                                    "docstring": "If `self.is_copy_pad`, transforms outside coordinates to the index from which the value is copied.\n\nOtherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.\nCoordinates are then snapped to the valid index range.\nThis is the default implementation.\n\nArgs:\n    coordinates: integer coordinates in index space\n    shape: tensor shape\n\nReturns:\n    Transformed coordinates",
                                    "signature": "(self, coordinates: phiml.math._tensors.Tensor, shape: phiml.math._shape.Shape, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "valid_outer_faces",
                                    "qualname": "Extrapolation.valid_outer_faces",
                                    "docstring": "Use `determines_boundary_values()` instead.\n\n `(lower: bool, upper: bool)` indicating whether the values sampled at the outer-most faces of a staggered grid with this extrapolation are valid, i.e. need to be stored and are not redundant.",
                                    "signature": "(self, dim) -> Tuple[bool, bool]"
                                }
                            ]
                        },
                        {
                            "name": "Extrapolation",
                            "qualname": "Extrapolation",
                            "docstring": "Extrapolations are used to determine values of grids or other structures outside the sampled bounds.\nThey play a vital role in padding and sampling.\n\nArgs:\n    pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.\n        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.",
                            "signature": "(pad_rank)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "determines_boundary_values",
                                    "qualname": "Extrapolation.determines_boundary_values",
                                    "docstring": "Tests whether this extrapolation fully determines the values at the boundary faces of the outermost cells or elements.\nIf so, the values need not be stored along with the inside values.\n\nOverride this function instead of `valid_outer_faces()`.\n\nArgs:\n    boundary_key: Boundary name as `str`.\n\nReturns:\n    Whether the value is fully determined by the boundary and need not be stored elsewhere.",
                                    "signature": "(self, boundary_key: str) -> bool"
                                },
                                {
                                    "name": "is_copy_pad",
                                    "qualname": "Extrapolation.is_copy_pad",
                                    "docstring": ":return: True if all pad values are copies of existing values in the tensor to be padded",
                                    "signature": "(self, dim: str, upper_edge: bool)"
                                },
                                {
                                    "name": "pad",
                                    "qualname": "Extrapolation.pad",
                                    "docstring": "Pads a tensor using values from `self.pad_values()`.\n\nIf `value` is a linear tracer, assume pad_values() to produce constant values, independent of `value`.\nTo change this behavior, override this method.\n\nArgs:\n    value: `Tensor` to be padded\n    widths: `dict` mapping `dim: str -> (lower: int, upper: int)`\n    already_padded: Used when padding a tensor with multiple extrapolations.\n        Contains all widths that have already been padded prior to this call.\n        This causes the shape of `value` to be different from the original tensor passed to `math.pad()`.\n    kwargs: Additional keyword arguments for padding, passed on to `pad_values()`.\n\nReturns:\n    Padded `Tensor`",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, widths: dict, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "pad_values",
                                    "qualname": "Extrapolation.pad_values",
                                    "docstring": "Determines the values with which the given tensor would be padded at the specified using this extrapolation.\n\nArgs:\n    value: `Tensor` to be padded.\n    width: `int > 0`: Number of cells to pad along `dimension`.\n    dim: Dimension name as `str`.\n    upper_edge: `True` for upper edge, `False` for lower edge.\n    already_padded: Used when padding a tensor with multiple extrapolations.\n        Contains all widths that have already been padded prior to this call.\n        This causes the shape of `value` to be different from the original tensor passed to `math.pad()`.\n\nReturns:\n    `Tensor` that can be concatenated to `value` along `dimension`",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, width: int, dim: str, upper_edge: bool, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "shortest_distance",
                                    "qualname": "Extrapolation.shortest_distance",
                                    "docstring": "Computes the shortest distance between two points.\nBoth points are assumed to lie within the domain\n\nArgs:\n    start: Start position.\n    end: End position.\n    domain_size: Domain side lengths as vector.\n\nReturns:\n    Shortest distance from `start` to `end`.",
                                    "signature": "(self, start: phiml.math._tensors.Tensor, end: phiml.math._tensors.Tensor, domain_size: phiml.math._tensors.Tensor)"
                                },
                                {
                                    "name": "sparse_pad_values",
                                    "qualname": "Extrapolation.sparse_pad_values",
                                    "docstring": "Determines pad values for boundary nodes of a sparsely connected graph.\n\nArgs:\n    value: `Tensor` to pad. Dense tensor containing an entry for each non-boundary node of the graph, laid out along a dual dim.\n    connectivity: Sliced graph connectivity as sparse matrix. Only the relevant entries along the primal node dim are given.\n    boundary: Boundary name to pad.\n    **kwargs: Additional provided arguments, such as `mesh`.\n\nReturns:\n    Values with which to pad `value`, laid out along the dual dim of `value`.",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, connectivity: phiml.math._tensors.Tensor, boundary: str, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "spatial_gradient",
                                    "qualname": "Extrapolation.spatial_gradient",
                                    "docstring": "Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.\n\nReturns:\n    `Extrapolation` or `NotImplemented`",
                                    "signature": "(self) -> 'Extrapolation'"
                                },
                                {
                                    "name": "to_dict",
                                    "qualname": "Extrapolation.to_dict",
                                    "docstring": "Serialize this extrapolation to a dictionary that is serializable (JSON-writable).\n\nUse `from_dict()` to restore the Extrapolation object.",
                                    "signature": "(self) -> dict"
                                },
                                {
                                    "name": "transform_coordinates",
                                    "qualname": "Extrapolation.transform_coordinates",
                                    "docstring": "If `self.is_copy_pad`, transforms outside coordinates to the index from which the value is copied.\n\nOtherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.\nCoordinates are then snapped to the valid index range.\nThis is the default implementation.\n\nArgs:\n    coordinates: integer coordinates in index space\n    shape: tensor shape\n\nReturns:\n    Transformed coordinates",
                                    "signature": "(self, coordinates: phiml.math._tensors.Tensor, shape: phiml.math._shape.Shape, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "valid_outer_faces",
                                    "qualname": "Extrapolation.valid_outer_faces",
                                    "docstring": "Use `determines_boundary_values()` instead.\n\n `(lower: bool, upper: bool)` indicating whether the values sampled at the outer-most faces of a staggered grid with this extrapolation are valid, i.e. need to be stored and are not redundant.",
                                    "signature": "(self, dim) -> Tuple[bool, bool]"
                                }
                            ]
                        },
                        {
                            "name": "Undefined",
                            "qualname": "Undefined",
                            "docstring": "The extrapolation is unknown and must be replaced before usage.\nAny access to outside values will raise an AssertionError.\n\nArgs:\n    pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.\n        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.",
                            "signature": "(derived_from: phiml.math.extrapolation.Extrapolation)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "determines_boundary_values",
                                    "qualname": "Undefined.determines_boundary_values",
                                    "docstring": "Tests whether this extrapolation fully determines the values at the boundary faces of the outermost cells or elements.\nIf so, the values need not be stored along with the inside values.\n\nOverride this function instead of `valid_outer_faces()`.\n\nArgs:\n    boundary_key: Boundary name as `str`.\n\nReturns:\n    Whether the value is fully determined by the boundary and need not be stored elsewhere.",
                                    "signature": "(self, boundary_key: Union[Tuple[str, bool], str]) -> bool"
                                },
                                {
                                    "name": "is_copy_pad",
                                    "qualname": "Extrapolation.is_copy_pad",
                                    "docstring": ":return: True if all pad values are copies of existing values in the tensor to be padded",
                                    "signature": "(self, dim: str, upper_edge: bool)"
                                },
                                {
                                    "name": "pad",
                                    "qualname": "Undefined.pad",
                                    "docstring": "Pads a tensor using values from `self.pad_values()`.\n\nIf `value` is a linear tracer, assume pad_values() to produce constant values, independent of `value`.\nTo change this behavior, override this method.\n\nArgs:\n    value: `Tensor` to be padded\n    widths: `dict` mapping `dim: str -> (lower: int, upper: int)`\n    already_padded: Used when padding a tensor with multiple extrapolations.\n        Contains all widths that have already been padded prior to this call.\n        This causes the shape of `value` to be different from the original tensor passed to `math.pad()`.\n    kwargs: Additional keyword arguments for padding, passed on to `pad_values()`.\n\nReturns:\n    Padded `Tensor`",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, widths: dict, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "pad_values",
                                    "qualname": "Undefined.pad_values",
                                    "docstring": "Determines the values with which the given tensor would be padded at the specified using this extrapolation.\n\nArgs:\n    value: `Tensor` to be padded.\n    width: `int > 0`: Number of cells to pad along `dimension`.\n    dim: Dimension name as `str`.\n    upper_edge: `True` for upper edge, `False` for lower edge.\n    already_padded: Used when padding a tensor with multiple extrapolations.\n        Contains all widths that have already been padded prior to this call.\n        This causes the shape of `value` to be different from the original tensor passed to `math.pad()`.\n\nReturns:\n    `Tensor` that can be concatenated to `value` along `dimension`",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, width: int, dim: str, upper_edge: bool, already_padded: Optional[dict] = None, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "shortest_distance",
                                    "qualname": "Extrapolation.shortest_distance",
                                    "docstring": "Computes the shortest distance between two points.\nBoth points are assumed to lie within the domain\n\nArgs:\n    start: Start position.\n    end: End position.\n    domain_size: Domain side lengths as vector.\n\nReturns:\n    Shortest distance from `start` to `end`.",
                                    "signature": "(self, start: phiml.math._tensors.Tensor, end: phiml.math._tensors.Tensor, domain_size: phiml.math._tensors.Tensor)"
                                },
                                {
                                    "name": "sparse_pad_values",
                                    "qualname": "Undefined.sparse_pad_values",
                                    "docstring": "Determines pad values for boundary nodes of a sparsely connected graph.\n\nArgs:\n    value: `Tensor` to pad. Dense tensor containing an entry for each non-boundary node of the graph, laid out along a dual dim.\n    connectivity: Sliced graph connectivity as sparse matrix. Only the relevant entries along the primal node dim are given.\n    boundary: Boundary name to pad.\n    **kwargs: Additional provided arguments, such as `mesh`.\n\nReturns:\n    Values with which to pad `value`, laid out along the dual dim of `value`.",
                                    "signature": "(self, value: phiml.math._tensors.Tensor, connectivity: phiml.math._tensors.Tensor, dim: str, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "spatial_gradient",
                                    "qualname": "Undefined.spatial_gradient",
                                    "docstring": "Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.\n\nReturns:\n    `Extrapolation` or `NotImplemented`",
                                    "signature": "(self) -> 'Extrapolation'"
                                },
                                {
                                    "name": "to_dict",
                                    "qualname": "Undefined.to_dict",
                                    "docstring": "Serialize this extrapolation to a dictionary that is serializable (JSON-writable).\n\nUse `from_dict()` to restore the Extrapolation object.",
                                    "signature": "(self) -> dict"
                                },
                                {
                                    "name": "transform_coordinates",
                                    "qualname": "Extrapolation.transform_coordinates",
                                    "docstring": "If `self.is_copy_pad`, transforms outside coordinates to the index from which the value is copied.\n\nOtherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.\nCoordinates are then snapped to the valid index range.\nThis is the default implementation.\n\nArgs:\n    coordinates: integer coordinates in index space\n    shape: tensor shape\n\nReturns:\n    Transformed coordinates",
                                    "signature": "(self, coordinates: phiml.math._tensors.Tensor, shape: phiml.math._shape.Shape, **kwargs) -> phiml.math._tensors.Tensor"
                                },
                                {
                                    "name": "valid_outer_faces",
                                    "qualname": "Extrapolation.valid_outer_faces",
                                    "docstring": "Use `determines_boundary_values()` instead.\n\n `(lower: bool, upper: bool)` indicating whether the values sampled at the outer-most faces of a staggered grid with this extrapolation are valid, i.e. need to be stored and are not redundant.",
                                    "signature": "(self, dim) -> Tuple[bool, bool]"
                                }
                            ]
                        }
                    ],
                    "submodules": []
                },
                {
                    "type": "module",
                    "name": "phiml.math.magic",
                    "docstring": "Magic methods allow custom classes to be compatible with various functions defined in `phiml.math`, analogous to how implementing `__hash__` allows objects to be used with `hash()`.\nThe magic methods are grouped into purely declarative classes (interfaces) by what functionality they provide.\n\n* `Shaped` objects have a `phiml.math.Shape`.\n* `Sliceable` objects can be sliced along dimensions.\n* `Shapable` objects can additionally be reshaped.\n* `PhiTreeNode` objects can be disassembled into tensors.\n\nAll of these magic classes declared here define a custom instance checks and should not be used as superclasses.\n\nAn object implements one of the types defined here by implementing one or more of the related magic methods.\nInstance checks can be performed via `isinstance(obj, <MagicClass>)`.\n\nThis is analogous to interfaces defined in the built-in `collections` package, such as `Sized, Iterable, Hashable, Callable`.\nTo check whether `len(obj)` can be performed, you check `isinstance(obj, Sized)`.",
                    "functions": [
                        {
                            "name": "slicing_dict",
                            "qualname": "slicing_dict",
                            "docstring": "Creates a slicing `dict` from `item` where `item` is an arbitrary value passed to `__getitem__()`.\n\n`Sliceable` objects should call this function inside `__getitem__()`, passing `self` and `item`.\n\nArgs:\n    obj: Object to be sliced.\n    item: Slices.\n    existing_only: Whether to include dims in the slicing dict which are present on `obj`.\n\nReturns:\n    `dict` mapping dimension names to slices.",
                            "signature": "(obj, item, existing_only=True) -> dict"
                        }
                    ],
                    "classes": [
                        {
                            "name": "BoundDim",
                            "qualname": "BoundDim",
                            "docstring": "Represents a dimension of a sliceable object to make slicing, renaming and retyping prettier.\nAny instance of `BoundDim` is bound to the sliceable object and is immutable.\nAll operations upon the dim affect return a copy of the sliceable object.\n\n`BoundDim` objects are generally created by and for objects that are `Sliceable` (and therefore also `Shaped`).\nThese objects should declare the following method to support the `.dim` syntax:\n\n```python\nfrom phiml.math.magic import BoundDim\n\nclass MyClass:\n\n    def __getattr__(self, name: str) -> BoundDim:\n        return BoundDim(self, name)\n```\n\n**Usage**\n\n* `obj.dim.size` returns the dimension size.\n* `obj.dim.labels` returns the dimension labels.\n* `obj.dim.exists` checks whether a dimension is listed in the shape of the bound object.\n* `obj.dim[0]` picks the first element along `dim`. The shape of the result will not contain `dim`.\n* `obj.dim[1:-1]` discards the first and last element along `dim`.\n* `obj.dim.rename('new_name')` renames `dim` to `new_name`.\n* `obj.dim.as_channel()` changes the type of `dim` to *channel*.\n* `obj.dim.unstack()` un-stacks the bound value along `dim`.\n* `for slice in obj.dim` loops over all slices of `dim`.\n\nMultiple dimensions can also be chained together using `obj.dim1.dim2...`.\nThis supports the following operations:\n\n* `obj.dim1.dim2...volume` returns the product of the sizes\n* `obj.dim1.dim2...[0, -1]` takes the first element along `dim1` and the last element along `dim2`\n* `obj.dim1.dim2...pack(new_dim)` packs the dimensions into a new dimension with size equal to their volume\n* `obj.dim1.dim2...unstack()` un-stacks `obj` along multiple dimensions\n* `obj.dim1.dim2...retype(type)` Changes the type of all selected dimensions\n* `for slice in obj.dim1.dim2...` loops over all slices as if unstacking first\n\nArgs:\n    obj: `Sliceable` bound object.\n    name: Dimension name as `str`.",
                            "signature": "(obj, name: str)",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "as_batch",
                                    "qualname": "BoundDim.as_batch",
                                    "docstring": "Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*.",
                                    "signature": "(self, name: str = None)"
                                },
                                {
                                    "name": "as_channel",
                                    "qualname": "BoundDim.as_channel",
                                    "docstring": "Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*.",
                                    "signature": "(self, name: str = None)"
                                },
                                {
                                    "name": "as_dual",
                                    "qualname": "BoundDim.as_dual",
                                    "docstring": "Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*.",
                                    "signature": "(self, name: str = None)"
                                },
                                {
                                    "name": "as_instance",
                                    "qualname": "BoundDim.as_instance",
                                    "docstring": "Returns a shallow copy of the `Tensor` where the type of this dimension is *instance*.",
                                    "signature": "(self, name: str = None)"
                                },
                                {
                                    "name": "as_spatial",
                                    "qualname": "BoundDim.as_spatial",
                                    "docstring": "Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*.",
                                    "signature": "(self, name: str = None)"
                                },
                                {
                                    "name": "as_type",
                                    "qualname": "BoundDim.retype",
                                    "docstring": "Returns a shallow copy of the `Tensor` where this dimension has the specified type.\n\nSee Also:\n    `phiml.math.rename_dims()`",
                                    "signature": "(self, dim_type: Callable, **kwargs)"
                                },
                                {
                                    "name": "items",
                                    "qualname": "BoundDim.items",
                                    "docstring": "",
                                    "signature": "(self)"
                                },
                                {
                                    "name": "keys",
                                    "qualname": "BoundDim.keys",
                                    "docstring": "Allows unstacking with labels as `dict(**obj.dim)`.\n\nReturns:\n    Sequence of labels or indices.",
                                    "signature": "(self)"
                                },
                                {
                                    "name": "rename",
                                    "qualname": "BoundDim.rename",
                                    "docstring": "Returns a shallow copy of the `Tensor` where this dimension has the specified name.\n\nSee Also:\n    `phiml.math.rename_dims()`",
                                    "signature": "(self, name: str, **kwargs)"
                                },
                                {
                                    "name": "replace",
                                    "qualname": "BoundDim.replace",
                                    "docstring": "Returns a shallow copy of the `Tensor` where this dimension has been replaced by `dim`.\n\nSee Also:\n    `phiml.math.rename_dims()`",
                                    "signature": "(self, dim: phiml.math._shape.Shape, **kwargs)"
                                },
                                {
                                    "name": "retype",
                                    "qualname": "BoundDim.retype",
                                    "docstring": "Returns a shallow copy of the `Tensor` where this dimension has the specified type.\n\nSee Also:\n    `phiml.math.rename_dims()`",
                                    "signature": "(self, dim_type: Callable, **kwargs)"
                                },
                                {
                                    "name": "unpack",
                                    "qualname": "BoundDim.unpack",
                                    "docstring": "Returns a shallow copy of the `Tensor` where this dimension has been unpacked into `dims`.\n\nSee Also:\n    `phiml.math.unpack_dim()`",
                                    "signature": "(self, *dims: phiml.math._shape.Shape, **kwargs)"
                                },
                                {
                                    "name": "unstack",
                                    "qualname": "BoundDim.unstack",
                                    "docstring": "Lists the slices along this dimension as a `tuple`.\n\nArgs:\n    size: (optional) If given as `int`, this dimension can be unstacked even if it is not present on the object.\n        In that case, `size` copies of the object are returned.\n\nReturns:\n    `tuple` of `Sliceable`",
                                    "signature": "(self, size: Optional[int] = None) -> tuple"
                                }
                            ]
                        },
                        {
                            "name": "OtherMagicFunctions",
                            "qualname": "OtherMagicFunctions",
                            "docstring": "",
                            "signature": "()",
                            "type": "class",
                            "methods": []
                        },
                        {
                            "name": "PhiTreeNode",
                            "qualname": "PhiTreeNode",
                            "docstring": "\u03a6-tree nodes can be iterated over and disassembled or flattened into elementary objects, such as tensors.\n`phiml.math.Tensor` instances as well as PyTree nodes (`tuple`, `list`, `dict` with `str` keys) are \u03a6-tree nodes.\nAll data classes are also considered PhiTreeNodes as of version 2.3.\n\nFor custom classes to be considered \u03a6-tree nodes, they have to be a dataclass or implement one of the following magic methods:\n\n* `__variable_attrs__()`\n* `__value_attrs__()`\n\nDataclasses may also implement these functions to specify which attributes should be considered value / variable properties.\n\nAdditionally, \u03a6-tree nodes must override `__eq__()` to allow comparison of data-stripped (key) instances.\n\nTo check whether an object is a \u03a6-tree node, use `isinstance(obj, PhiTreeNode)`.\n\n**Usage in `phiml.math`:**\n\n\u03a6-tree nodes can be used as keys, for example in `jit_compile()`.\nThey are converted to keys by stripping all variable tensors and replacing them by a placeholder object.\nIn key mode, `__eq__()` compares all non-variable properties that might invalidate a trace when changed.\n\nDisassembly and assembly of \u03a6-tree nodes uses `phiml.math.copy_with` which will call `__with_attrs__` if implemented.",
                            "signature": "()",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "__all_attrs__",
                                    "qualname": "PhiTreeNode.__all_attrs__",
                                    "docstring": "Returns all `Tensor` or `PhiTreeNode` attribute names of `self` needed to fully describe this instance.\nFields which never hold `Tensor` instances (directly or indirectly) should not be included.\n\nThe returned attributes are used to extract tensors for serializing and un-serializing the object.\n\nAll names returned by `__variable_attrs__` and `__value_attrs__` must be included in this list.\nIf not implemented, the union of `__variable_attrs__` and `__value_attrs__` will be used instead, and dataclasses default to all fields possibly containing data.\nThe only dataclass fields excluded are those of type `Shape` or primitive types `str`, `int`, `float`, etc. and collections of these.\n\nReturns:\n    `tuple` of `str` attributes.\n        Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.",
                                    "signature": "(self) -> Tuple[str, ...]"
                                },
                                {
                                    "name": "__value_attrs__",
                                    "qualname": "PhiTreeNode.__value_attrs__",
                                    "docstring": "Returns all `Tensor` or `PhiTreeNode` attribute names of `self` that should be transformed by single-operand math operations,\nsuch as `sin()`, `exp()`.\nOptimization functions, such as `minimize` also work on value attributes, as well as most derivative-related function, e.g.\n- `jacobian()`\n- `custom_gradient()`\n\nDataclasses may instead declare the field `values: Tuple[str,...]`\n\nReturns:\n    `tuple` of `str` attributes.\n        Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.",
                                    "signature": "(self) -> Tuple[str, ...]"
                                },
                                {
                                    "name": "__variable_attrs__",
                                    "qualname": "PhiTreeNode.__variable_attrs__",
                                    "docstring": "Returns all `Tensor` or `PhiTreeNode` attribute names of `self` whose values are variable.\nVariables denote values that can change from one function call to the next or for which gradients can be recorded.\nIf this method is not implemented, all attributes returned by `__value_attrs__()` are considered variable.\n\nThe returned properties are used by the following functions:\n\n- `jit_compile()`\n- `jit_compile_linear()`\n- `stop_gradient()`\n\nDataclasses may instead declare the field `variables: Tuple[str,...]`\n\nReturns:\n    `tuple` of `str` attributes.\n        Calling `getattr(self, attr)` must return a `Tensor` or `PhiTreeNode` for all returned attributes.",
                                    "signature": "(self) -> Tuple[str, ...]"
                                },
                                {
                                    "name": "__with_attrs__",
                                    "qualname": "PhiTreeNode.__with_attrs__",
                                    "docstring": "Used by `phiml.math.copy_with`.\nCreate a copy of this object which has the `Tensor` or `PhiTreeNode` attributes contained in `attrs` replaced.\nIf this method is not implemented, tensor attributes are replaced using `setattr()`.\n\nArgs:\n    **attrs: `dict` mapping `str` attribute names to `Tensor` or `PhiTreeNode`.\n\nReturns:\n    Altered copy of `self`",
                                    "signature": "(self, **attrs)"
                                }
                            ]
                        },
                        {
                            "name": "Shapable",
                            "qualname": "Shapable",
                            "docstring": "Shapable objects can be stacked, concatenated and reshaped.\n\nTo be considered `Shapable`, objects must be `Sliceable` and `Shaped` and implement\n\n* `__stack__()` or\n* `__concat__()` and `__expand__()`.\n\nObjects should additionally implement the other magic methods for performance reasons.\n\n**Usage in `phiml.math`:**\n\nShapable objects can be used with the following functions in addition to what they inherit from being `Sliceable` and `Shaped`:\n\n* `phiml.math.stack`\n* `phiml.math.concat`\n* `phiml.math.expand`\n* `phiml.math.rename_dims`\n* `phiml.math.pack_dims`\n* `phiml.math.unpack_dim`\n* `phiml.math.flatten`\n\nAdditionally, the `phiml.math.BoundDim` syntax for dimension renaming and retyping is enabled, e.g. `obj.dim.as_channel('vector')`.",
                            "signature": "()",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "__expand__",
                                    "qualname": "Shapable.__expand__",
                                    "docstring": "Adds new dimensions to this object.\nThe value of this object is constant along the new dimensions.\n\nArgs:\n    dims: Dimensions to add.\n        They are guaranteed to not already be present in `shape(self)`.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dimensions must always work without keyword arguments.\n\nReturns:\n    New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, dims: phiml.math._shape.Shape, **kwargs) -> 'Shapable'"
                                },
                                {
                                    "name": "__flatten__",
                                    "qualname": "Shapable.__flatten__",
                                    "docstring": "Lays out all elements along a single dimension.\nThis is equivalent to packing all dimensions.\n\nArgs:\n    flat_dim: Single dimension as `Shape`.\n    flatten_batch: Whether to flatten batch dimensions as well.\n    If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dimensions must always work without keyword arguments.\n\nReturns:\n    New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, flat_dim: phiml.math._shape.Shape, flatten_batch: bool, **kwargs) -> 'Shapable'"
                                },
                                {
                                    "name": "__pack_dims__",
                                    "qualname": "Shapable.__pack_dims__",
                                    "docstring": "Compresses multiple dimensions into a single dimension by concatenating the elements.\nElements along the new dimensions are laid out according to the order of `dims`.\n\nThe type of the new dimension will be equal to the types of `dims`.\nIf `dims` have varying types, the new dimension will be a batch dimension.\n\nArgs:\n    dims: Dimensions to be compressed in the specified order.\n    packed_dim: Single-dimension `Shape`.\n    pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dimensions must always work without keyword arguments.\n\nReturns:\n    New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, dims: phiml.math._shape.Shape, packed_dim: phiml.math._shape.Shape, pos: Optional[int], **kwargs) -> 'Shapable'"
                                },
                                {
                                    "name": "__replace_dims__",
                                    "qualname": "Shapable.__replace_dims__",
                                    "docstring": "Exchange existing dimensions.\nThis can be used to rename dimensions, change dimension types or change labels.\n\nArgs:\n    dims: Dimensions to be replaced.\n    new_dims: Replacement dimensions as `Shape` with `rank == len(dims)`.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dimensions must always work without keyword arguments.\n\nReturns:\n    New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, dims: Tuple[str, ...], new_dims: phiml.math._shape.Shape, **kwargs) -> 'Shapable'"
                                },
                                {
                                    "name": "__unpack_dim__",
                                    "qualname": "Shapable.__unpack_dim__",
                                    "docstring": "Decompresses a tensor dimension by unstacking the elements along it.\nThe compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.\n\nArgs:\n    dim: Dimension to be decompressed.\n    unpacked_dims: `Shape`: Ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.\n    **kwargs: Additional keyword arguments required by specific implementations.\n        Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.\n        Adding batch dimensions must always work without keyword arguments.\n\nReturns:\n    New instance of `Shapable` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, dim: str, unpacked_dims: phiml.math._shape.Shape, **kwargs) -> 'Shapable'"
                                }
                            ]
                        },
                        {
                            "name": "Shaped",
                            "qualname": "Shaped",
                            "docstring": "To be considered shaped, an object must either implement the magic method `__shape__()` or have a valid `shape` property.\nIn either case, the returned shape must be an instance of `phiml.math.Shape`.\n\nTo check whether an object is `Shaped`, use `isinstance(obj, Shaped)`.\n\n**Usage in `phiml.math`:**\n\nThe functions `phiml.math.shape` as well as dimension filters, such as `phiml.math.spatial` or `phiml.math.non_batch` can be called on all shaped objects.\n\nSee Also:\n    `Sliceable`, `Shapable`",
                            "signature": "()",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "__shape__",
                                    "qualname": "Shaped.__shape__",
                                    "docstring": "Returns the shape of this object.\n\nAlternatively, the shape can be declared via the property `shape`.\n\nReturns:\n    `phiml.math.Shape`",
                                    "signature": "(self) -> 'Shape'"
                                }
                            ]
                        },
                        {
                            "name": "Sliceable",
                            "qualname": "Sliceable",
                            "docstring": "Objects are considered sliceable if they are `Shaped` and implement `__getitem__` as defined below.\n\nTo enable the slicing syntax `obj.dim[slice]`, implement the `__getattr__` method as defined below.\n\nClasses implementing `Sliceable` should override `__getattr__` to enable the special slicing syntax defined in `BoundDim`.\n\n**Usage in `phiml.math`:**\n\nIn addition to slicing, sliceable objects can be unstacked along one or multiple dimensions using `phiml.math.unstack`.\n\nSee Also\n    `Shapable`, `Shaped`",
                            "signature": "()",
                            "type": "class",
                            "methods": [
                                {
                                    "name": "__getitem__",
                                    "qualname": "Sliceable.__getitem__",
                                    "docstring": "Slice this object along one or multiple existing or non-existing dimensions.\n\nWhen overriding this function, make sure to first call `slicing_dict(self, item)` to sort slices by dimension.\n\nArgs:\n    item: `dict` mapping dimension names to the corresponding selections.\n        Selections can be slices, indices, tuples, labels, bool tensors, int tensors or other custom types.\n        All Sliceable object must support indexing by `int`, `slice`, `tuple`, `list`, `str`.\n\nReturns:\n    Instance of the same class (or a compatible class) as `self`.",
                                    "signature": "(self, item) -> 'Sliceable'"
                                },
                                {
                                    "name": "__unstack__",
                                    "qualname": "Sliceable.__unstack__",
                                    "docstring": "Un-stack this object along one or multiple dimensions.\nUn-stacking along multiple dimensions is equal to first packing the dimensions and then unstacking along the packed dimension.\n\nImplementing this magic method is optional but the default implementation may be slow.\n\nArgs:\n    dims: Ordered `tuple` of dimension names along which to unstack this object.\n\nReturns:\n    `tuple` of slices along `dims` or `NotImplemented` to revert to default behavior for this object.",
                                    "signature": "(self, dims: Tuple[str, ...]) -> Tuple[ForwardRef('Sliceable'), ...]"
                                }
                            ]
                        }
                    ],
                    "submodules": []
                },
                {
                    "type": "module",
                    "name": "phiml.math.perm",
                    "docstring": "Functions related to tensor permutation.",
                    "functions": [
                        {
                            "name": "all_permutations",
                            "qualname": "all_permutations",
                            "docstring": "Returns a `Tensor` containing all possible permutation indices of `dims` along `list_dim`.\n\nArgs:\n    dims: Dims along which elements are permuted.\n    list_dim: Single dim along which to list the permutations.\n    index_dim: Dim listing vector components for multi-dim permutations. Can be `None` if `dims.rank == 1`.\n    convert: Whether to convert the permutations to the default backend. If `False`, the result is backed by NumPy.\n\nReturns:\n    Permutations as a single index `Tensor`.",
                            "signature": "(dims: phiml.math._shape.Shape, list_dim=(~perm\u1d48=None), index_dim: Optional[phiml.math._shape.Shape] = (index\u1d9c=None), convert=False) -> phiml.math._tensors.Tensor"
                        },
                        {
                            "name": "optimal_perm",
                            "qualname": "optimal_perm",
                            "docstring": "Given a pair-wise cost matrix of two equal-size vectors, finds the optimal permutation to apply to one vector (corresponding to the dual dim of `cost_matrix`) in order to obtain the minimum total cost for a bijective map.\n\nArgs:\n    cost_matrix: Pair-wise cost matrix. Must be a square matrix with a dual and primal dim.\n\nReturns:\n    dual_perm: Permutation that, when applied along the vector corresponding to the dual dim in `cost_matrix`, yields the minimum cost.\n    cost: Optimal cost vector, listed along primal dim of `cost_matrix`.",
                            "signature": "(cost_matrix: phiml.math._tensors.Tensor)"
                        },
                        {
                            "name": "pick_random",
                            "qualname": "pick_random",
                            "docstring": "Pick one or multiple random entries from `value`.\n\nArgs:\n    value: Tensor or tree. When containing multiple tensors, the corresponding entries are picked on all tensors that have `dim`.\n        You can pass `range` (the type) to retrieve the picked indices.\n    dim: Dimension along which to pick random entries. `Shape` with one dim.\n    count: Number of entries to pick. When specified as a `Shape`, lists picked values along `count` instead of `dim`.\n    weight: Probability weight of each item along `dim`. Will be normalized to sum to 1.\n    same_selection_dims: Dims along which to use the same random selection for each element. All other dims except `dim` are treated as batch.\n    selections: Additional dims to generate more random subsets. These will be part of the output.\n\nReturns:\n    `Tensor` or tree equal to `value`.",
                            "signature": "(value: ~TensorOrTree, dim: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType], count: Union[int, phiml.math._shape.Shape, NoneType] = 1, weight: Optional[phiml.math._tensors.Tensor] = None, same_selection_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function non_batch at 0x0000024F475F3A60>, selections: phiml.math._shape.Shape = ()) -> ~TensorOrTree"
                        },
                        {
                            "name": "random_permutation",
                            "qualname": "random_permutation",
                            "docstring": "Generate random permutations of the integers between 0 and the size of `shape`.\n\nWhen multiple dims are given, the permutation is randomized across all of them and tensor of multi-indices is returned.\n\nBatch dims result in batches of permutations.\n\nArgs:\n    *shape: `Shape` of the result tensor, including `dims` and batches.\n    *dims: Sequence dims for an individual permutation. The total `Shape.volume` defines the maximum integer.\n        All other dims from `shape` are treated as batch.\n\nReturns:\n    `Tensor`",
                            "signature": "(*shape: Union[phiml.math._shape.Shape, Any], dims=<function non_batch at 0x0000024F475F3A60>, index_dim=(index\u1d9c=None)) -> phiml.math._tensors.Tensor"
                        }
                    ],
                    "classes": [],
                    "submodules": []
                }
            ]
        },
        {
            "type": "module",
            "name": "phiml_nn",
            "docstring": "Unified neural network library.\nIncludes\n\n* Flexible NN creation of popular architectures\n* Optimizer creation\n* Training functionality\n* Parameter access\n* Saving and loading networks and optimizer states.",
            "functions": [
                {
                    "name": "adagrad",
                    "qualname": "adagrad",
                    "docstring": "Creates an Adagrad optimizer for 'net', alias for ['torch.optim.Adagrad'](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)\nAnalogue functions exist for other learning frameworks.",
                    "signature": "(net: ~Network, learning_rate: float = 0.001, lr_decay=0.0, weight_decay=0.0, initial_accumulator_value=0.0, eps=1e-10)"
                },
                {
                    "name": "adam",
                    "qualname": "adam",
                    "docstring": "Creates an Adam optimizer for `net`, alias for [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\nAnalogue functions exist for other learning frameworks.",
                    "signature": "(net: ~Network, learning_rate: float = 0.001, betas=(0.9, 0.999), epsilon=1e-07)"
                },
                {
                    "name": "conv_classifier",
                    "qualname": "conv_classifier",
                    "docstring": "Based on VGG16.",
                    "signature": "(in_features: int, in_spatial: Union[tuple, list], num_classes: int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)"
                },
                {
                    "name": "conv_net",
                    "qualname": "conv_net",
                    "docstring": "Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.\n\nArgs:\n    in_channels: input channels of the feature map, dtype: int\n    out_channels: output channels of the feature map, dtype: int\n    layers: list or tuple of output channels for each intermediate layer between the input and final output channels, dtype: list or tuple\n    activation: activation function used within the layers, dtype: string\n    batch_norm: use of batchnorm after each conv layer, dtype: bool\n    in_spatial: spatial dimensions of the input feature map, dtype: int\n\nReturns:\n    Conv-net model as specified by input arguments",
                    "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) -> ~Network"
                },
                {
                    "name": "get_learning_rate",
                    "qualname": "get_learning_rate",
                    "docstring": "Returns the global learning rate of the given optimizer.\n\nArgs:\n    optimizer (optim.Optimizer): The optimizer whose learning rate needs to be retrieved.\n\nReturns:\n    float: The learning rate of the optimizer.",
                    "signature": "(optimizer: ~Optimizer) -> float"
                },
                {
                    "name": "get_parameters",
                    "qualname": "get_parameters",
                    "docstring": "Returns all parameters of a neural network.\n\nArgs:\n    net: Neural network.\n\nReturns:\n    `dict` mapping parameter names to `phiml.math.Tensor`s.",
                    "signature": "(net: ~Network) -> Dict[str, phiml.math._tensors.Tensor]"
                },
                {
                    "name": "invertible_net",
                    "qualname": "invertible_net",
                    "docstring": "Invertible NNs are capable of inverting the output tensor back to the input tensor initially passed.\nThese networks have far-reaching applications in predicting input parameters of a problem given its observations.\nInvertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.\n\nCurrently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or mlp blocks with in_channels = out_channels.\nThe architecture used is popularized by [\"Real NVP\"](https://arxiv.org/abs/1605.08803).\n\nInvertible nets are only implemented for PyTorch and TensorFlow.\n\nArgs:\n    num_blocks: number of coupling blocks inside the invertible net, dtype: int\n    construct_net: Function to construct one part of the neural network.\n        This network must have the same number of inputs and outputs.\n        Can be a `lambda` function or one of the following strings: `mlp, u_net, res_net, conv_net`\n    construct_kwargs: Keyword arguments passed to `construct_net`.\n\nReturns:\n    Invertible neural network model",
                    "signature": "(num_blocks: int = 3, construct_net: Union[str, Callable] = 'u_net', **construct_kwargs)"
                },
                {
                    "name": "load_state",
                    "qualname": "load_state",
                    "docstring": "Read the state of a module or optimizer from a file.\n\nSee Also:\n    `save_state()`\n\nArgs:\n    obj: `torch.Network or torch.optim.Optimizer`\n    path: File path as `str`.",
                    "signature": "(obj: Union[~Network, ~Optimizer], path: str)"
                },
                {
                    "name": "mlp",
                    "qualname": "mlp",
                    "docstring": "Fully-connected neural networks are available in \u03a6-ML via mlp().\n\nArgs:\n    in_channels: size of input layer, int\n    out_channels = size of output layer, int\n    layers: tuple of linear layers between input and output neurons, list or tuple\n    activation: activation function used within the layers, string\n    batch_norm: use of batch norm after each linear layer, bool\n\nReturns:\n    Dense net model as specified by input arguments",
                    "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm=False, activation: Union[str, Callable] = 'ReLU', softmax=False) -> ~Network"
                },
                {
                    "name": "parameter_count",
                    "qualname": "parameter_count",
                    "docstring": "Counts the number of parameters in a model.\n\nSee Also:\n    `get_parameters()`.\n\nArgs:\n    net: PyTorch model\n\nReturns:\n    Total parameter count as `int`.",
                    "signature": "(net: ~Network) -> int"
                },
                {
                    "name": "res_net",
                    "qualname": "res_net",
                    "docstring": "Similar to the conv-net, the feature map spatial size remains the same throughout the layers.\nThese networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.\nA default filter size of 3 is used in the convolutional layers.\n\nArgs:\n    in_channels: input channels of the feature map, dtype: int\n    out_channels: output channels of the feature map, dtype: int\n    layers: list or tuple of output channels for each intermediate layer between the input and final output channels, dtype: list or tuple\n    activation: activation function used within the layers, dtype: string\n    batch_norm: use of batchnorm after each conv layer, dtype: bool\n    in_spatial: spatial dimensions of the input feature map, dtype: int\n\nReturns:\n    Res-net model as specified by input arguments",
                    "signature": "(in_channels: int, out_channels: int, layers: Sequence[int], batch_norm: bool = False, activation: Union[str, type] = 'ReLU', in_spatial: Union[int, tuple] = 2, periodic=False) -> ~Network"
                },
                {
                    "name": "rmsprop",
                    "qualname": "rmsprop",
                    "docstring": "Creates an RMSProp optimizer for 'net', alias for ['torch.optim.RMSprop'](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)\nAnalogue functions exist for other learning frameworks.",
                    "signature": "(net: ~Network, learning_rate: float = 0.001, alpha=0.99, eps=1e-08, weight_decay=0.0, momentum=0.0, centered=False)"
                },
                {
                    "name": "save_state",
                    "qualname": "save_state",
                    "docstring": "Write the state of a module or optimizer to a file.\n\nSee Also:\n    `load_state()`\n\nArgs:\n    obj: `torch.Network or torch.optim.Optimizer`\n    path: File path as `str`.\n\nReturns:\n    Path to the saved file.",
                    "signature": "(obj: Union[~Network, ~Optimizer], path: str) -> str"
                },
                {
                    "name": "set_learning_rate",
                    "qualname": "set_learning_rate",
                    "docstring": "Sets the global learning rate for the given optimizer.\n\nArgs:\n    optimizer (optim.Optimizer): The optimizer whose learning rate needs to be updated.\n    learning_rate (float): The new learning rate to set.",
                    "signature": "(optimizer: ~Optimizer, learning_rate: Union[float, phiml.math._tensors.Tensor])"
                },
                {
                    "name": "sgd",
                    "qualname": "sgd",
                    "docstring": "Creates an SGD optimizer for 'net', alias for ['torch.optim.SGD'](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\nAnalogue functions exist for other learning frameworks.",
                    "signature": "(net: ~Network, learning_rate: float = 0.001, momentum=0.0, dampening=0.0, weight_decay=0.0, nesterov=False)"
                },
                {
                    "name": "train",
                    "qualname": "train",
                    "docstring": "Call `update_weights()` for each batch in a loop for each epoch.\n\nArgs:\n    name: Name of the model. This is used as a name to save the model and optimizer states.\n        If not specified, no model or optimizer states are saved.\n    model: PyTorch module or Keras model (TensorFlow).\n    optimizer: PyTorch or TensorFlow/Keras optimizer.\n    loss_fn: Loss function for training. This function should take the data as input, run the model and return the loss. It may return additional outputs, but the loss must be the first value.\n    *files_or_data: Training data or file names containing training data or a mixture of both. Files are loaded using `loader`.\n    max_epochs: Epoch limit.\n    max_iter: Iteration limit. The number of iterations depends on the batch size and the number of files.\n    max_hours: Training time limit in hours (`float`).\n    stop_on_loss: Stop training if the mean epoch loss falls below this value.\n    batch_size: Batch size for training. The batch size is limited by the number of data points in the dataset.\n    file_shape: Shape of data stored in each file.\n    dataset_dims: Which dims of the training data list training examples, as opposed to features of data points.\n    device: Device to use for training. If `None`, the default device is used.\n    drop_last: If `True`, drop the last batch if it is smaller than `batch_size`.\n    loss_kwargs: Keyword arguments passed to `loss_fn`.\n    lr_schedule_iter: Function `(i: int) -> float` that returns the learning rate for iteration `i`. If `None`, the learning rate of the `optimizer` is used as is.\n    checkpoint_frequency: If not `None`, save the model and optimizer state every `checkpoint_frequency` epochs.\n    loader: Function `(file: str) -> data: Tensor` to load data from files. Defaults to `phiml.math.load()`.\n    on_iter_end: Function `(i: int, max_iter: int, name: str, model, optimizer, learning_rate, loss, *additional_output) -> None` called after each iteration. The function is called with the current iteration number `i` starting at 0, the maximum number of iterations `max_iter`, the name of the model `name`, the model `model`, the optimizer `optimizer`, the learning rate `learning_rate`, the loss value `loss` and any additional output from `loss_fn`.\n    on_epoch_end: Function `(epoch: int, max_epochs: int, name: str, model, optimizer, learning_rate, epoch_loss) -> None` called after each epoch. The function is called with the current epoch number `epoch` starting at 0, the maximum number of epochs `max_epochs`, the name of the model `name`, the model `model`, the optimizer `optimizer`, the learning rate `learning_rate` and the average loss for the epoch `epoch_loss`.\n    measure_peak_memory: If `True`, measure the peak memory usage during training and store it in the returned `TrainingState`. This is only supported by some backends.\n\nReturns:\n    `TrainingResult` containing the termination reason, last epoch and last iteration.",
                    "signature": "(name: Optional[str], model, optimizer, loss_fn: Callable, *files_or_data: Union[str, phiml.math._tensors.Tensor], max_epochs: int = None, max_iter: int = None, max_hours: float = None, stop_on_loss: float = None, batch_size: int = 1, file_shape: phiml.math._shape.Shape = (), dataset_dims: Union[str, Sequence, set, phiml.math._shape.Shape, Callable, NoneType] = <function batch at 0x0000018E7FE33560>, device: phiml.backend._backend.ComputeDevice = None, drop_last=False, loss_kwargs=None, lr_schedule_iter=None, checkpoint_frequency=None, loader=<function load at 0x0000018E7FEA6840>, on_iter_end: Callable = None, on_epoch_end: Callable = None, measure_peak_memory: bool = True) -> phiml.nn.TrainingState"
                },
                {
                    "name": "u_net",
                    "qualname": "u_net",
                    "docstring": "Built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.\n\nArgs:\n    in_channels: input channels of the feature map, dtype: int\n    out_channels: output channels of the feature map, dtype: int\n    levels: number of levels of down-sampling and upsampling, dtype: int\n    filters: filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,\n    activation: activation function used within the layers, dtype: string\n    batch_norm: use of batchnorm after each conv layer, dtype: bool\n    in_spatial: spatial dimensions of the input feature map, dtype: int\n    use_res_blocks: use convolutional blocks with skip connections instead of regular convolutional blocks, dtype: bool\n    down_kernel_size: Kernel size for convolutions on the down-sampling (first half) side of the U-Net.\n    up_kernel_size: Kernel size for convolutions on the up-sampling (second half) of the U-Net.\n\nReturns:\n    U-net model as specified by input arguments.",
                    "signature": "(in_channels: int, out_channels: int, levels: int = 4, filters: Union[int, Sequence] = 16, batch_norm: bool = True, activation: Union[str, type] = 'ReLU', in_spatial: Union[tuple, int] = 2, periodic=False, use_res_blocks: bool = False, down_kernel_size=3, up_kernel_size=3) -> ~Network"
                },
                {
                    "name": "update_weights",
                    "qualname": "update_weights",
                    "docstring": "Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.\n\nThis is the PyTorch version. Analogue functions exist for other learning frameworks.\n\nArgs:\n    net: Learning model.\n    optimizer: Optimizer.\n    loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.\n    *loss_args: Arguments given to `loss_function`.\n    **loss_kwargs: Keyword arguments given to `loss_function`.\n\nReturns:\n    Output of `loss_function`.",
                    "signature": "(net: ~Network, optimizer: ~Optimizer, loss_function: Callable, *loss_args, **loss_kwargs)"
                }
            ],
            "classes": [
                {
                    "name": "StopTraining",
                    "qualname": "StopTraining",
                    "docstring": "This exception is raised by the `on_epoch_end` or `on_iter_end` callbacks to stop training.",
                    "signature": "(reason: str = 'stop')",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "TrainingState",
                    "qualname": "TrainingState",
                    "docstring": "TrainingState(name: str, model: ~Network, optimizer: ~Optimizer, learning_rate: float, epoch: int, max_epochs: Optional[int], iter: int, max_iter: Optional[int], is_epoch_end: bool, epoch_loss: phiml.math._tensors.Tensor, batch_loss: Optional[phiml.math._tensors.Tensor], additional_batch_output: Optional[tuple], indices: phiml.math._tensors.Tensor, termination_reason: Optional[str], peak_memory: Optional[int])",
                    "signature": "(name: str, model: ~Network, optimizer: ~Optimizer, learning_rate: float, epoch: int, max_epochs: Optional[int], iter: int, max_iter: Optional[int], is_epoch_end: bool, epoch_loss: phiml.math._tensors.Tensor, batch_loss: Optional[phiml.math._tensors.Tensor], additional_batch_output: Optional[tuple], indices: phiml.math._tensors.Tensor, termination_reason: Optional[str], peak_memory: Optional[int]) -> None",
                    "type": "class",
                    "methods": []
                }
            ],
            "submodules": []
        },
        {
            "type": "module",
            "name": "phiml_os",
            "docstring": "Adding vectorization to `os` functions.\n\nExample:\n    >>> from phiml import os\n    >>> dirs = list_directories('root', startswith='data_')\n    >>> files = list_files(dirs, endswith='.h5')\n    >>> which_exist = os.path.exists(files)",
            "functions": [
                {
                    "name": "_exit",
                    "qualname": "_exit",
                    "docstring": "Exit to the system with specified status, without normal exit processing.",
                    "signature": "(status)"
                },
                {
                    "name": "abort",
                    "qualname": "abort",
                    "docstring": "Abort the interpreter immediately.\n\nThis function 'dumps core' or otherwise fails in the hardest way possible\non the hosting operating system.  This function never returns.",
                    "signature": "()"
                },
                {
                    "name": "access",
                    "qualname": "access",
                    "docstring": "Use the real uid/gid to test for access to a path.\n\n  path\n    Path to be tested; can be string, bytes, or a path-like object.\n  mode\n    Operating-system mode bitfield.  Can be F_OK to test existence,\n    or the inclusive-OR of R_OK, W_OK, and X_OK.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be relative; path will then be relative to that\n    directory.\n  effective_ids\n    If True, access will use the effective uid/gid instead of\n    the real uid/gid.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    access will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd, effective_ids, and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nNote that most operations will use the effective uid/gid, therefore this\n  routine can be used in a suid/sgid environment to test if the invoking user\n  has the specified access to the path.",
                    "signature": "(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True)"
                },
                {
                    "name": "chdir",
                    "qualname": "chdir",
                    "docstring": "Change the current working directory to the specified path.\n\npath may always be specified as a string.\nOn some platforms, path may also be specified as an open file descriptor.\nIf this functionality is unavailable, using it raises an exception.",
                    "signature": "(path)"
                },
                {
                    "name": "chmod",
                    "qualname": "chmod",
                    "docstring": "Change the access permissions of a file.\n\n  path\n    Path to be modified.  May always be specified as a str, bytes, or a path-like object.\n    On some platforms, path may also be specified as an open file descriptor.\n    If this functionality is unavailable, using it raises an exception.\n  mode\n    Operating-system mode bitfield.\n    Be careful when using number literals for *mode*. The conventional UNIX notation for\n    numeric modes uses an octal base, which needs to be indicated with a ``0o`` prefix in\n    Python.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be relative; path will then be relative to that\n    directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    chmod will modify the symbolic link itself instead of the file\n    the link points to.\n\nIt is an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.\ndir_fd and follow_symlinks may not be implemented on your platform.\n  If they are unavailable, using them will raise a NotImplementedError.",
                    "signature": "Error getting signature: ValueError"
                },
                {
                    "name": "close",
                    "qualname": "close",
                    "docstring": "Close a file descriptor.",
                    "signature": "(fd)"
                },
                {
                    "name": "closerange",
                    "qualname": "closerange",
                    "docstring": "Closes all file descriptors in [fd_low, fd_high), ignoring errors.",
                    "signature": "(fd_low, fd_high, /)"
                },
                {
                    "name": "cpu_count",
                    "qualname": "cpu_count",
                    "docstring": "Return the number of logical CPUs in the system.\n\nReturn None if indeterminable.",
                    "signature": "()"
                },
                {
                    "name": "device_encoding",
                    "qualname": "device_encoding",
                    "docstring": "Return a string describing the encoding of a terminal's file descriptor.\n\nThe file descriptor must be attached to a terminal.\nIf the device is not a terminal, return None.",
                    "signature": "(fd)"
                },
                {
                    "name": "dup",
                    "qualname": "dup",
                    "docstring": "Return a duplicate of a file descriptor.",
                    "signature": "(fd, /)"
                },
                {
                    "name": "dup2",
                    "qualname": "dup2",
                    "docstring": "Duplicate file descriptor.",
                    "signature": "(fd, fd2, inheritable=True)"
                },
                {
                    "name": "execl",
                    "qualname": "execl",
                    "docstring": "execl(file, *args)\n\nExecute the executable file with argument list args, replacing the\ncurrent process.",
                    "signature": "(file, *args)"
                },
                {
                    "name": "execle",
                    "qualname": "execle",
                    "docstring": "execle(file, *args, env)\n\nExecute the executable file with argument list args and\nenvironment env, replacing the current process.",
                    "signature": "(file, *args)"
                },
                {
                    "name": "execlp",
                    "qualname": "execlp",
                    "docstring": "execlp(file, *args)\n\nExecute the executable file (which is searched for along $PATH)\nwith argument list args, replacing the current process.",
                    "signature": "(file, *args)"
                },
                {
                    "name": "execlpe",
                    "qualname": "execlpe",
                    "docstring": "execlpe(file, *args, env)\n\nExecute the executable file (which is searched for along $PATH)\nwith argument list args and environment env, replacing the current\nprocess.",
                    "signature": "(file, *args)"
                },
                {
                    "name": "execv",
                    "qualname": "execv",
                    "docstring": "Execute an executable path with arguments, replacing current process.\n\npath\n  Path of executable file.\nargv\n  Tuple or list of strings.",
                    "signature": "(path, argv, /)"
                },
                {
                    "name": "execve",
                    "qualname": "execve",
                    "docstring": "Execute an executable path with arguments, replacing current process.\n\npath\n  Path of executable file.\nargv\n  Tuple or list of strings.\nenv\n  Dictionary of strings mapping to strings.",
                    "signature": "(path, argv, env)"
                },
                {
                    "name": "execvp",
                    "qualname": "execvp",
                    "docstring": "execvp(file, args)\n\nExecute the executable file (which is searched for along $PATH)\nwith argument list args, replacing the current process.\nargs may be a list or tuple of strings.",
                    "signature": "(file, args)"
                },
                {
                    "name": "execvpe",
                    "qualname": "execvpe",
                    "docstring": "execvpe(file, args, env)\n\nExecute the executable file (which is searched for along $PATH)\nwith argument list args and environment env, replacing the\ncurrent process.\nargs may be a list or tuple of strings.",
                    "signature": "(file, args, env)"
                },
                {
                    "name": "fchmod",
                    "qualname": "fchmod",
                    "docstring": "Change the access permissions of the file given by file descriptor fd.\n\n  fd\n    The file descriptor of the file to be modified.\n  mode\n    Operating-system mode bitfield.\n    Be careful when using number literals for *mode*. The conventional UNIX notation for\n    numeric modes uses an octal base, which needs to be indicated with a ``0o`` prefix in\n    Python.\n\nEquivalent to os.chmod(fd, mode).",
                    "signature": "(fd, mode)"
                },
                {
                    "name": "fdopen",
                    "qualname": "fdopen",
                    "docstring": "",
                    "signature": "(fd, mode='r', buffering=-1, encoding=None, *args, **kwargs)"
                },
                {
                    "name": "fsdecode",
                    "qualname": "_fscodec.<locals>.fsdecode",
                    "docstring": "Decode filename (an os.PathLike, bytes, or str) from the filesystem\nencoding with 'surrogateescape' error handler, return str unchanged. On\nWindows, use 'strict' error handler if the file system encoding is\n'mbcs' (which is the default encoding).",
                    "signature": "(filename)"
                },
                {
                    "name": "fsencode",
                    "qualname": "_fscodec.<locals>.fsencode",
                    "docstring": "Encode filename (an os.PathLike, bytes, or str) to the filesystem\nencoding with 'surrogateescape' error handler, return bytes unchanged.\nOn Windows, use 'strict' error handler if the file system encoding is\n'mbcs' (which is the default encoding).",
                    "signature": "(filename)"
                },
                {
                    "name": "fspath",
                    "qualname": "fspath",
                    "docstring": "Return the file system path representation of the object.\n\nIf the object is str or bytes, then allow it to pass through as-is. If the\nobject defines __fspath__(), then return the result of that method. All other\ntypes raise a TypeError.",
                    "signature": "(path)"
                },
                {
                    "name": "fstat",
                    "qualname": "fstat",
                    "docstring": "Perform a stat system call on the given file descriptor.\n\nLike stat(), but for an open file descriptor.\nEquivalent to os.stat(fd).",
                    "signature": "(fd)"
                },
                {
                    "name": "fsync",
                    "qualname": "fsync",
                    "docstring": "Force write of fd to disk.",
                    "signature": "(fd)"
                },
                {
                    "name": "ftruncate",
                    "qualname": "ftruncate",
                    "docstring": "Truncate a file, specified by file descriptor, to a specific length.",
                    "signature": "(fd, length, /)"
                },
                {
                    "name": "get_blocking",
                    "qualname": "get_blocking",
                    "docstring": "Get the blocking mode of the file descriptor.\n\nReturn False if the O_NONBLOCK flag is set, True if the flag is cleared.",
                    "signature": "(fd, /)"
                },
                {
                    "name": "get_exec_path",
                    "qualname": "get_exec_path",
                    "docstring": "Returns the sequence of directories that will be searched for the\nnamed executable (similar to a shell) when launching a process.\n\n*env* must be an environment variable dict or None.  If *env* is None,\nos.environ will be used.",
                    "signature": "(env=None)"
                },
                {
                    "name": "get_handle_inheritable",
                    "qualname": "get_handle_inheritable",
                    "docstring": "Get the close-on-exe flag of the specified file descriptor.",
                    "signature": "(handle, /)"
                },
                {
                    "name": "get_inheritable",
                    "qualname": "get_inheritable",
                    "docstring": "Get the close-on-exe flag of the specified file descriptor.",
                    "signature": "(fd, /)"
                },
                {
                    "name": "get_terminal_size",
                    "qualname": "get_terminal_size",
                    "docstring": "Return the size of the terminal window as (columns, lines).\n\nThe optional argument fd (default standard output) specifies\nwhich file descriptor should be queried.\n\nIf the file descriptor is not connected to a terminal, an OSError\nis thrown.\n\nThis function will only be defined if an implementation is\navailable for this system.\n\nshutil.get_terminal_size is the high-level function which should\nnormally be used, os.get_terminal_size is the low-level implementation.",
                    "signature": "Error getting signature: ValueError"
                },
                {
                    "name": "getcwd",
                    "qualname": "getcwd",
                    "docstring": "Return a unicode string representing the current working directory.",
                    "signature": "()"
                },
                {
                    "name": "getcwdb",
                    "qualname": "getcwdb",
                    "docstring": "Return a bytes string representing the current working directory.",
                    "signature": "()"
                },
                {
                    "name": "getenv",
                    "qualname": "getenv",
                    "docstring": "Get an environment variable, return None if it doesn't exist.\nThe optional second argument can specify an alternate default.\nkey, default and the result are str.",
                    "signature": "(key, default=None)"
                },
                {
                    "name": "getlogin",
                    "qualname": "getlogin",
                    "docstring": "Return the actual login name.",
                    "signature": "()"
                },
                {
                    "name": "getpid",
                    "qualname": "getpid",
                    "docstring": "Return the current process id.",
                    "signature": "()"
                },
                {
                    "name": "getppid",
                    "qualname": "getppid",
                    "docstring": "Return the parent's process id.\n\nIf the parent process has already exited, Windows machines will still\nreturn its id; others systems will return the id of the 'init' process (1).",
                    "signature": "()"
                },
                {
                    "name": "isatty",
                    "qualname": "isatty",
                    "docstring": "Return True if the fd is connected to a terminal.\n\nReturn True if the file descriptor is an open file descriptor\nconnected to the slave end of a terminal.",
                    "signature": "(fd, /)"
                },
                {
                    "name": "kill",
                    "qualname": "kill",
                    "docstring": "Kill a process with a signal.",
                    "signature": "(pid, signal, /)"
                },
                {
                    "name": "lchmod",
                    "qualname": "lchmod",
                    "docstring": "Change the access permissions of a file, without following symbolic links.\n\nIf path is a symlink, this affects the link itself rather than the target.\nEquivalent to chmod(path, mode, follow_symlinks=False).\"",
                    "signature": "(path, mode)"
                },
                {
                    "name": "link",
                    "qualname": "link",
                    "docstring": "Create a hard link to a file.\n\nIf either src_dir_fd or dst_dir_fd is not None, it should be a file\n  descriptor open to a directory, and the respective path string (src or dst)\n  should be relative; the path will then be relative to that directory.\nIf follow_symlinks is False, and the last element of src is a symbolic\n  link, link will create a link to the symbolic link itself instead of the\n  file the link points to.\nsrc_dir_fd, dst_dir_fd, and follow_symlinks may not be implemented on your\n  platform.  If they are unavailable, using them will raise a\n  NotImplementedError.",
                    "signature": "(src, dst, *, src_dir_fd=None, dst_dir_fd=None, follow_symlinks=True)"
                },
                {
                    "name": "listdir",
                    "qualname": "listdir",
                    "docstring": "Returns a `Tensor` of all entries in the directory or directory batch given by `path`.\n\nArgs:\n    path: Single directory as `str` or multiple directories as string `Tensor`.\n    list_dim: Dim along which to list entries.\n    file_filter: (Optional) Function with signature `(directory: str, filename: str) -> bool` used for filtering the files.\n    full_paths: Whether to return the full paths or just the file names.\n\nReturns:\n    `Tensor` with all dims of `path` and `list_dim`. If directories contain different numbers of entries, the result `Tensor` will be non-uniform.",
                    "signature": "(path: Union[str, phiml.math._tensors.Tensor], list_dim: phiml.math._shape.Shape = (files\u1d47=None), file_filter: Callable[[str, str], bool] = None, full_paths=False) -> phiml.math._tensors.Tensor"
                },
                {
                    "name": "listdrives",
                    "qualname": "listdrives",
                    "docstring": "Return a list containing the names of drives in the system.\n\nA drive name typically looks like 'C:\\\\'.",
                    "signature": "()"
                },
                {
                    "name": "listmounts",
                    "qualname": "listmounts",
                    "docstring": "Return a list containing mount points for a particular volume.\n\n'volume' should be a GUID path as returned from os.listvolumes.",
                    "signature": "(volume)"
                },
                {
                    "name": "listvolumes",
                    "qualname": "listvolumes",
                    "docstring": "Return a list containing the volumes in the system.\n\nVolumes are typically represented as a GUID path.",
                    "signature": "()"
                },
                {
                    "name": "lseek",
                    "qualname": "lseek",
                    "docstring": "Set the position of a file descriptor.  Return the new position.\n\n  fd\n    An open file descriptor, as returned by os.open().\n  position\n    Position, interpreted relative to 'whence'.\n  whence\n    The relative position to seek from. Valid values are:\n    - SEEK_SET: seek from the start of the file.\n    - SEEK_CUR: seek from the current file position.\n    - SEEK_END: seek from the end of the file.\n\nThe return value is the number of bytes relative to the beginning of the file.",
                    "signature": "(fd, position, whence, /)"
                },
                {
                    "name": "lstat",
                    "qualname": "lstat",
                    "docstring": "Perform a stat system call on the given path, without following symbolic links.\n\nLike stat(), but do not follow symbolic links.\nEquivalent to stat(path, follow_symlinks=False).",
                    "signature": "(path, *, dir_fd=None)"
                },
                {
                    "name": "makedirs",
                    "qualname": "makedirs",
                    "docstring": "makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.",
                    "signature": "(name, mode=511, exist_ok=False)"
                },
                {
                    "name": "mkdir",
                    "qualname": "mkdir",
                    "docstring": "Create a directory.\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.\n\nThe mode argument is ignored on Windows. Where it is used, the current umask\nvalue is first masked out.",
                    "signature": "(path, mode=511, *, dir_fd=None)"
                },
                {
                    "name": "open",
                    "qualname": "open",
                    "docstring": "Open a file for low level IO.  Returns a file descriptor (integer).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.",
                    "signature": "(path, flags, mode=511, *, dir_fd=None)"
                },
                {
                    "name": "pipe",
                    "qualname": "pipe",
                    "docstring": "Create a pipe.\n\nReturns a tuple of two file descriptors:\n  (read_fd, write_fd)",
                    "signature": "()"
                },
                {
                    "name": "popen",
                    "qualname": "popen",
                    "docstring": "",
                    "signature": "(cmd, mode='r', buffering=-1)"
                },
                {
                    "name": "putenv",
                    "qualname": "putenv",
                    "docstring": "Change or add an environment variable.",
                    "signature": "(name, value, /)"
                },
                {
                    "name": "read",
                    "qualname": "read",
                    "docstring": "Read from a file descriptor.  Returns a bytes object.",
                    "signature": "(fd, length, /)"
                },
                {
                    "name": "readlink",
                    "qualname": "readlink",
                    "docstring": "Return a string representing the path to which the symbolic link points.\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\nand path should be relative; path will then be relative to that directory.\n\ndir_fd may not be implemented on your platform.  If it is unavailable,\nusing it will raise a NotImplementedError.",
                    "signature": "(path, *, dir_fd=None)"
                },
                {
                    "name": "remove",
                    "qualname": "remove",
                    "docstring": "Remove a file (same as unlink()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.",
                    "signature": "(path, *, dir_fd=None)"
                },
                {
                    "name": "removedirs",
                    "qualname": "removedirs",
                    "docstring": "removedirs(name)\n\nSuper-rmdir; remove a leaf directory and all empty intermediate\nones.  Works like rmdir except that, if the leaf directory is\nsuccessfully removed, directories corresponding to rightmost path\nsegments will be pruned away until either the whole path is\nconsumed or an error occurs.  Errors during this latter phase are\nignored -- they generally mean that a directory was not empty.",
                    "signature": "(name)"
                },
                {
                    "name": "rename",
                    "qualname": "rename",
                    "docstring": "Rename a file or directory.\n\nIf either src_dir_fd or dst_dir_fd is not None, it should be a file\n  descriptor open to a directory, and the respective path string (src or dst)\n  should be relative; the path will then be relative to that directory.\nsrc_dir_fd and dst_dir_fd, may not be implemented on your platform.\n  If they are unavailable, using them will raise a NotImplementedError.",
                    "signature": "(src, dst, *, src_dir_fd=None, dst_dir_fd=None)"
                },
                {
                    "name": "renames",
                    "qualname": "renames",
                    "docstring": "renames(old, new)\n\nSuper-rename; create directories as necessary and delete any left\nempty.  Works like rename, except creation of any intermediate\ndirectories needed to make the new pathname good is attempted\nfirst.  After the rename, directories corresponding to rightmost\npath segments of the old name will be pruned until either the\nwhole path is consumed or a nonempty directory is found.\n\nNote: this function can fail with the new directory structure made\nif you lack permissions needed to unlink the leaf directory or\nfile.",
                    "signature": "(old, new)"
                },
                {
                    "name": "replace",
                    "qualname": "replace",
                    "docstring": "Rename a file or directory, overwriting the destination.\n\nIf either src_dir_fd or dst_dir_fd is not None, it should be a file\n  descriptor open to a directory, and the respective path string (src or dst)\n  should be relative; the path will then be relative to that directory.\nsrc_dir_fd and dst_dir_fd, may not be implemented on your platform.\n  If they are unavailable, using them will raise a NotImplementedError.",
                    "signature": "(src, dst, *, src_dir_fd=None, dst_dir_fd=None)"
                },
                {
                    "name": "rmdir",
                    "qualname": "rmdir",
                    "docstring": "Remove a directory.\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.",
                    "signature": "(path, *, dir_fd=None)"
                },
                {
                    "name": "scandir",
                    "qualname": "scandir",
                    "docstring": "Return an iterator of DirEntry objects for given path.\n\npath can be specified as either str, bytes, or a path-like object.  If path\nis bytes, the names of yielded DirEntry objects will also be bytes; in\nall other circumstances they will be str.\n\nIf path is None, uses the path='.'.",
                    "signature": "(path=None)"
                },
                {
                    "name": "set_blocking",
                    "qualname": "set_blocking",
                    "docstring": "Set the blocking mode of the specified file descriptor.\n\nSet the O_NONBLOCK flag if blocking is False,\nclear the O_NONBLOCK flag otherwise.",
                    "signature": "(fd, blocking, /)"
                },
                {
                    "name": "set_handle_inheritable",
                    "qualname": "set_handle_inheritable",
                    "docstring": "Set the inheritable flag of the specified handle.",
                    "signature": "(handle, inheritable, /)"
                },
                {
                    "name": "set_inheritable",
                    "qualname": "set_inheritable",
                    "docstring": "Set the inheritable flag of the specified file descriptor.",
                    "signature": "(fd, inheritable, /)"
                },
                {
                    "name": "spawnl",
                    "qualname": "spawnl",
                    "docstring": "spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it.",
                    "signature": "(mode, file, *args)"
                },
                {
                    "name": "spawnle",
                    "qualname": "spawnle",
                    "docstring": "spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it.",
                    "signature": "(mode, file, *args)"
                },
                {
                    "name": "spawnv",
                    "qualname": "spawnv",
                    "docstring": "Execute the program specified by path in a new process.\n\nmode\n  Mode of process creation.\npath\n  Path of executable file.\nargv\n  Tuple or list of strings.",
                    "signature": "(mode, path, argv, /)"
                },
                {
                    "name": "spawnve",
                    "qualname": "spawnve",
                    "docstring": "Execute the program specified by path in a new process.\n\nmode\n  Mode of process creation.\npath\n  Path of executable file.\nargv\n  Tuple or list of strings.\nenv\n  Dictionary of strings mapping to strings.",
                    "signature": "(mode, path, argv, env, /)"
                },
                {
                    "name": "startfile",
                    "qualname": "startfile",
                    "docstring": "Start a file with its associated application.\n\nWhen \"operation\" is not specified or \"open\", this acts like\ndouble-clicking the file in Explorer, or giving the file name as an\nargument to the DOS \"start\" command: the file is opened with whatever\napplication (if any) its extension is associated.\nWhen another \"operation\" is given, it specifies what should be done with\nthe file.  A typical operation is \"print\".\n\n\"arguments\" is passed to the application, but should be omitted if the\nfile is a document.\n\n\"cwd\" is the working directory for the operation. If \"filepath\" is\nrelative, it will be resolved against this directory. This argument\nshould usually be an absolute path.\n\n\"show_cmd\" can be used to override the recommended visibility option.\nSee the Windows ShellExecute documentation for values.\n\nstartfile returns as soon as the associated application is launched.\nThere is no option to wait for the application to close, and no way\nto retrieve the application's exit status.\n\nThe filepath is relative to the current directory.  If you want to use\nan absolute path, make sure the first character is not a slash (\"/\");\nthe underlying Win32 ShellExecute function doesn't work if it is.",
                    "signature": "Error getting signature: ValueError"
                },
                {
                    "name": "stat",
                    "qualname": "stat",
                    "docstring": "Perform a stat system call on the given path.\n\n  path\n    Path to be examined; can be string, bytes, a path-like object or\n    open-file-descriptor int.\n  dir_fd\n    If not None, it should be a file descriptor open to a directory,\n    and path should be a relative string; path will then be relative to\n    that directory.\n  follow_symlinks\n    If False, and the last element of the path is a symbolic link,\n    stat will examine the symbolic link itself instead of the file\n    the link points to.\n\ndir_fd and follow_symlinks may not be implemented\n  on your platform.  If they are unavailable, using them will raise a\n  NotImplementedError.\n\nIt's an error to use dir_fd or follow_symlinks when specifying path as\n  an open file descriptor.",
                    "signature": "(path, *, dir_fd=None, follow_symlinks=True)"
                },
                {
                    "name": "strerror",
                    "qualname": "strerror",
                    "docstring": "Translate an error code to a message string.",
                    "signature": "(code, /)"
                },
                {
                    "name": "symlink",
                    "qualname": "symlink",
                    "docstring": "Create a symbolic link pointing to src named dst.\n\ntarget_is_directory is required on Windows if the target is to be\n  interpreted as a directory.  (On Windows, symlink requires\n  Windows 6.0 or greater, and raises a NotImplementedError otherwise.)\n  target_is_directory is ignored on non-Windows platforms.\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.",
                    "signature": "(src, dst, target_is_directory=False, *, dir_fd=None)"
                },
                {
                    "name": "system",
                    "qualname": "system",
                    "docstring": "Execute the command in a subshell.",
                    "signature": "(command)"
                },
                {
                    "name": "times",
                    "qualname": "times",
                    "docstring": "Return a collection containing process timing information.\n\nThe object returned behaves like a named tuple with these fields:\n  (utime, stime, cutime, cstime, elapsed_time)\nAll fields are floating-point numbers.",
                    "signature": "()"
                },
                {
                    "name": "truncate",
                    "qualname": "truncate",
                    "docstring": "Truncate a file, specified by path, to a specific length.\n\nOn some platforms, path may also be specified as an open file descriptor.\n  If this functionality is unavailable, using it raises an exception.",
                    "signature": "(path, length)"
                },
                {
                    "name": "umask",
                    "qualname": "umask",
                    "docstring": "Set the current numeric umask and return the previous umask.",
                    "signature": "(mask, /)"
                },
                {
                    "name": "unlink",
                    "qualname": "unlink",
                    "docstring": "Remove a file (same as remove()).\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\ndir_fd may not be implemented on your platform.\n  If it is unavailable, using it will raise a NotImplementedError.",
                    "signature": "(path, *, dir_fd=None)"
                },
                {
                    "name": "unsetenv",
                    "qualname": "unsetenv",
                    "docstring": "Delete an environment variable.",
                    "signature": "(name, /)"
                },
                {
                    "name": "urandom",
                    "qualname": "urandom",
                    "docstring": "Return a bytes object containing random bytes suitable for cryptographic use.",
                    "signature": "(size, /)"
                },
                {
                    "name": "utime",
                    "qualname": "utime",
                    "docstring": "Set the access and modified time of path.\n\npath may always be specified as a string.\nOn some platforms, path may also be specified as an open file descriptor.\n  If this functionality is unavailable, using it raises an exception.\n\nIf times is not None, it must be a tuple (atime, mtime);\n    atime and mtime should be expressed as float seconds since the epoch.\nIf ns is specified, it must be a tuple (atime_ns, mtime_ns);\n    atime_ns and mtime_ns should be expressed as integer nanoseconds\n    since the epoch.\nIf times is None and ns is unspecified, utime uses the current time.\nSpecifying tuples for both times and ns is an error.\n\nIf dir_fd is not None, it should be a file descriptor open to a directory,\n  and path should be relative; path will then be relative to that directory.\nIf follow_symlinks is False, and the last element of the path is a symbolic\n  link, utime will modify the symbolic link itself instead of the file the\n  link points to.\nIt is an error to use dir_fd or follow_symlinks when specifying path\n  as an open file descriptor.\ndir_fd and follow_symlinks may not be available on your platform.\n  If they are unavailable, using them will raise a NotImplementedError.",
                    "signature": "Error getting signature: ValueError"
                },
                {
                    "name": "waitpid",
                    "qualname": "waitpid",
                    "docstring": "Wait for completion of a given process.\n\nReturns a tuple of information regarding the process:\n    (pid, status << 8)\n\nThe options argument is ignored on Windows.",
                    "signature": "(pid, options, /)"
                },
                {
                    "name": "waitstatus_to_exitcode",
                    "qualname": "waitstatus_to_exitcode",
                    "docstring": "Convert a wait status to an exit code.\n\nOn Unix:\n\n* If WIFEXITED(status) is true, return WEXITSTATUS(status).\n* If WIFSIGNALED(status) is true, return -WTERMSIG(status).\n* Otherwise, raise a ValueError.\n\nOn Windows, return status shifted right by 8 bits.\n\nOn Unix, if the process is being traced or if waitpid() was called with\nWUNTRACED option, the caller must first check if WIFSTOPPED(status) is true.\nThis function must not be called if WIFSTOPPED(status) is true.",
                    "signature": "(status)"
                },
                {
                    "name": "walk",
                    "qualname": "walk",
                    "docstring": "Directory tree generator.\n\nFor each directory in the directory tree rooted at top (including top\nitself, but excluding '.' and '..'), yields a 3-tuple\n\n    dirpath, dirnames, filenames\n\ndirpath is a string, the path to the directory.  dirnames is a list of\nthe names of the subdirectories in dirpath (including symlinks to directories,\nand excluding '.' and '..').\nfilenames is a list of the names of the non-directory files in dirpath.\nNote that the names in the lists are just names, with no path components.\nTo get a full path (which begins with top) to a file or directory in\ndirpath, do os.path.join(dirpath, name).\n\nIf optional arg 'topdown' is true or not specified, the triple for a\ndirectory is generated before the triples for any of its subdirectories\n(directories are generated top down).  If topdown is false, the triple\nfor a directory is generated after the triples for all of its\nsubdirectories (directories are generated bottom up).\n\nWhen topdown is true, the caller can modify the dirnames list in-place\n(e.g., via del or slice assignment), and walk will only recurse into the\nsubdirectories whose names remain in dirnames; this can be used to prune the\nsearch, or to impose a specific order of visiting.  Modifying dirnames when\ntopdown is false has no effect on the behavior of os.walk(), since the\ndirectories in dirnames have already been generated by the time dirnames\nitself is generated. No matter the value of topdown, the list of\nsubdirectories is retrieved before the tuples for the directory and its\nsubdirectories are generated.\n\nBy default errors from the os.scandir() call are ignored.  If\noptional arg 'onerror' is specified, it should be a function; it\nwill be called with one argument, an OSError instance.  It can\nreport the error to continue with the walk, or raise the exception\nto abort the walk.  Note that the filename is available as the\nfilename attribute of the exception object.\n\nBy default, os.walk does not follow symbolic links to subdirectories on\nsystems that support them.  In order to get this functionality, set the\noptional argument 'followlinks' to true.\n\nCaution:  if you pass a relative pathname for top, don't change the\ncurrent working directory between resumptions of walk.  walk never\nchanges the current directory, and assumes that the client doesn't\neither.\n\nExample:\n\nimport os\nfrom os.path import join, getsize\nfor root, dirs, files in os.walk('python/Lib/xml'):\n    print(root, \"consumes \")\n    print(sum(getsize(join(root, name)) for name in files), end=\" \")\n    print(\"bytes in\", len(files), \"non-directory files\")\n    if '__pycache__' in dirs:\n        dirs.remove('__pycache__')  # don't visit __pycache__ directories",
                    "signature": "(top, topdown=True, onerror=None, followlinks=False)"
                },
                {
                    "name": "write",
                    "qualname": "write",
                    "docstring": "Write a bytes object to a file descriptor.",
                    "signature": "(fd, data, /)"
                }
            ],
            "classes": [
                {
                    "name": "DirEntry",
                    "qualname": "DirEntry",
                    "docstring": "",
                    "signature": "()",
                    "type": "class",
                    "methods": [
                        {
                            "name": "inode",
                            "qualname": "DirEntry.inode",
                            "docstring": "Return inode of the entry; cached per entry.",
                            "signature": "(self, /)"
                        },
                        {
                            "name": "is_dir",
                            "qualname": "DirEntry.is_dir",
                            "docstring": "Return True if the entry is a directory; cached per entry.",
                            "signature": "(self, /, *, follow_symlinks=True)"
                        },
                        {
                            "name": "is_file",
                            "qualname": "DirEntry.is_file",
                            "docstring": "Return True if the entry is a file; cached per entry.",
                            "signature": "(self, /, *, follow_symlinks=True)"
                        },
                        {
                            "name": "is_junction",
                            "qualname": "DirEntry.is_junction",
                            "docstring": "Return True if the entry is a junction; cached per entry.",
                            "signature": "(self, /)"
                        },
                        {
                            "name": "is_symlink",
                            "qualname": "DirEntry.is_symlink",
                            "docstring": "Return True if the entry is a symbolic link; cached per entry.",
                            "signature": "(self, /)"
                        },
                        {
                            "name": "stat",
                            "qualname": "DirEntry.stat",
                            "docstring": "Return stat_result object for the entry; cached per entry.",
                            "signature": "(self, /, *, follow_symlinks=True)"
                        }
                    ]
                },
                {
                    "name": "error",
                    "qualname": "OSError",
                    "docstring": "Base class for I/O related errors.",
                    "signature": "Error getting signature: ValueError",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "stat_result",
                    "qualname": "stat_result",
                    "docstring": "stat_result: Result from stat, fstat, or lstat.\n\nThis object may be accessed either as a tuple of\n  (mode, ino, dev, nlink, uid, gid, size, atime, mtime, ctime)\nor via the attributes st_mode, st_ino, st_dev, st_nlink, st_uid, and so on.\n\nPosix/windows: If your platform supports st_blksize, st_blocks, st_rdev,\nor st_flags, they are available as attributes only.\n\nSee os.stat for more information.",
                    "signature": "(iterable=(), /)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "statvfs_result",
                    "qualname": "statvfs_result",
                    "docstring": "statvfs_result: Result from statvfs or fstatvfs.\n\nThis object may be accessed either as a tuple of\n  (bsize, frsize, blocks, bfree, bavail, files, ffree, favail, flag, namemax),\nor via the attributes f_bsize, f_frsize, f_blocks, f_bfree, and so on.\n\nSee os.statvfs for more information.",
                    "signature": "(iterable=(), /)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "terminal_size",
                    "qualname": "terminal_size",
                    "docstring": "A tuple of (columns, lines) for holding terminal window size",
                    "signature": "(iterable=(), /)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "times_result",
                    "qualname": "times_result",
                    "docstring": "times_result: Result from os.times().\n\nThis object may be accessed either as a tuple of\n  (user, system, children_user, children_system, elapsed),\nor via the attributes user, system, children_user, children_system,\nand elapsed.\n\nSee os.times for more information.",
                    "signature": "(iterable=(), /)",
                    "type": "class",
                    "methods": []
                },
                {
                    "name": "uname_result",
                    "qualname": "uname_result",
                    "docstring": "uname_result: Result from os.uname().\n\nThis object may be accessed either as a tuple of\n  (sysname, nodename, release, version, machine),\nor via the attributes sysname, nodename, release, version, and machine.\n\nSee os.uname for more information.",
                    "signature": "(iterable=(), /)",
                    "type": "class",
                    "methods": []
                }
            ],
            "submodules": [
                {
                    "type": "module",
                    "name": "phiml.os.path",
                    "docstring": "Stand-in replacement for `os.path` with vectorization support.\nAll functions accept `Tensor` as inputs, broadcasting the function call.",
                    "functions": [
                        {
                            "name": "commonpath",
                            "qualname": "commonpath",
                            "docstring": "",
                            "signature": "(paths: Sequence[Union[str, phiml.math._tensors.Tensor]])"
                        },
                        {
                            "name": "commonprefix",
                            "qualname": "commonprefix",
                            "docstring": "",
                            "signature": "(list: Sequence[Union[str, phiml.math._tensors.Tensor]])"
                        }
                    ],
                    "classes": [],
                    "submodules": []
                }
            ]
        }
    ],
    "docstring": "Unified documentation for the phi package and its submodules."
}