Selected Files Directory Structure:

└── ./
    ├── src
    │   ├── config
    │   │   ├── __init__.py
    │   │   ├── augmentation_config.py
    │   │   ├── config_helper.py
    │   │   ├── data_config.py
    │   │   ├── evaluation_config.py
    │   │   ├── experiment_config.py
    │   │   ├── generation_config.py
    │   │   ├── model_config.py
    │   │   └── trainer_config.py
    │   ├── data
    │   │   ├── adapters
    │   │   │   └── bvts_adapter.py
    │   │   ├── __init__.py
    │   │   ├── abstract_dataset.py
    │   │   ├── data_manager.py
    │   │   ├── dataset_utilities.py
    │   │   ├── field_dataset.py
    │   │   ├── generator.py
    │   │   ├── tensor_dataset.py
    │   │   └── validation.py
    │   ├── evaluation
    │   │   ├── __init__.py
    │   │   ├── evaluator.py
    │   │   ├── metrics.py
    │   │   └── visualizations.py
    │   ├── factories
    │   │   ├── __init__.py
    │   │   ├── dataloader_factory.py
    │   │   ├── model_factory.py
    │   │   └── trainer_factory.py
    │   ├── models
    │   │   ├── physical
    │   │   │   ├── __init__.py
    │   │   │   ├── advection.py
    │   │   │   ├── base.py
    │   │   │   ├── burgers.py
    │   │   │   └── smoke.py
    │   │   ├── synthetic
    │   │   │   ├── __init__.py
    │   │   │   ├── base.py
    │   │   │   ├── convnet.py
    │   │   │   ├── resnet.py
    │   │   │   └── unet.py
    │   │   ├── __init__.py
    │   │   └── registry.py
    │   ├── tests
    │   │   ├── migration
    │   │   │   └── test_bvts_io.py
    │   │   ├── test_dataset_builder.py
    │   │   ├── test_field_dataset.py
    │   │   └── test_tensor_dataset.py
    │   ├── training
    │   │   ├── hybrid
    │   │   │   ├── __init__.py
    │   │   │   └── trainer.py
    │   │   ├── physical
    │   │   │   ├── __init__.py
    │   │   │   └── trainer.py
    │   │   ├── synthetic
    │   │   │   ├── __init__.py
    │   │   │   └── trainer.py
    │   │   ├── __init__.py
    │   │   ├── abstract_trainer.py
    │   │   ├── field_trainer.py
    │   │   └── tensor_trainer.py
    │   ├── utils
    │   │   ├── __init__.py
    │   │   ├── gpu_memory_profiler.py
    │   │   ├── logger.py
    │   │   └── memory_monitor.py
    │   └── __init__.py
    ├── tests
    │   └── test_bvts_compliance.py
    └── run.py



--- run.py ---

"""
Hydra-based experiment runner with structured logging.

Usage:
    python run.py --config-name=burgers_experiment
    python run.py --config-name=burgers_experiment trainer_params.epochs=200
    python run.py --config-name=smoke_experiment run_params.mode=[generate,train]
"""

import os
import sys
import logging
from pathlib import Path
from typing import List

import hydra
from omegaconf import DictConfig, OmegaConf

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from src.data.generator import run_generation
from src.factories.trainer_factory import TrainerFactory
from src.factories.dataloader_factory import DataLoaderFactory
from src.evaluation import Evaluator
from src.utils.logger import setup_logger

# Setup root logger
logger = setup_logger("hyco_phiflow", level=logging.INFO)


@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig) -> None:
    """Main entry point with Hydra configuration."""
    config = OmegaConf.to_container(cfg, resolve=True)
    config["project_root"] = str(PROJECT_ROOT)

    # Configure logging from config
    log_config = config.get("logging", {})
    log_level = getattr(logging, log_config.get("root_level", "INFO"))
    logger.setLevel(log_level)

    # Apply module-specific log levels
    module_levels = log_config.get("module_levels", {})
    for module_name, level_str in module_levels.items():
        module_logger = logging.getLogger(module_name)
        module_logger.setLevel(getattr(logging, level_str))

    tasks = config["run_params"]["mode"]

    if isinstance(tasks, str):
        tasks = [tasks]

    logger.info(f"Starting HYCO-PhiFlow with tasks: {tasks}")
    logger.info(
        f"Experiment: {config['run_params'].get('experiment_name', 'unknown')}"
    )

    # Execute tasks
    for task in tasks:
        logger.info(f"=== Starting task: {task} ===")

        if task == "generate":
            logger.info("Running data generation")
            run_generation(config)

        elif task == "train":
            logger.info("Running training")
            # Use factory to create trainer (Phase 1 API)
            trainer = TrainerFactory.create_trainer(config)
            
            # Create data loader/dataset based on model type using NEW DataLoaderFactory
            model_type = config["run_params"]["model_type"]
            
            if model_type == "synthetic":
                # Create DataLoader for synthetic training using DataLoaderFactory
                data_loader = DataLoaderFactory.create(
                    config,
                    mode='tensor',
                    shuffle=True,
                )
                
                # Train with explicit data passing (Phase 1 API)
                num_epochs = config["trainer_params"]["epochs"]
                trainer.train(data_source=data_loader, num_epochs=num_epochs)
                
            elif model_type == "physical":
                # Create FieldDataset for physical training using DataLoaderFactory
                dataset = DataLoaderFactory.create(
                    config,
                    mode='field',
                    batch_size=None,  # Physical models don't use batching
                )
                
                # Train with explicit data passing (Phase 1 API)
                # Physical typically uses single epoch (optimization per sample)
                num_epochs = config["trainer_params"].get("epochs", 1)
                trainer.train(data_source=dataset, num_epochs=num_epochs)
                
            elif model_type == "hybrid":
                # Phase 3: Hybrid training with data augmentation
                # HybridTrainer manages data creation internally
                # Just call train() with no arguments
                logger.info("Starting hybrid training...")
                trainer.train()
            
            else:
                raise ValueError(f"Unknown model_type: {model_type}")


        elif task == "evaluate":
            logger.info("Running evaluation")
            evaluator = Evaluator(config)
            evaluator.evaluate()

        else:
            logger.warning(f"Unknown task '{task}', skipping")

        logger.info(f"=== Completed task: {task} ===")

    logger.info("All tasks completed successfully")


if __name__ == "__main__":
    main()


--- src/__init__.py ---



--- src/config/__init__.py ---

"""Structured configuration using Hydra dataclasses."""

from .config_helper import ConfigHelper
from .data_config import DataConfig
from .model_config import PhysicalModelConfig, SyntheticModelConfig
from .trainer_config import SyntheticTrainerConfig, PhysicalTrainerConfig
from .generation_config import GenerationConfig
from .evaluation_config import EvaluationConfig
from .experiment_config import ExperimentConfig
from .augmentation_config import (
    AugmentationConfig,
    validate_cache_config,
    create_cache_path,
    get_augmentation_summary,
)

__all__ = [
    "ConfigHelper",
    "DataConfig",
    "PhysicalModelConfig",
    "SyntheticModelConfig",
    "SyntheticTrainerConfig",
    "PhysicalTrainerConfig",
    "GenerationConfig",
    "EvaluationConfig",
    "ExperimentConfig",
    "AugmentationConfig",
    "validate_cache_config",
    "create_cache_path",
    "get_augmentation_summary",
]


--- src/config/augmentation_config.py ---

"""
Augmentation Configuration Validation and Utilities

This module provides validation and helper functions for augmentation configuration.
Ensures that augmentation settings are valid and compatible with the training setup.
"""

from typing import Dict, Any, List, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class AugmentationConfig:
    """Validates and manages augmentation configuration."""

    # Note: Only 'cached' strategy is supported (on_the_fly removed)
    VALID_FORMATS = ["dict", "tuple"]

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize augmentation configuration.

        Args:
            config: Configuration dictionary with augmentation settings

        Note: Strategy is hardcoded to 'cached' only.
        """
        self.config = config
        self.enabled = config.get("enabled", False)

        if self.enabled:
            self._validate()

    def _validate(self):
        """Validate augmentation configuration."""
        # Validate alpha
        alpha = self.config.get("alpha", 0.1)
        if not isinstance(alpha, (int, float)):
            raise ValueError(f"alpha must be numeric, got {type(alpha)}")
        if alpha < 0.0 or alpha > 1.0:
            raise ValueError(f"alpha must be in [0.0, 1.0], got {alpha}")

        # Note: Strategy is always 'cached' (hardcoded)
        # Validate cache settings
        self._validate_cache_settings()

        # Validate device
        device = self.config.get("device", "cuda")
        if not isinstance(device, str):
            raise ValueError(f"device must be string, got {type(device)}")

    def _validate_cache_settings(self):
        """Validate cache-specific settings."""
        cache_config = self.config.get("cache", {})

        # Validate experiment_name
        experiment_name = cache_config.get("experiment_name")
        if not experiment_name:
            raise ValueError("cache.experiment_name must be specified")

        # Validate format
        format_type = cache_config.get("format", "dict")
        if format_type not in self.VALID_FORMATS:
            raise ValueError(
                f"cache.format must be one of {self.VALID_FORMATS}, got '{format_type}'"
            )

        # Validate max_memory_samples
        max_samples = cache_config.get("max_memory_samples", 1000)
        if not isinstance(max_samples, int) or max_samples <= 0:
            raise ValueError(
                f"cache.max_memory_samples must be positive integer, got {max_samples}"
            )

        # Validate reuse_existing
        reuse = cache_config.get("reuse_existing", True)
        if not isinstance(reuse, bool):
            raise ValueError(f"cache.reuse_existing must be boolean, got {type(reuse)}")

    def get_alpha(self) -> float:
        """Get alpha value."""
        return self.config.get("alpha", 0.1)

    def get_strategy(self) -> str:
        """Get augmentation strategy (always 'cached')."""
        return "cached"  # Hardcoded

    def get_cache_config(self) -> Dict[str, Any]:
        """Get cache configuration."""
        return self.config.get("cache", {})

    def get_device(self) -> str:
        """Get device for prediction generation."""
        return self.config.get("device", "cuda")

    def should_regenerate(self, epoch: int) -> bool:
        """
        Check if predictions should be regenerated at this epoch.

        Args:
            epoch: Current training epoch

        Returns:
            False (cached strategy never regenerates - hardcoded)
        """
        return False  # Cached strategy doesn't regenerate (hardcoded)

    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary."""
        return {
            "enabled": self.enabled,
            "alpha": self.get_alpha(),
            "strategy": "cached",  # Hardcoded
            "cache": self.get_cache_config(),
            "device": self.get_device(),
        }

    def __repr__(self) -> str:
        if not self.enabled:
            return "AugmentationConfig(enabled=False)"

        return (
            f"AugmentationConfig("
            f"enabled=True, "
            f"alpha={self.get_alpha()}, "
            f"strategy='cached', "  # Hardcoded
            f"device='{self.get_device()}')"
        )


def validate_cache_config(cache_config: Dict[str, Any]) -> List[str]:
    """
    Validate cache configuration from main config.yaml.

    Args:
        cache_config: Cache configuration dictionary

    Returns:
        List of validation errors (empty if valid)
    """
    errors = []

    # Validate root
    root = cache_config.get("root")
    if not root:
        errors.append("cache.root must be specified")
    elif not isinstance(root, str):
        errors.append(f"cache.root must be string, got {type(root)}")

    # Validate auto_create
    auto_create = cache_config.get("auto_create", True)
    if not isinstance(auto_create, bool):
        errors.append(f"cache.auto_create must be boolean, got {type(auto_create)}")

    # Validate validation settings
    validation_config = cache_config.get("validation", {})
    if not isinstance(validation_config, dict):
        errors.append(f"cache.validation must be dict, got {type(validation_config)}")
    else:
        check_on_load = validation_config.get("check_on_load", True)
        if not isinstance(check_on_load, bool):
            errors.append(
                f"cache.validation.check_on_load must be boolean, got {type(check_on_load)}"
            )

        expected_count = validation_config.get("expected_count")
        if expected_count is not None and not isinstance(expected_count, int):
            errors.append(
                f"cache.validation.expected_count must be int or null, got {type(expected_count)}"
            )

    # Validate cleanup settings
    cleanup_config = cache_config.get("cleanup", {})
    if not isinstance(cleanup_config, dict):
        errors.append(f"cache.cleanup must be dict, got {type(cleanup_config)}")
    else:
        for key in ["on_start", "on_error"]:
            value = cleanup_config.get(key, False)
            if not isinstance(value, bool):
                errors.append(f"cache.cleanup.{key} must be boolean, got {type(value)}")

    return errors


def create_cache_path(cache_root: str, experiment_name: str) -> Path:
    """
    Create cache path for augmented data.

    Args:
        cache_root: Root cache directory
        experiment_name: Experiment/dataset name

    Returns:
        Path to cache directory
    """
    cache_path = Path(cache_root) / "hybrid_generated" / experiment_name
    return cache_path


def get_augmentation_summary(config: AugmentationConfig) -> str:
    """
    Get human-readable summary of augmentation configuration.

    Args:
        config: Augmentation configuration

    Returns:
        Formatted summary string
    """
    if not config.enabled:
        return "Augmentation: Disabled"

    lines = [
        "Augmentation Configuration:",
        f"  Enabled: True",
        f"  Alpha: {config.get_alpha()}",
        f"  Strategy: {config.get_strategy()}",
        f"  Device: {config.get_device()}",
    ]

    if config.get_strategy() == "cached":
        cache_config = config.get_cache_config()
        lines.extend(
            [
                "  Cache Settings:",
                f"    Experiment: {cache_config.get('experiment_name')}",
                f"    Format: {cache_config.get('format')}",
                f"    Max Memory Samples: {cache_config.get('max_memory_samples')}",
                f"    Reuse Existing: {cache_config.get('reuse_existing')}",
            ]
        )
    else:
        otf_config = config.get_on_the_fly_config()
        lines.extend(
            [
                "  On-the-fly Settings:",
                f"    Generate Every: {otf_config.get('generate_every')} epochs",
                f"    Batch Size: {otf_config.get('batch_size')}",
                f"    Rollout Steps: {otf_config.get('rollout_steps')}",
            ]
        )

    return "\n".join(lines)


--- src/config/config_helper.py ---

"""
Configuration Helper

Centralizes all config extraction logic to reduce coupling between components
and configuration structure. Makes refactoring easier by providing a single
point of access to configuration values.

This helper class knows about the structure of the Hydra configuration and
provides clean, typed access to all configuration values needed by the data
loading system.
"""

from typing import List, Tuple, Dict, Any, Optional
from pathlib import Path


class ConfigHelper:
    """
    Helper class for extracting configuration values.

    This reduces coupling between components and config structure,
    making refactoring easier. All config path knowledge is centralized here.

    The config structure expected:
    ```yaml
    data:
      dset_name: "dataset_name"
      data_dir: "data"
      fields: ["field1", "field2"]
      validate_cache: true
      auto_clear_invalid: false

    model:
      synthetic:
        input_specs:
          field1: {...}
          field2: {...}
        output_specs:
          field1: {...}
      physical:
        ...

    trainer_params:
      train_sim: [0, 1, 2, ...]
      batch_size: 16
      num_predict_steps: 10
      augmentation:
        enabled: true
        alpha: 0.1
        strategy: "cached"
        cache:
          experiment_name: "..."

    cache:
      root: "data/cache"
    ```

    Args:
        config: Full configuration dictionary from Hydra

    Example:
        >>> cfg = ConfigHelper(config)
        >>> field_names = cfg.get_field_names()
        >>> dynamic, static = cfg.get_field_types()
        >>> batch_size = cfg.get_batch_size()
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize ConfigHelper with full configuration.

        Args:
            config: Complete configuration dictionary (typically from Hydra)
        """
        self.config = config

        # Handle both nested (data: {fields: ...}) and flat (fields: ...) structures
        # This allows ConfigHelper to work with both test configs and real Hydra configs
        if "data" in config and isinstance(config["data"], dict):
            # Nested structure: {data: {fields: [...], ...}}
            self.data_config = config["data"]
        else:
            # Flat structure (Hydra with defaults): {fields: [...], dset_name: ..., ...}
            # Create a virtual data_config from top-level keys
            self.data_config = {
                "fields": config.get("fields", []),
                "dset_name": config.get("dset_name", "unknown"),
                "data_dir": config.get("data_dir", "data/"),
                "cache_dir": config.get("cache_dir", "data/cache"),
                "validate_cache": config.get("validate_cache", True),
                "auto_clear_invalid": config.get("auto_clear_invalid", False),
            }

        self.model_config = config.get("model", {})
        self.run_params = config.get("run_params", {})
        self.trainer_config = config.get("trainer_params", {})
        self.augmentation_config = self.trainer_config.get("augmentation", {})
        self.cache_config = config.get("cache", {})

    # ==================== Data Configuration ====================

    def get_field_names(self) -> List[str]:
        """
        Get list of field names to load.

        Returns:
            List of field names (e.g., ['velocity', 'density', 'inflow'])
        """
        return self.data_config.get("fields", [])

    def get_dataset_name(self) -> str:
        """
        Get dataset name.

        Returns:
            Dataset name (e.g., 'burgers_128', 'smoke_128')
        """
        return self.data_config.get("dset_name", "unknown")

    def get_raw_data_dir(self) -> Path:
        """
        Get raw data directory path.

        Returns:
            Path to raw data (e.g., Path('data/burgers_128'))
        """
        data_dir = self.data_config.get("data_dir", "data")
        dset_name = self.get_dataset_name()
        return Path(data_dir) / dset_name

    def get_cache_dir(self) -> Path:
        """
        Get cache directory path.

        Returns:
            Path to cache directory (e.g., Path('data/cache'))
        """
        data_dir = self.data_config.get("data_dir", "data")
        return Path(data_dir) / "cache"

    def should_validate_cache(self) -> bool:
        """
        Check if cache validation is enabled.

        Returns:
            True if cache should be validated on load
        """
        return self.data_config.get("validate_cache", True)

    def should_auto_clear_invalid(self) -> bool:
        """
        Check if auto-clear invalid cache is enabled.

        Returns:
            True if invalid cache should be automatically cleared
        """
        return self.data_config.get("auto_clear_invalid", False)

    # ==================== Field Specifications ====================

    def get_field_types(self) -> Tuple[List[str], List[str]]:
        """
        Get dynamic and static field names.

        Dynamic fields are predicted by the model (outputs).
        Static fields are input-only (not predicted).

        For synthetic models:
        - Dynamic: Fields in output_specs
        - Static: Fields in input_specs but not in output_specs

        For physical models:
        - Dynamic: All fields (physical models predict all fields)
        - Static: Empty list

        Returns:
            Tuple of (dynamic_fields, static_fields)

        Example:
            >>> dynamic, static = cfg.get_field_types()
            >>> # For synthetic: dynamic=['velocity', 'density'], static=['inflow']
            >>> # For physical: dynamic=['velocity', 'density'], static=[]
        """
        model_type = self.run_params.get("model_type", "synthetic")

        if model_type == "synthetic":
            # Extract from model specs
            synthetic_config = self.model_config.get("synthetic", {})
            input_specs = synthetic_config.get("input_specs", {})
            output_specs = synthetic_config.get("output_specs", {})

            input_specs = {
                field: self.model_config['physical']['fields_scheme'].lower().count(field[0].lower())
                for i, field in enumerate(self.model_config['physical']['fields'])
                if field
            }
            output_specs = {
            field: self.model_config['physical']['fields_scheme'].lower().count(field[0].lower())
            for i, field in enumerate(self.model_config['physical']['fields'])
            if field and self.model_config['physical']['fields_type'][i].upper() == 'D'
            }

            # Dynamic = output fields
            dynamic_fields = list(output_specs.keys())

            # Static = input-only fields (in input but not in output)
            static_fields = [
                field for field in input_specs.keys() if field not in output_specs
            ]

            print(dynamic_fields, static_fields)

            return dynamic_fields, static_fields

        else:
            # Physical model: all fields are dynamic
            field_names = self.get_field_names()
            return field_names, []

    def get_input_specs(self) -> Dict[str, Any]:
        """
        Get input field specifications for synthetic model.

        Returns:
            Dictionary mapping field names to input specs
        """
        return self.model_config.get("synthetic", {}).get("input_specs", {})

    def get_output_specs(self) -> Dict[str, Any]:
        """
        Get output field specifications for synthetic model.

        Returns:
            Dictionary mapping field names to output specs
        """
        return self.model_config.get("synthetic", {}).get("output_specs", {})

    # ==================== Training Configuration ====================

    def get_train_sim_indices(self) -> List[int]:
        """
        Get training simulation indices.

        Returns:
            List of simulation indices for training
        """
        return self.trainer_config.get("train_sim", [])

    def get_val_sim_indices(self) -> List[int]:
        """
        Get validation simulation indices.

        Returns:
            List of simulation indices for validation
        """
        return self.trainer_config.get("val_sim", [])

    def get_batch_size(self) -> int:
        """
        Get batch size.

        Returns:
            Batch size for training
        """
        return self.trainer_config.get("batch_size", 16)

    def get_num_predict_steps(self) -> int:
        """
        Get number of prediction steps.

        Returns:
            Number of autoregressive prediction steps
        """
        return self.trainer_config.get("num_predict_steps", 10)

    def should_use_sliding_window(self) -> bool:
        """
        Check if sliding window should be used.

        Returns:
            True if sliding window is enabled in config
        """
        return self.trainer_config.get("use_sliding_window", True)

    # ==================== Augmentation Configuration ====================

    def is_augmentation_enabled(self) -> bool:
        """
        Check if augmentation is enabled.

        Returns:
            True if augmentation is enabled in config
        """
        return self.augmentation_config.get("enabled", False)

    def get_augmentation_alpha(self) -> float:
        """
        Get augmentation alpha parameter.

        Alpha determines the proportion of augmented samples:
        - alpha=0.1 means 10% augmented samples
        - num_augmented = int(num_real * alpha)

        Returns:
            Augmentation proportion (typically 0.1 to 0.5)
        """
        return self.augmentation_config.get("alpha", 0.1)

    def get_augmentation_strategy(self) -> str:
        """
        Get augmentation strategy.

        Returns:
            Strategy string: 'cached', 'on_the_fly', or 'memory'
        """
        return self.augmentation_config.get("strategy", "cached")

    def get_augmentation_mode(self) -> str:
        """
        Get augmentation mode (normalized version of strategy).

        Maps strategy names to standardized mode names:
        - 'cached' → 'cache'
        - 'on_the_fly' → 'on_the_fly'
        - 'memory' → 'memory'

        Returns:
            Mode string: 'cache', 'on_the_fly', or 'memory'
        """
        strategy = self.get_augmentation_strategy()

        # Normalize strategy names
        if strategy == "cached":
            return "cache"
        elif strategy in ["on_the_fly", "on-the-fly"]:
            return "on_the_fly"
        else:
            return strategy

    def get_augmentation_config(self) -> Optional[Dict[str, Any]]:
        """
        Get complete augmentation configuration for dataset.

        Returns dictionary with all parameters needed by AbstractDataset:
        - mode: 'memory', 'cache', or 'on_the_fly'
        - alpha: Proportion of augmented samples
        - cache_dir: Path to cache directory (for cache mode)
        - data: Pre-loaded data (for memory mode, if available)

        Returns:
            Augmentation config dict, or None if augmentation disabled

        Example:
            >>> aug_config = cfg.get_augmentation_config()
            >>> # {'mode': 'cache', 'alpha': 0.1, 'cache_dir': 'data/cache/...'}
        """
        if not self.is_augmentation_enabled():
            return None

        mode = self.get_augmentation_mode()
        alpha = self.get_augmentation_alpha()

        config = {
            "mode": mode,
            "alpha": alpha,
        }

        if mode == "cache":
            # Build cache directory path
            cache_root = self.cache_config.get("root", "data/cache")
            cache_subconfig = self.augmentation_config.get("cache", {})
            experiment_name = cache_subconfig.get(
                "experiment_name", self.get_dataset_name()
            )

            # Full path: <cache_root>/hybrid_generated/<experiment_name>
            cache_dir = Path(cache_root) / "hybrid_generated" / experiment_name
            config["cache_dir"] = str(cache_dir)

        return config

    def get_augmentation_cache_dir(self) -> Optional[Path]:
        """
        Get augmentation cache directory path.

        Returns:
            Path to augmentation cache, or None if not using cache mode
        """
        if not self.is_augmentation_enabled():
            return None

        if self.get_augmentation_mode() != "cache":
            return None

        aug_config = self.get_augmentation_config()
        return Path(aug_config["cache_dir"]) if aug_config else None

    # ==================== Model Configuration ====================

    def get_model_type(self) -> str:
        """
        Get model type.

        Returns:
            Model type: 'synthetic', 'physical', or 'hybrid'
        """
        return self.run_params.get("model_type", "synthetic")

    def is_hybrid_training(self) -> bool:
        """
        Check if using hybrid training mode.

        Returns:
            True if model_type is 'hybrid'
        """
        return self.get_model_type() == "hybrid"

    # ==================== Project Paths ====================

    def get_project_root(self) -> Path:
        """
        Get project root directory.

        Returns:
            Path to project root
        """
        return Path(self.config.get("project_root", "."))

    def get_absolute_raw_data_dir(self) -> Path:
        """
        Get absolute path to raw data directory.

        Returns:
            Absolute path: <project_root>/<data_dir>/<dset_name>
        """
        project_root = self.get_project_root()
        return project_root / self.get_raw_data_dir()

    def get_absolute_cache_dir(self) -> Path:
        """
        Get absolute path to cache directory.

        Returns:
            Absolute path: <project_root>/<data_dir>/cache
        """
        project_root = self.get_project_root()
        return project_root / self.get_cache_dir()

    def get_absolute_augmentation_cache_dir(self) -> Optional[Path]:
        """
        Get absolute path to augmentation cache directory.

        Returns:
            Absolute path or None if not using cache mode
        """
        cache_dir = self.get_augmentation_cache_dir()
        if cache_dir is None:
            return None

        project_root = self.get_project_root()
        return project_root / cache_dir

    # ==================== Utility Methods ====================

    def get_summary(self) -> Dict[str, Any]:
        """
        Get summary of all configuration values.

        Useful for logging and debugging.

        Returns:
            Dictionary with all extracted config values
        """
        dynamic_fields, static_fields = self.get_field_types()

        return {
            # Data
            "dataset_name": self.get_dataset_name(),
            "field_names": self.get_field_names(),
            "dynamic_fields": dynamic_fields,
            "static_fields": static_fields,
            "raw_data_dir": str(self.get_raw_data_dir()),
            "cache_dir": str(self.get_cache_dir()),
            # Training
            "model_type": self.get_model_type(),
            "train_sims": len(self.get_train_sim_indices()),
            "batch_size": self.get_batch_size(),
            "num_predict_steps": self.get_num_predict_steps(),
            "use_sliding_window": self.should_use_sliding_window(),
            # Augmentation
            "augmentation_enabled": self.is_augmentation_enabled(),
            "augmentation_alpha": (
                self.get_augmentation_alpha()
                if self.is_augmentation_enabled()
                else None
            ),
            "augmentation_mode": (
                self.get_augmentation_mode() if self.is_augmentation_enabled() else None
            ),
            "augmentation_cache_dir": (
                str(self.get_augmentation_cache_dir())
                if self.get_augmentation_cache_dir()
                else None
            ),
        }

    def validate(self) -> List[str]:
        """
        Validate configuration and return list of issues.

        Returns:
            List of validation error messages (empty if valid)
        """
        issues = []

        # Check required fields
        if not self.get_field_names():
            issues.append("No fields specified in data.fields")

        if not self.get_train_sim_indices():
            issues.append(
                "No training simulations specified in trainer_params.train_sim"
            )

        if self.get_batch_size() <= 0:
            issues.append(f"Invalid batch_size: {self.get_batch_size()}")

        if self.get_num_predict_steps() <= 0:
            issues.append(f"Invalid num_predict_steps: {self.get_num_predict_steps()}")

        # Check augmentation config
        if self.is_augmentation_enabled():
            alpha = self.get_augmentation_alpha()
            if alpha < 0 or alpha > 1:
                issues.append(f"Invalid augmentation alpha: {alpha} (must be 0-1)")

            mode = self.get_augmentation_mode()
            if mode not in ["cache", "memory", "on_the_fly"]:
                issues.append(f"Invalid augmentation mode: {mode}")

            if mode == "cache":
                cache_dir = self.get_augmentation_cache_dir()
                if cache_dir is None:
                    issues.append(
                        "Augmentation cache mode enabled but cache_dir not configured"
                    )

        return issues


--- src/config/data_config.py ---

from dataclasses import dataclass, field
from typing import List, Optional
from omegaconf import MISSING


@dataclass
class DataConfig:
    """Configuration for dataset."""

    data_dir: str = "data/"
    dset_name: str = MISSING  # Required
    fields: List[str] = field(default_factory=list)  # Required
    fields_scheme: str = "unknown"
    cache_dir: str = "data/cache"

    # Validation options
    validate_cache: bool = True
    auto_clear_invalid: bool = False


--- src/config/evaluation_config.py ---

from dataclasses import dataclass, field
from typing import List


@dataclass
class EvaluationConfig:
    """Configuration for model evaluation."""

    test_sim: List[int] = field(default_factory=list)
    num_frames: int = 51
    metrics: List[str] = field(default_factory=lambda: ["mse", "mae", "rmse"])

    keyframe_count: int = 5
    animation_fps: int = 10
    save_animations: bool = True
    save_plots: bool = True

    output_dir: str = "results/evaluation"


--- src/config/experiment_config.py ---

from dataclasses import dataclass, field
from typing import List, Optional, Any, Dict
from omegaconf import MISSING

from .data_config import DataConfig
from .model_config import PhysicalModelConfig, SyntheticModelConfig
from .trainer_config import SyntheticTrainerConfig, PhysicalTrainerConfig
from .generation_config import GenerationConfig
from .evaluation_config import EvaluationConfig


@dataclass
class RunConfig:
    """Top-level run configuration."""

    experiment_name: str = MISSING
    notes: str = ""
    mode: List[str] = field(default_factory=list)  # ['generate', 'train', 'evaluate']
    model_type: str = "synthetic"  # 'synthetic' or 'physical'


@dataclass
class ExperimentConfig:
    """Complete experiment configuration."""

    # Hydra settings
    defaults: List[Any] = field(
        default_factory=lambda: [
            "_self_",
            {"data": "burgers_128"},
            {"model/physical": "burgers"},
            {"model/synthetic": "unet"},
            {"trainer": "synthetic"},
        ]
    )

    # Main config sections
    run_params: RunConfig = field(default_factory=RunConfig)
    data: DataConfig = field(default_factory=DataConfig)

    # Model configs (one will be used based on model_type)
    model: Dict[str, Any] = field(default_factory=dict)

    # Task-specific configs
    generation_params: Optional[GenerationConfig] = None
    trainer_params: Optional[Any] = None  # Can be Synthetic or Physical
    evaluation_params: Optional[EvaluationConfig] = None

    # Runtime
    project_root: str = "."


--- src/config/generation_config.py ---

from dataclasses import dataclass
from typing import Optional


@dataclass
class GenerationConfig:
    """Configuration for data generation."""

    num_simulations: int = 10
    total_steps: int = 50
    save_interval: int = 1

    # Optional: random seed for reproducibility
    seed: Optional[int] = None


--- src/config/model_config.py ---

from dataclasses import dataclass, field
from typing import Dict, Any, Optional
from omegaconf import MISSING


@dataclass
class DomainConfig:
    """Physical domain configuration."""

    size_x: float = 100.0
    size_y: float = 100.0
    size_z: Optional[float] = None


@dataclass
class ResolutionConfig:
    """Grid resolution configuration."""

    x: int = MISSING
    y: int = MISSING
    z: Optional[int] = None


@dataclass
class PhysicalModelConfig:
    """Configuration for physical PDE models."""

    name: str = MISSING  # e.g., 'BurgersModel', 'SmokeModel'
    domain: DomainConfig = field(default_factory=DomainConfig)
    resolution: ResolutionConfig = field(default_factory=ResolutionConfig)
    dt: float = 0.8
    pde_params: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ArchitectureConfig:
    """Neural network architecture parameters."""

    levels: int = 4
    filters: int = 64
    batch_norm: bool = True


@dataclass
class SyntheticModelConfig:
    """Configuration for synthetic (neural network) models."""

    name: str = MISSING  # e.g., 'UNet'
    model_path: str = "results/models"
    model_save_name: str = MISSING

    input_specs: Dict[str, int] = field(default_factory=dict)
    output_specs: Dict[str, int] = field(default_factory=dict)

    architecture: ArchitectureConfig = field(default_factory=ArchitectureConfig)


--- src/config/trainer_config.py ---

from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any


@dataclass
class SyntheticTrainerConfig:
    """Configuration for synthetic model training."""

    learning_rate: float = 1e-4
    batch_size: int = 16
    epochs: int = 100
    num_predict_steps: int = 4

    train_sim: List[int] = field(default_factory=list)
    val_sim: Optional[List[int]] = None

    # Optimizer settings
    optimizer: str = "adam"
    scheduler: str = "cosine"
    weight_decay: float = 0.0

    # Checkpoint settings
    save_interval: int = 10
    save_best_only: bool = True


@dataclass
class LearnableParameter:
    """Definition of a learnable parameter for inverse problems."""

    name: str
    initial_guess: float
    bounds: Optional[tuple] = None


@dataclass
class PhysicalTrainerConfig:
    """Configuration for physical model inverse problem training."""

    epochs: int = 100
    num_predict_steps: int = 10
    train_sim: List[int] = field(default_factory=list)

    learnable_parameters: List[LearnableParameter] = field(default_factory=list)

    # Optimizer settings
    method: str = "L-BFGS-B"
    abs_tol: float = 1e-6
    max_iterations: Optional[int] = None

    # Convergence handling for hybrid training
    suppress_convergence_errors: bool = False


--- src/data/__init__.py ---

# src/data/__init__.py

from .abstract_dataset import AbstractDataset
from .data_manager import DataManager
from .tensor_dataset import TensorDataset
from .field_dataset import FieldDataset


__all__ = [
    "AbstractDataset",
    "DataManager",
    "TensorDataset",
    "FieldDataset",
]


--- src/data/abstract_dataset.py ---

"""
Abstract Dataset Base Class - Refactored for Clean Augmentation Access

Key Changes:
- All augmented data accessed via _get_augmented_sample() (not direct access)
- Subclasses control how augmented data is accessed/processed
- Maintains distinction between real and augmented data
- Clean separation of concerns between abstract and concrete classes
"""

from typing import List, Optional, Dict, Any, Tuple
from abc import ABC, abstractmethod
from torch.utils.data import Dataset
from functools import lru_cache
import torch

from .data_manager import DataManager
from src.utils.logger import get_logger

logger = get_logger(__name__)


class AbstractDataset(Dataset, ABC):
    """
    Abstract base class for all HYCO datasets.
    
    Supports two data categories:
    1. Real data: From actual simulations (windowed)
    2. Augmented data: Generated data (format varies by subclass)
    
    Index ranges:
    [0, num_real) → Real data
    [num_real, num_real + num_augmented) → Augmented data
    
    Key principle: Both real and augmented access go through abstract methods
    that subclasses implement according to their needs.
    """
    
    __slots__ = (
        'data_manager', 'sim_indices', 'field_names', 'num_frames',
        'num_predict_steps', 'max_cached_sims', 'access_policy',
        'num_real', 'augmented_samples', 'num_augmented',
        '_cached_load_simulation', '_total_length', '_index_mapper'
    )

    def __init__(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: int,
        num_predict_steps: int,
        access_policy: str,
        num_real: int,
        augmented_samples: List[Any],
        index_mapper: Optional[Any],
        max_cached_sims: int = 5,
    ):
        """Initialize the abstract dataset."""
        if access_policy not in ("both", "real_only", "generated_only"):
            raise ValueError(
                f"Invalid access_policy: '{access_policy}'. "
                f"Must be one of ['both', 'real_only', 'generated_only']"
            )
        
        self.data_manager = data_manager
        self.sim_indices = sim_indices
        self.field_names = field_names
        self.num_frames = num_frames
        self.num_predict_steps = num_predict_steps
        self.access_policy = access_policy
        self.num_real = num_real
        self.augmented_samples = augmented_samples
        self.num_augmented = len(augmented_samples)
        self._index_mapper = index_mapper
        self.max_cached_sims = max_cached_sims
        
        # Create LRU cache
        self._cached_load_simulation = lru_cache(maxsize=max_cached_sims)(
            self._load_simulation
        )
        
        self._total_length = self._compute_length()

    # ==================== Dataset Interface ====================

    def __len__(self) -> int:
        """Return total number of samples."""
        return self._total_length

    def __getitem__(self, idx: int):
        """
        Get a sample by index.
        
        Routes to real or augmented samples based on access policy.
        Now both routes go through abstract methods.
        """
        if self.access_policy == "real_only":
            if idx >= self.num_real:
                raise IndexError(f"Index {idx} out of range [0, {self.num_real})")
            
            sample = self._get_real_sample(idx)
            return sample
        
        elif self.access_policy == "generated_only":
            if idx >= self.num_augmented:
                raise IndexError(f"Index {idx} out of range [0, {self.num_augmented})")
            
            sample = self._get_augmented_sample(idx)
            return sample
        
        else:  # 'both'
            if idx >= self._total_length:
                raise IndexError(f"Index {idx} out of range [0, {self._total_length})")
            
            if idx < self.num_real:
                sample = self._get_real_sample(idx)
                return sample
            else:
                aug_idx = idx - self.num_real
                sample = self._get_augmented_sample(aug_idx)
                return sample

    def _compute_length(self) -> int:
        """Compute total dataset length based on access policy."""
        if self.access_policy == "real_only":
            return self.num_real
        elif self.access_policy == "generated_only":
            return self.num_augmented
        else:  # 'both'
            return self.num_real + self.num_augmented

    def _get_real_sample(self, idx: int):
        """
        Get a real sample with optional filtering.
        
        Args:
            idx: Filtered index (0 to num_real-1)
        
        Returns:
            Sample in dataset-specific format
        """
        if self._index_mapper is not None:
            actual_idx = self._index_mapper.get_actual_index(idx)
        else:
            actual_idx = idx
        
        return self._extract_sample(actual_idx)

    # ==================== Abstract Methods ====================

    @abstractmethod
    def _load_simulation(self, sim_idx: int) -> Any:
        """
        Load simulation data (wrapped by LRU cache).
        
        Args:
            sim_idx: Simulation index to load
        
        Returns:
            Simulation data in format appropriate for subclass
        """
        pass

    @abstractmethod
    def _extract_sample(self, idx: int):
        """
        Extract a single sample from loaded simulations.
        
        This method should:
        1. Compute simulation and frame from index
        2. Load simulation using self._cached_load_simulation()
        3. Extract and return sample in appropriate format
        
        Args:
            idx: Actual (unfiltered) sample index
        
        Returns:
            Sample in dataset-specific format
        """
        pass

    @abstractmethod
    def _get_augmented_sample(self, idx: int):
        """
        Get an augmented sample.
        
        Subclasses implement this to handle their specific augmented format:
        - TensorDataset: Augmented = physically-generated trajectories (windowed)
        - FieldDataset: Augmented = synthetically-generated predictions (pre-windowed)
        
        Args:
            idx: Index within augmented samples (0 to num_augmented-1)
        
        Returns:
            Sample in dataset-specific format
        """
        pass

    # ==================== Utility Methods ====================

    def _compute_sim_and_frame(self, idx: int) -> Tuple[int, int]:
        """
        Compute simulation index and starting frame from a real sample index.
        
        Args:
            idx: Sample index
        
        Returns:
            Tuple of (sim_idx, start_frame)
        """
        samples_per_sim = self.num_frames - self.num_predict_steps
        if samples_per_sim <= 0:
            return self.sim_indices[0], 0

        sim_offset = idx // samples_per_sim
        start_frame = idx % samples_per_sim

        if sim_offset < len(self.sim_indices):
            sim_idx = self.sim_indices[sim_offset]
        else:
            sim_idx = sim_offset
            
        return sim_idx, start_frame

    def clear_cache(self):
        """Clear the LRU cache of loaded simulations."""
        self._cached_load_simulation.cache_clear()
        logger.info("Cleared LRU cache for dataset")

    def get_cache_info(self):
        """Get statistics about the LRU cache."""
        return self._cached_load_simulation.cache_info()

    def get_dataset_info(self) -> Dict[str, Any]:
        """Get comprehensive information about the dataset."""
        cache_info = self.get_cache_info()
        
        info = {
            "total_samples": len(self),
            "real_samples": self.num_real,
            "augmented_samples": self.num_augmented,
            "access_policy": self.access_policy,
            "num_simulations": len(self.sim_indices),
            "num_frames": self.num_frames,
            "num_predict_steps": self.num_predict_steps,
            "field_names": self.field_names,
            "lru_cache_size": cache_info.maxsize,
            "lru_cache_current": cache_info.currsize,
            "lru_cache_hits": cache_info.hits,
            "lru_cache_misses": cache_info.misses,
            "filtering_active": self._index_mapper is not None,
        }
        
        return info

    def is_augmented_sample(self, idx: int) -> bool:
        """Check if an index refers to an augmented sample."""
        if self.access_policy == "generated_only":
            return True
        elif self.access_policy == "real_only":
            return False
        else:  # 'both'
            return idx >= self.num_real

    def resample_real_data(self, seed: Optional[int] = None):
        """
        Resample the subset of real data (if filtering is active).
        
        Args:
            seed: Optional random seed for reproducibility
        """
        if self._index_mapper is None:
            logger.debug("No resampling needed (percentage_real_data=1.0)")
            return
        
        self._index_mapper.resample(seed)

--- src/data/adapters/bvts_adapter.py ---

"""Adapter utilities to convert cached simulation tensor_data to BVTS layout

Provides small helpers to convert per-field cached tensors into BVTS shape
and to concatenate multiple fields into a single BVTS tensor suitable for
feeding to BVTS-aware models.
"""
from typing import Dict, List
import torch

from src.utils.field_conversion.bvts import to_bvts


def sim_dict_to_bvts(sim_data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
    """Convert a simulation's tensor_data dict into BVTS tensors.

    Args:
        sim_data: mapping field_name -> tensor (common input shapes such as
                  [T, C, H, W], [C, T, H, W], or already BVTS)

    Returns:
        mapping field_name -> tensor in BVTS [B, V, T, *spatial]
    """
    converted = {}
    for name, tensor in sim_data.items():
        if isinstance(tensor, torch.Tensor):
            converted[name] = to_bvts(tensor)
        else:
            converted[name] = tensor
    return converted


def concat_fields_bvts(sim_data_bvts: Dict[str, torch.Tensor], field_names: List[str]) -> torch.Tensor:
    """Concatenate listed fields (already BVTS) into a single BVTS tensor.

    The concatenation is along the vector/channel dimension (dim=1).

    Returns:
        Tensor [B, V_total, T, *spatial]
    """
    tensors = [sim_data_bvts[name] for name in field_names]
    return torch.cat(tensors, dim=1)


--- src/data/data_manager.py ---

"""
Data Manager for Hybrid PDE Modeling

This module provides a centralized data management system that:
1. Loads data from PhiFlow Scene directories
2. Converts Field objects to tensors once and caches them
3. Stores metadata needed to reconstruct Field objects
4. Provides a clean interface for trainers to access data

The goal is to eliminate redundant conversions and provide a unified
data source for both physical and synthetic models.
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

import torch
from phi.torch.flow import Scene, stack, batch

from .validation import (
    CacheValidator,
    compute_hash,
    get_cache_version,
    get_phiflow_version,
)
from src.utils.logger import get_logger

logger = get_logger(__name__)


class DataManager:
    """
    Manages the conversion and caching of PhiFlow Scene data to tensors.

    This class handles the expensive one-time conversion of Field objects
    to tensors and saves them with metadata that allows reconstruction.

    Attributes:
        raw_data_dir: Path to directory containing Scene subdirectories
        cache_dir: Path to directory where processed data will be cached
        config: Configuration dictionary for the dataset
    """

    def __init__(
        self,
        raw_data_dir: str,
        cache_dir: str,
        config: Dict[str, Any],
        auto_clear_invalid: bool = False,
    ):
        """
        Initialize the DataManager.

        Args:
            raw_data_dir: Absolute path to the directory containing Scene data
                         (e.g., "data/burgers_128")
            cache_dir: Absolute path where cached tensors will be stored
                      (e.g., "data/cache")
            config: Configuration dictionary containing dataset parameters
            auto_clear_invalid: Whether to automatically remove invalid cache files

        Note: Cache creation and validation are always enabled (hardcoded).
        """
        self.raw_data_dir = Path(raw_data_dir)
        self.cache_dir = Path(cache_dir)
        self.config = config
        self.auto_clear_invalid = auto_clear_invalid

        # Create cache validator
        self.validator = CacheValidator(config)

        # Always create cache directory if it doesn't exist (hardcoded behavior)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_cached_path(self, sim_index: int) -> Path:
        """
        Get the path where cached data for a simulation should be stored.

        Args:
            sim_index: Index of the simulation

        Returns:
            Path object for the cached file
        """
        # Handle both flat and nested config structures
        if "dset_name" in self.config:
            dataset_name = self.config["dset_name"]
        else:
            dataset_name = self.config.get("data", {}).get("dset_name", "default")

        cache_subdir = self.cache_dir / dataset_name
        cache_subdir.mkdir(parents=True, exist_ok=True)
        return cache_subdir / f"sim_{sim_index:06d}.pt"

    def is_cached(
        self,
        sim_index: int,
        field_names: Optional[List[str]] = None,
        num_frames: Optional[int] = None,
    ) -> bool:
        """
        Check if a simulation has already been cached with matching parameters.

        This method always performs comprehensive validation (hardcoded behavior),
        checking PDE parameters, resolution, domain, and more. If validation
        fails and auto_clear_invalid is True, the invalid cache will be removed.

        Args:
            sim_index: Index of the simulation
            field_names: Optional list of field names to verify
            num_frames: Optional number of frames to verify

        Returns:
            True if cached data exists AND matches parameters, False otherwise
        """
        cache_path = self.get_cached_path(sim_index)

        if not cache_path.exists():
            return False

        # If no validation parameters provided, just check existence
        if field_names is None and num_frames is None:
            return True

        # Load and validate metadata (always performed - hardcoded)
        try:
            cached_data = torch.load(cache_path, weights_only=False)
            metadata = cached_data.get("metadata", {})

            # Basic validation (always performed)
            if field_names is not None:
                cached_fields = set(cached_data["tensor_data"].keys())
                requested_fields = set(field_names)
                if cached_fields != requested_fields:
                    if self.auto_clear_invalid:
                        logger.warning(
                            f"Cache invalid for sim_{sim_index:06d}: field mismatch. Removing..."
                        )
                        cache_path.unlink()
                    return False

            if num_frames is not None:
                cached_num_frames = metadata.get("num_frames", 0)
                if cached_num_frames < num_frames:
                    if self.auto_clear_invalid:
                        logger.warning(
                            f"Cache invalid for sim_{sim_index:06d}: insufficient frames. Removing..."
                        )
                        cache_path.unlink()
                    return False

            # Enhanced validation (always performed - hardcoded)
            if field_names is not None:
                is_valid, reasons = self.validator.validate_cache(
                    metadata, field_names, num_frames
                )

                if not is_valid:
                    if self.auto_clear_invalid:
                        logger.warning(
                            f"Cache invalid for sim_{sim_index:06d}: {', '.join(reasons)}. Removing..."
                        )
                        cache_path.unlink()
                    else:
                        logger.warning(
                            f"Cache invalid for sim_{sim_index:06d}: {', '.join(reasons)}"
                        )
                    return False

            return True

        except Exception as e:
            # If we can't load/validate, treat as not cached
            logger.error(f"Error validating cache for sim_{sim_index:06d}: {e}")
            if self.auto_clear_invalid:
                logger.info(f"Removing corrupted cache...")
                try:
                    cache_path.unlink()
                except:
                    pass
            return False

    def load_and_cache_simulation(
        self, sim_index: int, field_names: List[str], num_frames: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Load a simulation from Scene files, convert to tensors, and cache.

        This is the core method that performs the expensive conversion once.
        It loads all specified fields from a Scene, converts them to tensors,
        extracts metadata, and saves everything for future fast loading.

        Args:
            sim_index: Index of the simulation to load
            field_names: List of field names to load (e.g., ['velocity'])
            num_frames: Optional limit on number of frames to load.
                       If None, loads all available frames.

        Returns:
            Dictionary with 'tensor_data' and 'metadata' keys

        Raises:
            FileNotFoundError: If the Scene directory doesn't exist
        """
        # Construct path to Scene directory
        scene_name = f"sim_{sim_index:06d}"
        scene_path = self.raw_data_dir / scene_name

        # Handle non-zero-padded names as fallback
        if not scene_path.exists():
            scene_path_alt = self.raw_data_dir / f"sim_{sim_index}"
            if scene_path_alt.exists():
                scene_path = scene_path_alt
            else:
                raise FileNotFoundError(
                    f"Scene not found at {scene_path} or {scene_path_alt}"
                )

        scene = Scene.at(str(scene_path))

        # Load scene metadata from description.json
        description_path = scene_path / "description.json"
        with open(description_path, "r") as f:
            scene_metadata = json.load(f)

        # Determine frames to load
        available_frames = scene.frames
        if num_frames is not None:
            frames_to_load = available_frames[:num_frames]
        else:
            frames_to_load = available_frames

        # Dictionary to store tensor data for all fields
        tensor_data = {}
        field_metadata = {}

        # Load each field
        for field_name in field_names:
            if field_name not in scene.fieldnames:
                continue

            # Load all frames for this field
            field_frames = []
            original_field_type = None  # Track if original was staggered

            for frame_idx in frames_to_load:
                field_obj = scene.read_field(
                    field_name,
                    frame=frame_idx,
                    convert_to_backend=True,  # Converts to torch backend
                )
                field_frames.append(field_obj)

            # Stack along time dimension
            stacked_field = stack(field_frames, batch("time"))

            # Convert to tensor in VTS Layout
            dims = 'vector,time,' + ','.join(stacked_field.shape.spatial.names)
            tensor = stacked_field.values.native(dims)
            tensor_data[field_name] = tensor.cpu()

            # Extract metadata needed to reconstruct the Field
            # We need: shape, domain, resolution, extrapolation, boundary, field_type
            first_field = field_frames[0]

            # Extract actual bounds values (not just string representation)
            bounds_lower = tuple(
                [
                    float(first_field.bounds.lower[i])
                    for i in range(len(first_field.bounds.lower))
                ]
            )
            bounds_upper = tuple(
                [
                    float(first_field.bounds.upper[i])
                    for i in range(len(first_field.bounds.upper))
                ]
            )

            field_metadata[field_name] = {
                "shape": str(first_field.shape),  # Full shape info
                "spatial_dims": list(first_field.shape.spatial.names),
                "channel_dims": (
                    list(first_field.shape.channel.names)
                    if first_field.shape.channel
                    else []
                ),
                "extrapolation": str(first_field.extrapolation),
                "bounds": str(first_field.bounds),  # Keep for reference
                "bounds_lower": bounds_lower,  # Actual lower bounds values
                "bounds_upper": bounds_upper,  # Actual upper bounds values
                "field_type": original_field_type,  # Store original field type (before conversion to centered)
            }

        cache_data = {
            "tensor_data": tensor_data,
            "metadata": {
                # Version and timestamp information
                "version": get_cache_version(),
                "created_at": datetime.now().isoformat(),
                "phiflow_version": get_phiflow_version(),
                # Scene and field metadata
                "scene_metadata": scene_metadata,
                "field_metadata": field_metadata,
                "num_frames": len(frames_to_load),
                "frame_indices": frames_to_load,
                # NEW: Generation parameters for validation
                "generation_params": {
                    "pde_name": self.config.get("model", {})
                    .get("physical", {})
                    .get("name", "unknown"),
                    "pde_params": self.config.get("model", {})
                    .get("physical", {})
                    .get("pde_params", {}),
                    "domain": self.config.get("model", {})
                    .get("physical", {})
                    .get("domain", {}),
                    "resolution": self.config.get("model", {})
                    .get("physical", {})
                    .get("resolution", {}),
                    "dt": self.config.get("model", {})
                    .get("physical", {})
                    .get("dt", 0.0),
                },
                "data_config": {
                    "fields": field_names,
                    "fields_scheme": self.config.get("data", {}).get(
                        "fields_scheme", "unknown"
                    ),
                    "dset_name": self.config.get("data", {}).get(
                        "dset_name", "unknown"
                    ),
                },
                "checksums": {
                    "pde_params_hash": compute_hash(
                        self.config.get("model", {})
                        .get("physical", {})
                        .get("pde_params", {})
                    ),
                    "resolution_hash": compute_hash(
                        self.config.get("model", {})
                        .get("physical", {})
                        .get("resolution", {})
                    ),
                    "domain_hash": compute_hash(
                        self.config.get("model", {})
                        .get("physical", {})
                        .get("domain", {})
                    ),
                },
            },
        }
        cache_path = self.get_cached_path(sim_index)
        torch.save(cache_data, cache_path)

        return cache_data

    def load_from_cache(self, sim_index: int) -> Dict[str, Any]:
        """
        Load cached tensor data for a simulation.

        Args:
            sim_index: Index of the simulation

        Returns:
            Dictionary with 'tensor_data' and 'metadata' keys

        Raises:
            FileNotFoundError: If cached data doesn't exist
        """
        cache_path = self.get_cached_path(sim_index)

        if not cache_path.exists():
            raise FileNotFoundError(
                f"Cached data not found at {cache_path}. "
                f"Call load_and_cache_simulation() first."
            )

        # Use weights_only=False because we're loading our own trusted data
        # and the metadata dict contains non-tensor types
        cached = torch.load(cache_path, weights_only=False)
        # Return cached data (validated)
        return cached

    def load_simulation(
        self, sim_index: int, field_names: List[str], num_frames: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Get simulation data, loading from cache if available, otherwise loading and caching.

        This method validates that cached data matches the requested parameters
        (field names and num_frames) before using it.

        Args:
            sim_index: Index of the simulation
            field_names: List of field names to load
            num_frames: Optional limit on number of frames

        Returns:
            Dictionary with 'tensor_data' and 'metadata' keys
        """
        # Check if cache exists AND matches requested parameters
        if self.is_cached(sim_index, field_names, num_frames):
            return self.load_from_cache(sim_index)
        else:
            return self.load_and_cache_simulation(sim_index, field_names, num_frames)


--- src/data/dataset_utilities.py ---

"""
Dataset Utilities - Shared Components

Extracted from AbstractDataset to reduce complexity:
- DatasetBuilder: Setup and validation
- AugmentationHandler: Load and process augmentation
- FilteringManager: Percentage filtering and resampling
"""

from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path
import random
import torch
from phi.field import Field

from .data_manager import DataManager
from src.utils.logger import get_logger

logger = get_logger(__name__)


class DatasetBuilder:
    """
    Handles dataset setup and validation.
    
    Responsibilities:
    - Cache validation and warming
    - Sliding window computation
    - Coordinate all setup components
    """
    
    def __init__(self, data_manager: DataManager):
        self.data_manager = data_manager
    
    def setup_cache(
        self,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: Optional[int],
        num_predict_steps: Optional[int] = None,
    ) -> int:
        """
        Validate cache and determine num_frames.
        
        Args:
            sim_indices: List of simulation indices
            field_names: List of field names
            num_frames: Optional number of frames
        
        Returns:
            Determined num_frames
        """
        # Determine num_frames if not specified
        if num_frames is None:
            logger.debug("num_frames not specified, determining from first simulation...")
            first_sim_data = self.data_manager.load_simulation(
                sim_indices[0], field_names=field_names, num_frames=None
            )
            sample_tensor = first_sim_data["tensor_data"][field_names[0]]
            # Detect BVTS canonical layout [B, C, T, *spatial] -> time at dim=2
            if isinstance(sample_tensor, torch.Tensor):
                if sample_tensor.dim() >= 3:
                    if sample_tensor.dim() == 5:
                        # BVTS with batch: [B, C, T, *spatial]
                        num_frames = sample_tensor.shape[2]
                    elif sample_tensor.dim() == 4:
                        # BVTS without batch per-field: [C, T, *spatial]
                        num_frames = sample_tensor.shape[1]
                    elif sample_tensor.dim() == 3:
                        # [C, H, W] -> single frame
                        num_frames = 1
                    else:
                        num_frames = sample_tensor.shape[0]
                else:
                    num_frames = 1
            else:
                raise RuntimeError("First simulation tensor is not a torch.Tensor")

            logger.debug(f"Determined num_frames = {num_frames}")
            del first_sim_data

        # If a predict horizon is provided and the discovered num_frames is
        # too small for at least one training window, attempt to force a
        # re-load with a larger num_frames. This handles stale/invalid cache
        # cases where metadata reports too few frames.
        if num_predict_steps is not None and num_frames < (num_predict_steps + 1):
            logger.warning(
                f"Discovered num_frames={num_frames} < required {num_predict_steps + 1}; "
                "attempting to reload simulation with larger frame count..."
            )
            try:
                # Ask DataManager to load at least the required number of frames
                forced = self.data_manager.load_simulation(
                    sim_indices[0], field_names=field_names, num_frames=(num_predict_steps + 1)
                )
                forced_tensor = forced["tensor_data"][field_names[0]]
                if isinstance(forced_tensor, torch.Tensor):
                    if forced_tensor.dim() == 5:
                        num_frames = forced_tensor.shape[2]
                    elif forced_tensor.dim() == 4:
                        # BVTS without batch per-field: [C, T, *spatial]
                        num_frames = forced_tensor.shape[1]
                    elif forced_tensor.dim() == 3:
                        num_frames = 1
                    else:
                        num_frames = forced_tensor.shape[0]
                else:
                    raise RuntimeError("Forced simulation tensor is not a torch.Tensor")

                logger.debug(f"After forced reload, num_frames = {num_frames}")
            except Exception as e:
                logger.error(f"Failed to reload simulation to satisfy predict steps: {e}")
                # Let the caller handle the invalid configuration
                raise
        
        # Check and cache uncached simulations
        uncached_sims = [
            sim_idx for sim_idx in sim_indices
            if not self.data_manager.is_cached(sim_idx)
        ]
        
        if uncached_sims:
            logger.info(f"Caching {len(uncached_sims)} simulations...")
            for i, sim_idx in enumerate(uncached_sims, 1):
                logger.debug(f"  [{i}/{len(uncached_sims)}] Caching simulation {sim_idx}...")
                _ = self.data_manager.load_simulation(
                    sim_idx, field_names=field_names, num_frames=num_frames
                )
            logger.debug("All simulations cached successfully!")
        else:
            logger.debug(f"All {len(sim_indices)} simulations already cached.")
        
        return num_frames
    
    def compute_sliding_window(
        self,
        num_frames: int,
        num_predict_steps: int
    ) -> int:
        """
        Compute samples per simulation for sliding window.
        
        Args:
            num_frames: Total frames in simulation
            num_predict_steps: Number of prediction steps
        
        Returns:
            Number of samples per simulation
        
        Raises:
            ValueError: If window configuration is invalid
        """
        if num_frames < num_predict_steps + 1:
            raise ValueError(
                f"num_frames ({num_frames}) must be >= num_predict_steps + 1 "
                f"({num_predict_steps + 1})"
            )
        
        samples_per_sim = num_frames - num_predict_steps
        
        if samples_per_sim <= 0:
            raise ValueError(
                f"Invalid sliding window: num_frames ({num_frames}) must be > "
                f"num_predict_steps ({num_predict_steps})"
            )
        
        return samples_per_sim


class AugmentationHandler:
    """
    Handles loading and processing of augmented samples.
    
    Responsibilities:
    - Load from memory or cache
    - Process raw trajectory data
    - Convert to appropriate format
    """
    
    @staticmethod
    def load_augmentation(
        config: Dict[str, Any],
        num_real: int,
        num_predict_steps: int,
        field_names: List[str]
    ) -> List[Any]:
        """
        Load augmented samples based on configuration.
        
        Args:
            config: Augmentation configuration dict
            num_real: Number of real samples (for alpha calculation)
            num_predict_steps: Number of prediction steps
            field_names: List of field names
        
        Returns:
            List of augmented samples (format depends on mode)
        """
        mode = config.get("mode", "cache")
        alpha = config.get("alpha", 0.0)
        
        logger.debug(f"Loading augmentation (mode={mode}, alpha={alpha})...")
        
        if mode == "memory":
            return AugmentationHandler._load_from_memory(
                config, num_predict_steps, field_names
            )
        elif mode == "cache":
            return AugmentationHandler._load_from_cache(
                config, num_real, alpha
            )
        else:
            raise ValueError(f"Unknown augmentation mode: {mode}")
    
    @staticmethod
    def _load_from_memory(
        config: Dict[str, Any],
        num_predict_steps: int,
        field_names: List[str]
    ) -> List[Any]:
        """Load augmentation from memory."""
        if "data" not in config:
            raise ValueError("Augmentation mode 'memory' requires 'data' key")
        
        data = config["data"]
        
        # Check if data is raw trajectories or processed samples
        if AugmentationHandler._is_trajectory_data(data):
            logger.debug("  Processing raw trajectory data...")
            samples = AugmentationHandler._process_trajectory_data(
                data, num_predict_steps, field_names
            )
        else:
            # Pre-processed samples
            samples = data
        
        logger.debug(f"  Loaded {len(samples)} augmented samples from memory")
        return samples
    
    @staticmethod
    def _load_from_cache(
        config: Dict[str, Any],
        num_real: int,
        alpha: float
    ) -> List[Any]:
        """Load augmentation from cache directory."""
        if "cache_dir" not in config:
            raise ValueError("Augmentation mode 'cache' requires 'cache_dir' key")
        
        cache_dir = config["cache_dir"]
        expected_count = int(num_real * alpha) if alpha > 0 else None
        
        cache_path = Path(cache_dir)
        
        if not cache_path.exists():
            raise FileNotFoundError(
                f"Augmentation cache not found: {cache_dir}\n"
                f"Please run cache generation first."
            )
        
        cache_files = sorted(cache_path.glob("sample_*.pt"))
        
        if len(cache_files) == 0:
            raise ValueError(
                f"No cached samples found in {cache_dir}\n"
                f"Expected to find sample_*.pt files."
            )
        
        if expected_count is not None and len(cache_files) != expected_count:
            logger.warning(
                f"Expected {expected_count} cached samples but found {len(cache_files)}"
            )
        
        samples = []
        for file_path in cache_files:
            try:
                data = torch.load(file_path, map_location="cpu")
                if isinstance(data, dict):
                    samples.append((data["input"], data["target"]))
                else:
                    samples.append(data)
            except Exception as e:
                logger.error(f"Failed to load {file_path}: {e}")
                raise
        
        logger.debug(f"  Loaded {len(samples)} cached augmented samples")
        return samples
    
    @staticmethod
    def _is_trajectory_data(data: Any) -> bool:
        """Check if data is raw trajectories vs processed samples."""
        if not isinstance(data, list) or len(data) == 0:
            return False
        
        first_item = data[0]
        
        # Check if it's a list of dicts (trajectory)
        if isinstance(first_item, list):
            if len(first_item) > 0 and isinstance(first_item[0], dict):
                return True
        
        return False
    
    @staticmethod
    def _process_trajectory_data(
        trajectories: List[List[Dict[str, Field]]],
        num_predict_steps: int,
        field_names: List[str]
    ) -> List[List[Dict[str, Field]]]:
        """
        Apply sliding window to raw trajectories.
        
        Returns list of windowed states (not yet converted to final format).
        Format: List[List[Dict[str, Field]]] where inner list has length
        num_predict_steps + 1.
        """
        logger.debug(
            f"  Windowing {len(trajectories)} trajectories with "
            f"num_predict_steps={num_predict_steps}"
        )
        
        all_windows = []
        
        for traj_idx, trajectory in enumerate(trajectories):
            traj_length = len(trajectory)
            
            if traj_length < num_predict_steps + 1:
                logger.warning(
                    f"  Trajectory {traj_idx} too short ({traj_length} steps), "
                    f"need at least {num_predict_steps + 1}. Skipping."
                )
                continue
            
            # Apply sliding window
            num_windows = traj_length - num_predict_steps
            
            for window_start in range(num_windows):
                window_states = trajectory[
                    window_start : window_start + num_predict_steps + 1
                ]
                all_windows.append(window_states)
        
        logger.debug(
            f"  Created {len(all_windows)} windowed samples from "
            f"{len(trajectories)} trajectories"
        )
        
        return all_windows


class FilteringManager:
    """
    Manages percentage-based filtering of real data.
    
    Responsibilities:
    - Apply percentage filter to create index mapping
    - Resample indices for different epochs
    - Map filtered indices to actual indices
    """
    
    def __init__(
        self,
        total_samples: int,
        percentage: float,
        seed: Optional[int] = None
    ):
        """
        Initialize filtering manager.
        
        Args:
            total_samples: Total number of available samples
            percentage: Percentage of data to use (0.0 < percentage <= 1.0)
            seed: Optional random seed
        """
        if not 0.0 < percentage <= 1.0:
            raise ValueError(
                f"percentage must be in (0.0, 1.0], got {percentage}"
            )
        
        self.total_samples = total_samples
        self.percentage = percentage
        self.num_samples = int(total_samples * percentage)
        
        # Generate initial index mapping
        self._active_indices = self._generate_indices(seed)
    
    def _generate_indices(self, seed: Optional[int]) -> List[int]:
        """Generate random indices."""
        if seed is not None:
            random.seed(seed)
        
        all_indices = list(range(self.total_samples))
        random.shuffle(all_indices)
        return sorted(all_indices[:self.num_samples])
    
    def get_actual_index(self, filtered_idx: int) -> int:
        """
        Map filtered index to actual index.
        
        Args:
            filtered_idx: Index in filtered dataset (0 to num_samples-1)
        
        Returns:
            Actual index in full dataset
        """
        if filtered_idx >= self.num_samples:
            raise IndexError(
                f"Filtered index {filtered_idx} out of range [0, {self.num_samples})"
            )
        return self._active_indices[filtered_idx]
    
    def resample(self, seed: Optional[int] = None):
        """
        Resample the subset of data.
        
        Args:
            seed: Optional random seed for reproducibility
        """
        self._active_indices = self._generate_indices(seed)
        logger.debug(
            f"Resampled indices: using {self.num_samples}/{self.total_samples} "
            f"samples ({self.percentage*100:.1f}%)"
        )
    
    def get_info(self) -> Dict[str, Any]:
        """Get information about filtering."""
        return {
            "total_samples": self.total_samples,
            "filtered_samples": self.num_samples,
            "percentage": self.percentage,
        }

from dataclasses import dataclass
from typing import Dict, Union, Tuple
from phi.field import Field
from phi.geom import Box
from phi.math import Shape
from phi.field._field_math import Extrapolation


@dataclass
class FieldMetadata:
    """
    Metadata needed to reconstruct a PhiFlow Field from a tensor.

    This stores all the information required to convert a PyTorch tensor
    back into its original Field representation.

    Attributes:
        domain: The physical domain (Box) for the field
        resolution: The spatial resolution (Shape with x, y dimensions)
        extrapolation: Boundary condition (e.g., 'periodic', 'zero-gradient', etc.)
        field_type: 'centered' or 'staggered'
        spatial_dims: Names of spatial dimensions (e.g., ['x', 'y'])
        channel_dims: Names of channel dimensions (e.g., ['vector'])
    """

    domain: Box
    resolution: Shape
    extrapolation: Union[Extrapolation, str]
    field_type: str  # 'centered' or 'staggered'
    spatial_dims: Tuple[str, ...]
    channel_dims: Tuple[str, ...]

    @classmethod
    def from_field(cls, field: Field) -> "FieldMetadata":
        """
        Extract metadata from an existing Field.

        Args:
            field: The Field to extract metadata from

        Returns:
            FieldMetadata object
        """
        # Determine field type using is_staggered property
        field_type = "staggered" if field.is_staggered else "centered"

        return cls(
            domain=field.bounds,
            resolution=field.resolution,
            extrapolation=field.extrapolation,
            field_type=field_type,
            spatial_dims=tuple(field.shape.spatial.names),
            channel_dims=(
                tuple(field.shape.channel.names) if field.shape.channel else ()
            ),
        )

    @classmethod
    def from_cache_metadata(
        cls, cached_meta: Dict, domain: Box, resolution: Shape
    ) -> "FieldMetadata":
        """
        Reconstruct FieldMetadata from cached metadata dictionary.

        Args:
            cached_meta: Dictionary containing field metadata from cache
            domain: The physical domain (must be provided externally)
            resolution: The spatial resolution (must be provided externally)

        Returns:
            FieldMetadata object
        """
        # Parse extrapolation from string
        extrap_str = cached_meta.get("extrapolation", "ZERO")

        # Map common extrapolation strings to PhiFlow objects
        from phi.math import extrapolation as extrap_module

        extrapolation_map = {
            "ZERO": extrap_module.ZERO,
            "BOUNDARY": extrap_module.BOUNDARY,
            "PERIODIC": extrap_module.PERIODIC,
            "zero-gradient": extrap_module.ZERO_GRADIENT,
            "ZERO_GRADIENT": extrap_module.ZERO_GRADIENT,
        }

        # Try to parse the extrapolation
        if extrap_str in extrapolation_map:
            extrapolation = extrapolation_map[extrap_str]
        else:
            # Try to extract the extrapolation name from a string like "<ZERO>"
            for key in extrapolation_map:
                if key in extrap_str.upper():
                    extrapolation = extrapolation_map[key]
                    break
            else:
                extrapolation = extrap_module.ZERO  # Default fallback

        # Determine field type (default to centered if not specified)
        field_type = cached_meta.get("field_type", "centered")

        return cls(
            domain=domain,
            resolution=resolution,
            extrapolation=extrapolation,
            field_type=field_type,
            spatial_dims=tuple(cached_meta.get("spatial_dims", ["x", "y"])),
            channel_dims=tuple(cached_meta.get("channel_dims", [])),
        )


def create_field_metadata_from_model(
    model, field_names: list[str], field_types: Dict[str, str] = None
) -> Dict[str, FieldMetadata]:
    """
    Create FieldMetadata for each field from a PhysicalModel instance.

    This is useful for physical trainers that need to convert tensors
    back to Fields for use with the model.

    Args:
        model: PhysicalModel instance with domain, resolution attributes
        field_names: List of field names (e.g., ['velocity', 'density'])
        field_types: Optional dict mapping field names to types ('centered' or 'staggered')
                    Defaults to 'centered' for all fields

    Returns:
        Dictionary mapping field names to FieldMetadata

    Example:
        >>> model = BurgersModel(domain=Box(...), resolution=spatial(x=128, y=128), ...)
        >>> metadata = create_field_metadata_from_model(model, ['velocity'], {'velocity': 'staggered'})
    """
    from phi.math import extrapolation

    field_types = field_types or {}

    metadata_dict = {}
    for field_name in field_names:
        field_type = field_types.get(field_name, "centered")

        # Determine channel dimensions based on common field types
        if "velocity" in field_name.lower():
            channel_dims = ("vector",)  # Velocity is typically a vector field
        else:
            channel_dims = ()  # Scalar field

        metadata_dict[field_name] = FieldMetadata(
            domain=model.domain,
            resolution=model.resolution,
            extrapolation=extrapolation.PERIODIC,  # Default, may need to be configurable
            field_type=field_type,
            spatial_dims=tuple(model.resolution.names),
            channel_dims=channel_dims,
        )

    return metadata_dict

    

from phi.torch.flow import stack, batch
from typing import List, Dict, Tuple
from phi.field import Field

# In dataset_utilities.py or dataloader_factory.py

def tensor_collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]):
    """
    Collate per-sample tensors [V, T, H, W] into batches [B, V, T, H, W].
    
    Args:
        batch: List of (initial, targets) tuples, each [V, T, H, W]
    
    Returns:
        (batched_initial, batched_targets) each [B, V, T, H, W]
    """
    initials, targets = zip(*batch)

    logger.info(f"Device tensor_collate_fn: batching {len(batch)} samples on device {initials[0].device, targets[0].device}, ")
    
    # Stack along new batch dimension
    batched_initial = torch.stack(initials, dim=0)  # [B, V, T_init, H, W]
    batched_targets = torch.stack(targets, dim=0)   # [B, V, T_pred, H, W]
    
    return batched_initial, batched_targets


def field_collate_fn(field_batch: List[Tuple[Dict[str, Field], Dict[str, List[Field]]]]):
    """
    Collate per-sample Fields into batched Fields.
    
    Args:
        batch: List of (initial_fields, target_fields) tuples
    
    Returns:
        (batched_initial, batched_targets) with batch dimension in Fields
    """

    initial_fields, target_fields = zip(*field_batch)
    
    # Stack initial fields
    field_names = initial_fields[0].keys()
    stacked_initial = {}
    stacked_targets = {}
    for name in field_names:
        initial = [sample[name] for sample in initial_fields]
        targets = [sample[name] for sample in target_fields]
        stacked_initial[name] = stack(initial, batch('batch'))
        stacked_targets[name] = stack(targets, batch('batch'))
  
    return stacked_initial, stacked_targets


--- src/data/field_dataset.py ---

"""
FieldDataset - Refactored for Synthetic Prediction Augmentation

Key Changes:
- Augmented data = synthetically-generated prediction trajectories
- Trajectories stored in cache format: {'tensor_data': {field: [C, T, H, W]}}
- _get_augmented_sample() applies windowing to trajectories
- Clear distinction maintained between real and synthetically-generated data
"""

from typing import List, Optional, Dict, Any, Tuple
import torch
from phi.field import Field
from phi.torch.flow import *

from .abstract_dataset import AbstractDataset
from .data_manager import DataManager
from .dataset_utilities import DatasetBuilder, AugmentationHandler, FilteringManager, FieldMetadata
from src.utils.logger import get_logger
from phi.geom import Box
from phi.math import spatial

logger = get_logger(__name__)


class FieldDataset(AbstractDataset):
    """
    PyTorch Dataset that returns PhiFlow Fields for physical training.
    
    Augmentation source: Synthetically-generated prediction trajectories
    - Stored as cache-format dicts: {'tensor_data': {field: [C, T, H, W]}}
    - Windowed on-the-fly during access
    - Remain distinguishable via _is_augmented_prediction()
    """
    
    def __init__(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: Optional[int],
        num_predict_steps: int,
        augmentation_config: Optional[Dict[str, Any]] = None,
        access_policy: str = "both",
        max_cached_sims: int = 5,
        percentage_real_data: float = 1.0,
    ):
        """Initialize FieldDataset."""
        self._field_metadata_cache = None
        self._num_augmented_trajectories = 0
        
        logger.debug("Setting up FieldDataset...")
        num_frames, num_real, augmented_samples, index_mapper = self._setup_dataset(
            data_manager, sim_indices, field_names, num_frames, num_predict_steps,
            augmentation_config, percentage_real_data
        )
        
        super().__init__(
            data_manager=data_manager,
            sim_indices=sim_indices,
            field_names=field_names,
            num_frames=num_frames,
            num_predict_steps=num_predict_steps,
            access_policy=access_policy,
            num_real=num_real,
            augmented_samples=augmented_samples,
            index_mapper=index_mapper,
            max_cached_sims=max_cached_sims,
        )
        
        self._log_dataset_info()

    # ==================== Setup ====================
    
    def _setup_dataset(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: Optional[int],
        num_predict_steps: int,
        augmentation_config: Optional[Dict[str, Any]],
        percentage_real_data: float,
    ) -> Tuple[int, int, List[Any], Optional[FilteringManager]]:
        """Setup dataset components."""
        builder = DatasetBuilder(data_manager)
        
        num_frames = builder.setup_cache(sim_indices, field_names, num_frames, num_predict_steps)
        samples_per_sim = builder.compute_sliding_window(num_frames, num_predict_steps)
        total_real_samples = len(sim_indices) * samples_per_sim
        
        # Setup filtering
        index_mapper = None
        if percentage_real_data < 1.0:
            index_mapper = FilteringManager(total_real_samples, percentage_real_data)
            num_real = index_mapper.num_samples
            logger.debug(
                f"  Filtering enabled: using {num_real}/{total_real_samples} "
                f"samples ({percentage_real_data*100:.1f}%)"
            )
        else:
            num_real = total_real_samples
        
        # Setup augmentation (if provided)
        augmented_samples = []
        if augmentation_config:
            raw_samples = AugmentationHandler.load_augmentation(
                augmentation_config, num_real, num_predict_steps, field_names
            )
            augmented_samples = raw_samples  # Already in correct format
        
        return num_frames, num_real, augmented_samples, index_mapper
    
    def _log_dataset_info(self):
        """Log dataset information."""
        if self.access_policy == "both":
            logger.debug(
                f"  Dataset: {self.num_real} real + {self.num_augmented} "
                f"augmented = {len(self)} samples"
            )
        elif self.access_policy == "real_only":
            logger.debug(f"  Dataset: {self.num_real} real samples")
        elif self.access_policy == "generated_only":
            if self.num_augmented == 0:
                logger.warning(
                    "  Dataset: access_policy=generated_only but no augmented samples!"
                )
            logger.debug(f"  Dataset: {self.num_augmented} generated samples")

    # ==================== Implementation of Abstract Methods ====================
    
    def _load_simulation(self, sim_idx: int) -> Dict[str, Any]:
        """Load simulation data from cache."""
        full_data = self.data_manager.load_simulation(
            sim_idx, field_names=self.field_names, num_frames=self.num_frames
        )
        return full_data
    
    def _extract_sample(self, idx: int) -> Tuple[Dict[str, Field], Dict[str, List[Field]]]:
        """
        Extract a windowed sample from real simulations as Fields.
        
        Returns:
            Tuple of (initial_fields, target_fields)
        """
        sim_idx, start_frame = self._compute_sim_and_frame(idx)
        data = self._cached_load_simulation(sim_idx)
        
        # Move to GPU if needed (temporary)
        for name in data["tensor_data"]:
            data["tensor_data"][name] = data["tensor_data"][name].to("cuda")

        # Reconstruct field metadata
        field_metadata = self._reconstruct_metadata(data)
        
        # Convert initial state (single frame)
        initial_fields = self._tensors_to_fields(
            data, field_metadata, start_frame, start_frame + 1
        )
        
        # Convert target rollout (multiple frames)
        target_start = start_frame + 1
        target_end = start_frame + 1 + self.num_predict_steps
        target_fields = self._tensors_to_fields(
            data, field_metadata, target_start, target_end
        )

        return initial_fields, target_fields
    
    def _get_augmented_sample(self, idx: int) -> Tuple[Dict[str, Field], Dict[str, List[Field]]]:
        """
        Get a synthetically-generated prediction sample.
        
        UPDATED: Now handles trajectory format with windowing.
        Trajectories are stored as cache-format dicts that can be indexed.
        
        Args:
            idx: Index within augmented samples
        
        Returns:
            Tuple of (initial_fields, target_fields) as Fields
        """
        # Compute which trajectory and which window within that trajectory
        samples_per_traj = self.num_frames - self.num_predict_steps
        if samples_per_traj <= 0:
            raise ValueError(
                f"Invalid configuration: num_frames={self.num_frames} must be > "
                f"num_predict_steps={self.num_predict_steps}"
            )
        
        traj_idx = idx // samples_per_traj
        window_start = idx % samples_per_traj
        
        if traj_idx >= self._num_augmented_trajectories:
            raise IndexError(
                f"Augmented index {idx} out of range "
                f"(trajectory {traj_idx} >= {self._num_augmented_trajectories})"
            )
        
        # Get the trajectory data (in cache format)
        trajectory_data = self.augmented_samples[traj_idx]
        
        # Extract tensor_data
        if isinstance(trajectory_data, dict) and 'tensor_data' in trajectory_data:
            tensor_data = trajectory_data['tensor_data']
        else:
            # Assume it's already tensor_data
            tensor_data = trajectory_data
        
        # Move to GPU if needed (temporary)
        for field_name in self.field_names:
            if isinstance(tensor_data[field_name], torch.Tensor):
                tensor_data[field_name] = tensor_data[field_name].to("cuda")
        
        # Get field metadata for conversion
        field_metadata = self._get_field_metadata()
        
        # Convert windowed portion to Fields
        # tensor_data format: {field_name: tensor[C, T, H, W]}
        
        # Extract initial state (single frame at window_start)
        initial_fields = self._tensors_to_fields(
            trajectory_data,
            field_metadata,
            window_start,
            window_start + 1
        )
        # Extract target trajectory
        target_start = window_start + 1
        target_end = window_start + 1 + self.num_predict_steps
        target_fields = self._tensors_to_fields(
            trajectory_data,
            field_metadata,
            target_start,
            target_end
        )
        
        return initial_fields, target_fields

    # ==================== Augmentation Management ====================

    def set_augmented_trajectories(
        self,
        trajectory_rollouts: List[Dict[str, torch.Tensor]]
    ):
        """
        Set augmented data from synthetically-generated prediction trajectories.
        
        NEW BEHAVIOR:
        - Input: List of cache-format dicts with tensor trajectories
        - Format: [{'tensor_data': {field_name: tensor[C, T, H, W]}}]
        - Trajectories can be windowed using the same indexing logic
        
        Args:
            trajectory_rollouts: List of trajectory dicts in cache format
        """
        if not trajectory_rollouts:
            self.augmented_samples = []
            self.num_augmented = 0
            self._num_augmented_trajectories = 0
            self._total_length = self._compute_length()
            return
        
        logger.debug(f"Setting {len(trajectory_rollouts)} synthetic prediction trajectories...")
        
        # Store trajectories directly (already in cache format)
        self.augmented_samples = trajectory_rollouts
        self._num_augmented_trajectories = len(trajectory_rollouts)
        
        # Calculate total augmented samples (with windowing)
        # Each trajectory of length T can produce (T - num_predict_steps) windows
        first_traj = trajectory_rollouts[0]
        
        # Extract tensor_data to check trajectory length
        if isinstance(first_traj, dict) and 'tensor_data' in first_traj:
            first_field = list(first_traj['tensor_data'].values())[0]
        else:
            first_field = list(first_traj.values())[0]
        
        traj_length = first_field.shape[1]  # T dimension in [C, T, H, W]
        
        samples_per_trajectory = traj_length - self.num_predict_steps
        if samples_per_trajectory <= 0:
            logger.warning(
                f"Trajectory length {traj_length} too short for "
                f"num_predict_steps={self.num_predict_steps}"
            )
            self.num_augmented = 0
        else:
            self.num_augmented = self._num_augmented_trajectories * samples_per_trajectory
        
        # Recompute total length
        self._total_length = self._compute_length()
        
        logger.debug(
            f"Set {self._num_augmented_trajectories} synthetic trajectories "
            f"({self.num_augmented} windowed samples, {samples_per_trajectory} per trajectory)"
        )

    def clear_augmented_trajectories(self):
        """Clear all augmented trajectories."""
        self.augmented_samples = []
        self.num_augmented = 0
        self._num_augmented_trajectories = 0
        self._total_length = self._compute_length()
        logger.debug("Cleared augmented trajectories")

    # ==================== Helper Methods ====================
    
    def _reconstruct_metadata(self, data: Dict[str, Any]) -> Dict[str, FieldMetadata]:
        """Reconstruct FieldMetadata from cached metadata."""

        
        field_metadata_dict = data["metadata"]["field_metadata"]
        field_metadata = {}
        
        for name, meta in field_metadata_dict.items():
            if "bounds_lower" in meta and "bounds_upper" in meta:
                lower = meta["bounds_lower"]
                upper = meta["bounds_upper"]
                
                if len(lower) == 2:
                    domain = Box(x=(lower[0], upper[0]), y=(lower[1], upper[1]))
                elif len(lower) == 3:
                    domain = Box(
                        x=(lower[0], upper[0]),
                        y=(lower[1], upper[1]),
                        z=(lower[2], upper[2])
                    )
                else:
                    domain = Box(x=1, y=1)
            else:
                raise ValueError(
                    f"Invalid cache format for field '{name}'. "
                    f"Missing bounds information."
                )
            
            sample_tensor = data["tensor_data"][name]
            tensor_shape = sample_tensor.shape
            spatial_dims = meta["spatial_dims"]
            
            # Determine spatial offset depending on tensor layout
            if isinstance(sample_tensor, torch.Tensor):
                if sample_tensor.dim() == 4:
                    spatial_offset = 2  # [C, T, H, W]
                elif sample_tensor.dim() == 3:
                    spatial_offset = 1  # [C, H, W]
                else:
                    spatial_offset = 2  # Fallback
            else:
                spatial_offset = 2

            resolution_sizes = {
                dim: tensor_shape[i + spatial_offset] for i, dim in enumerate(spatial_dims)
            }
            resolution = spatial(**resolution_sizes)
            
            field_metadata[name] = FieldMetadata.from_cache_metadata(
                meta, domain, resolution
            )
        
        return field_metadata

    def _tensors_to_fields(
        self,
        data: Dict[str, Any],
        field_metadata: Dict[str, FieldMetadata],
        start_frame: int,
        end_frame: int
    ) -> Dict[str, Field]:
        """
        Convert windowed tensors to Fields.
        
        Args:
            data: Data dict with 'tensor_data'
            field_metadata: Metadata for reconstruction
            start_frame: Start of time window
            end_frame: End of time window (exclusive)
            
        Returns:
            Dict mapping field names to Field objects
        """
        fields_dict = {}
        
        for name in self.field_names:
            sample_tensor = data["tensor_data"][name]  # [C, T, H, W]
            
            # Validate shape
            assert sample_tensor.dim() == 4, \
                f"Expected tensor [C, T, H, W], got {sample_tensor.shape}"
            
            # Slice time window
            field_meta = field_metadata[name]
            window_tensor = sample_tensor[:, start_frame:end_frame, :, :]  # [C, T_window, H, W]
            
            # Create PhiML tensor with explicit dimensions
            phiml_tensor = math.tensor(
                window_tensor, 
                channel("vector"), 
                batch("time"), 
                spatial(*field_meta.spatial_dims)
            )
            
            # Create Field
            window_field = CenteredGrid(
                phiml_tensor, 
                field_meta.extrapolation, 
                bounds=field_meta.domain
            )
            
            fields_dict[name] = window_field
        
        return fields_dict
        
    def _get_field_metadata(self) -> Dict[str, FieldMetadata]:
        """Get field metadata (cached)."""
        if self._field_metadata_cache is not None:
            return self._field_metadata_cache
        
        first_sim_idx = self.sim_indices[0]
        data = self._cached_load_simulation(first_sim_idx)
        
        self._field_metadata_cache = self._reconstruct_metadata(data)
        return self._field_metadata_cache

    # ==================== Utility Methods ====================

    def _is_augmented_prediction(self, idx: int) -> bool:
        """
        Check if a sample index corresponds to an augmented prediction.
        
        Args:
            idx: Global sample index
        
        Returns:
            True if from augmented prediction, False if real
        """
        if self.access_policy == "generated_only":
            return True
        elif self.access_policy == "real_only":
            return False
        else:  # 'both'
            return idx >= self.num_real

    def get_sample_source(self, idx: int) -> str:
        """
        Get the source of a sample.
        
        Args:
            idx: Sample index
        
        Returns:
            'real' or 'synthetic_generated'
        """
        return 'synthetic_generated' if self._is_augmented_prediction(idx) else 'real'

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"FieldDataset(\n"
            f"  simulations={len(self.sim_indices)},\n"
            f"  samples={len(self)} (real={self.num_real}, aug={self.num_augmented}),\n"
            f"  fields={len(self.field_names)},\n"
            f"  frames={self.num_frames},\n"
            f"  predict_steps={self.num_predict_steps}\n"
            f")"
        )

--- src/data/generator.py ---

# src/data_generation/generator_scene.py

import os
import yaml
from tqdm import tqdm
import random

# --- PhiFlow Imports ---
from phi.torch.flow import *

# --- Our Model Imports ---
import src.models.physical as physical_models
from matplotlib import pyplot as plt


def get_physical_model(config: dict) -> physical_models.PhysicalModel:
    """
    Dynamically imports and instantiates a physical model from config.
    """
    phys_model_cfg = config["model"]["physical"]
    model_name = phys_model_cfg["name"]

    try:
        ModelClass = getattr(physical_models, model_name)
    except AttributeError:
        raise ImportError(
            f"Model '{model_name}' not found in src/models/physical/__init__.py"
        )

    # Pass the physical model config directly - the base class handles parsing
    model = ModelClass(phys_model_cfg)
    return model


def run_generation(config: dict):
    """
    Main function to run data generation based on a config,
    saving the output to a phi.vis.Scene directory.
    """
    # Get parameters from the UNIFIED config schema
    gen_cfg = config["generation_params"]
    data_cfg = config["data"]
    model_cfg = config["model"]["physical"]
    project_root = config["project_root"]

    # --- Setup Output Directory ---
    output_dir = os.path.join(project_root, data_cfg["data_dir"], data_cfg["dset_name"])
    os.makedirs(output_dir, exist_ok=True)

    print(f"Starting {gen_cfg['num_simulations']} simulations.")
    print(f"Scene data will be saved in: {output_dir}")

    # --- Main Simulation Loop ---
    for i in tqdm(range(gen_cfg["num_simulations"]), desc="Total Simulations"):

        scene = Scene.create(output_dir, copy_calling_script=False)

        # --- 2. Save metadata ---
        saved_dt = model_cfg["dt"] * gen_cfg["save_interval"]

        metadata = {
            "PDE": model_cfg["name"],
            "Fields": data_cfg["fields"],
            "Fields_Scheme": data_cfg.get("fields_scheme", "unknown"),
            "Dt": float(saved_dt),
            "Domain": model_cfg["domain"],
            "Resolution": model_cfg["resolution"],
            "PDE_Params": model_cfg.get("pde_params", {}),
            "Generation_Params": gen_cfg,
        }

        scene.put_properties(metadata)

        # --- 3. Get the physical model ---
        model = get_physical_model(config)

        # --- 4. Get initial state (t=0) ---
        current_state_dict = model.get_initial_state()

        # --- 5. Write initial frame (frame 0) ---
        state_to_save = {}
        for name in data_cfg["fields"]:
            # Remove batch dimension for saving
            state_to_save[name] = current_state_dict[name].batch[0]

        scene.write(state_to_save, frame=0)

        # --- 6. Run simulation steps ---
        save_interval = gen_cfg["save_interval"]

        for t in tqdm(
            range(1, gen_cfg["total_steps"] + 1), desc=f"Sim {i} Steps", leave=False
        ):

            # Step the model forward
            current_state_dict = model.forward(current_state_dict)

            # Save at intervals
            if t % save_interval == 0:
                frame_index = t // save_interval

                state_to_save = {}
                for name in data_cfg["fields"]:
                    state_to_save[name] = current_state_dict[name].batch[0]

                scene.write(state_to_save, frame=frame_index)

    print(f"\nScene generation complete. Data saved in {output_dir}")


--- src/data/tensor_dataset.py ---

"""
TensorDataset - Refactored for Physical Trajectory Augmentation

Key Changes:
- Augmented data = physically-generated trajectories (stored as cache-format)
- These trajectories are windowed exactly like real data
- _get_augmented_sample() applies sliding window to trajectory data
- Clear distinction maintained between real and physically-generated data
"""

from typing import List, Optional, Dict, Any, Tuple
import torch
from phi.field import Field

from .abstract_dataset import AbstractDataset
from .data_manager import DataManager
from .dataset_utilities import DatasetBuilder, AugmentationHandler, FilteringManager
from src.utils.logger import get_logger, logging

logger = get_logger(__name__)


class TensorDataset(AbstractDataset):
    """
    PyTorch Dataset that returns tensors for synthetic training.
    
    Augmentation source: Physically-generated trajectories
    - Stored in cache-compatible format: {'tensor_data': {field: tensor[T,C,H,W]}}
    - Windowed exactly like real data
    - Remain distinguishable via _is_augmented_trajectory()
    """
    
    def __init__(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: Optional[int],
        num_predict_steps: int,
        augmentation_config: Optional[Dict[str, Any]] = None,
        access_policy: str = "both",
        max_cached_sims: int = 5,
        pin_memory: bool = True,
        percentage_real_data: float = 1.0,
    ):
        """Initialize TensorDataset."""
        self.pin_memory = pin_memory and torch.cuda.is_available()
        
        # Setup dataset using builder
        logger.debug("Setting up TensorDataset...")
        num_frames, num_real, augmented_samples, index_mapper = self._setup_dataset(
            data_manager, sim_indices, field_names, num_frames, num_predict_steps,
            augmentation_config, percentage_real_data
        )
        
        # Call parent constructor
        super().__init__(
            data_manager=data_manager,
            sim_indices=sim_indices,
            field_names=field_names,
            num_frames=num_frames,
            num_predict_steps=num_predict_steps,
            access_policy=access_policy,
            num_real=num_real,
            augmented_samples=augmented_samples,
            index_mapper=index_mapper,
            max_cached_sims=max_cached_sims,
        )
        
        # Track number of augmented trajectories (for indexing)
        self._num_augmented_trajectories = len(augmented_samples)
        self._samples_per_trajectory = num_frames - num_predict_steps


        self._log_dataset_info()


    # ==================== Setup ====================
    
    def _setup_dataset(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: Optional[int],
        num_predict_steps: int,
        augmentation_config: Optional[Dict[str, Any]],
        percentage_real_data: float,
    ) -> Tuple[int, int, List[Any], Optional[FilteringManager]]:
        """Setup dataset components."""
        builder = DatasetBuilder(data_manager)

        num_frames = builder.setup_cache(sim_indices, field_names, num_frames, num_predict_steps)
        samples_per_sim = builder.compute_sliding_window(num_frames, num_predict_steps)
        total_real_samples = len(sim_indices) * samples_per_sim
        
        # Setup filtering
        index_mapper = None
        if percentage_real_data < 1.0:
            index_mapper = FilteringManager(total_real_samples, percentage_real_data)
            num_real = index_mapper.num_samples
            logger.debug(
                f"  Filtering enabled: using {num_real}/{total_real_samples} "
                f"samples ({percentage_real_data*100:.1f}%)"
            )
        else:
            num_real = total_real_samples
        
        # Setup augmentation (if provided)
        augmented_samples = []
        if augmentation_config:
            augmented_samples = AugmentationHandler.load_augmentation(
                augmentation_config, num_real, num_predict_steps, field_names
            )
        
        return num_frames, num_real, augmented_samples, index_mapper
    
    def _log_dataset_info(self):
        """Log dataset information."""
        if self.access_policy == "both":
            logger.debug(
                f"  Dataset: {self.num_real} real + {self.num_augmented} "
                f"augmented = {len(self)} samples"
            )
        elif self.access_policy == "real_only":
            logger.debug(f"  Dataset: {self.num_real} real samples")
        elif self.access_policy == "generated_only":
            if self.num_augmented == 0:
                logger.warning(
                    "  Dataset: access_policy=generated_only but no augmented samples!"
                )
            logger.debug(f"  Dataset: {self.num_augmented} generated samples")

    # ==================== Implementation of Abstract Methods ====================
    
    def _load_simulation(self, sim_idx: int) -> Dict[str, torch.Tensor]:
        """
        Load simulation data from cache.
        
        Returns tensor_data dict directly.
        """
        full_data = self.data_manager.load_simulation(
            sim_idx, field_names=self.field_names, num_frames=self.num_frames
        )
        
        sim_data = full_data["tensor_data"]


        if self.pin_memory:
            sim_data = {field: tensor.pin_memory() if isinstance(tensor, torch.Tensor) else tensor for field, tensor in sim_data.items()}

        return sim_data
    
    def _extract_sample(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Extract a windowed sample from real simulations.
        
        Returns:
            Tuple of (initial_state, rollout_targets)
        """
        sim_idx, start_frame = self._compute_sim_and_frame(idx)
        sim_data = self._cached_load_simulation(sim_idx)

        
        # Concatenate all fields
        all_field_tensors = [sim_data[name] for name in self.field_names]

        all_data = torch.cat(all_field_tensors, dim=0)  # [C_all, T, H, W]

        # Extract window
        initial_state = all_data[:, start_frame:start_frame+1, :, :]  # [V, 1, H, W]
        target_start = start_frame + 1
        target_end = start_frame + 1 + self.num_predict_steps
        rollout_targets = all_data[:, target_start:target_end, :, :]  # [V, T_pred, H, W]
        
        return initial_state, rollout_targets
    
    def _get_augmented_sample(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get sample from augmented (physically-generated) trajectories.
        
        These are stored as full trajectories and windowed on-the-fly,
        exactly like real data.
        
        Args:
            idx: Index within augmented samples
        
        Returns:
            Tuple of (initial_state, rollout_targets)
        """
        # Compute which trajectory and which window within that trajectory
        traj_idx = idx // self._samples_per_trajectory
        window_start = idx % self._samples_per_trajectory
        
        if traj_idx >= self._num_augmented_trajectories:
            raise IndexError(
                f"Augmented index {idx} out of range "
                f"(trajectory {traj_idx} >= {self._num_augmented_trajectories})"
            )
        
        # Get the trajectory data (in cache format)
        trajectory_data = self.augmented_samples[traj_idx]
        
        # Extract tensor_data
        if isinstance(trajectory_data, dict) and 'tensor_data' in trajectory_data:
            tensor_data = trajectory_data['tensor_data']
        else:
            # Assume it's already tensor_data
            tensor_data = trajectory_data
        
        # Concatenate all fields
        all_field_tensors = [tensor_data[name] for name in self.field_names]

        # Augmented trajectories are stored in BVTS cache-format and have been
        # normalized by _normalize_sim_tensor_data earlier. Expect [C_all, T, H, W]
        all_data = torch.cat(all_field_tensors, dim=0).cpu()  # [C_all, T, H, W]


        initial_state = all_data[:, window_start : window_start + 1]  # [C_all, 1, H, W]
        target_start = window_start + 1
        target_end = window_start + 1 + self.num_predict_steps
        rollout_targets = all_data[:, target_start:target_end]  # [C_all, T_pred, H, W]

        return initial_state, rollout_targets

    # ==================== Augmentation Management ====================

    def set_augmented_trajectories(self, trajectory_rollouts: List[Dict[str, Field]]):
        """
        Set augmented data from physically-generated Field trajectories.
        
        Converts Field rollouts to cache-compatible tensor format.
        
        Args:
            trajectory_rollouts: List of rollout dicts where each is
                                {'field_name': Field[time, x, y]}
        """
        if not trajectory_rollouts:
            self.augmented_samples = []
            self.num_augmented = 0
            self._num_augmented_trajectories = 0
            self._total_length = self._compute_length()
            return
        
        logger.debug(f"Converting {len(trajectory_rollouts)} physical trajectories...")
        
        # Convert each Field trajectory to cache format
        converted_trajectories = []
        for idx, rollout in enumerate(trajectory_rollouts):
            if idx % 10 == 0 and idx > 0:
                logger.debug(f"  Converted {idx}/{len(trajectory_rollouts)} trajectories...")
            
            tensor_trajectory =  {field_name: rollout[field_name].values.native('vector,time,x,y') for field_name in self.field_names if field_name in rollout}
            converted_trajectories.append(tensor_trajectory)
        
        # Store trajectories
        self.augmented_samples = converted_trajectories
        self._num_augmented_trajectories = len(converted_trajectories)
        
        # Calculate total augmented samples (with windowing)
        self.num_augmented = self._num_augmented_trajectories * self._samples_per_trajectory
        logger.debug(
            f"Set {self._num_augmented_trajectories} augmented trajectories "
            f"{self._samples_per_trajectory} samples each = "
            f"({self.num_augmented} windowed samples)"
        )
        # Recompute total length
        self._total_length = self._compute_length()
        
        logger.debug(
            f"Set {self._num_augmented_trajectories} augmented trajectories "
            f"({self.num_augmented} windowed samples)"
            f"; total dataset length is now {self._total_length} samples"
            f", where it should be {self.num_augmented + self.num_real} samples."
        )

    def clear_augmented_trajectories(self):
        """Clear all augmented trajectories."""
        self.augmented_samples = []
        self.num_augmented = 0
        self._num_augmented_trajectories = 0
        self._total_length = self._compute_length()
        logger.debug("Cleared augmented trajectories")

    # ==================== Utility Methods ====================

    def _is_augmented_trajectory(self, idx: int) -> bool:
        """
        Check if a sample index corresponds to an augmented trajectory.
        
        Args:
            idx: Global sample index
        
        Returns:
            True if from augmented trajectory, False if real
        """
        if self.access_policy == "generated_only":
            return True
        elif self.access_policy == "real_only":
            return False
        else:  # 'both'
            return idx >= self.num_real

    def get_sample_source(self, idx: int) -> str:
        """
        Get the source of a sample.
        
        Args:
            idx: Sample index
        
        Returns:
            'real' or 'physical_generated'
        """
        return 'physical_generated' if self._is_augmented_trajectory(idx) else 'real'

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"TensorDataset(\n"
            f"  simulations={len(self.sim_indices)},\n"
            f"  samples={len(self)} (real={self.num_real}, aug={self.num_augmented}),\n"
            f"  augmented_trajectories={self._num_augmented_trajectories},\n"
            f"  fields={len(self.field_names)},\n"
            f"  frames={self.num_frames},\n"
            f"  predict_steps={self.num_predict_steps},\n"
            f"  pin_memory={self.pin_memory}\n"
            f")"
        )

--- src/data/validation.py ---

"""
Cache Validation for Data Management

This module provides comprehensive validation for cached simulation data,
ensuring that cached data matches current configuration parameters.

Features:
- PDE parameter validation with checksums
- Resolution and domain matching
- Field name and frame count verification
- Version compatibility checking
- Detailed reporting of validation failures
"""

import hashlib
import json
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime

try:
    from omegaconf import DictConfig, OmegaConf

    HAS_OMEGACONF = True
except ImportError:
    HAS_OMEGACONF = False


class CacheValidator:
    """
    Validates cached simulation data against current configuration.

    This class performs comprehensive validation to ensure cached data
    is still valid for the current configuration, checking:
    - PDE parameters (nu, buoyancy, etc.)
    - Domain size and resolution
    - Field names and types
    - Number of frames
    - Generation parameters (dt, save_interval)

    Attributes:
        config: Configuration dictionary with current parameters
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the cache validator.

        Args:
            config: Configuration dictionary containing model, data, and generation params
        """
        self.config = config

    def validate_cache(
        self,
        cached_metadata: Dict[str, Any],
        field_names: List[str],
        num_frames: Optional[int] = None,
    ) -> Tuple[bool, List[str]]:
        """
        Validate cached data against current configuration.

        Args:
            cached_metadata: Metadata dictionary from cached file
            field_names: List of field names that should be present
            num_frames: Minimum number of frames required (None = don't check)

        Returns:
            Tuple of (is_valid, reasons_if_invalid)
            - is_valid: True if cache is valid, False otherwise
            - reasons_if_invalid: List of strings explaining why cache is invalid
        """
        reasons = []

        # Check 1: Version compatibility
        cache_version = cached_metadata.get("version", "1.0")
        if not self._is_version_compatible(cache_version):
            reasons.append(
                f"Cache version '{cache_version}' is incompatible with current version '2.0'"
            )

        # Check 2: Field names
        if not self._validate_field_names(cached_metadata, field_names):
            cached_fields = set(cached_metadata.get("field_metadata", {}).keys())
            requested_fields = set(field_names)
            reasons.append(
                f"Field mismatch - cached: {cached_fields}, requested: {requested_fields}"
            )

        # Check 3: Number of frames
        if num_frames is not None:
            cached_frames = cached_metadata.get("num_frames", 0)
            if cached_frames < num_frames:
                reasons.append(
                    f"Insufficient frames - cached: {cached_frames}, requested: {num_frames}"
                )

        # Check 4: PDE parameters (only if metadata has new format)
        if cache_version >= "2.0" and "generation_params" in cached_metadata:
            if not self._validate_pde_params(cached_metadata):
                reasons.append("PDE parameters have changed")

            # Check 5: Resolution
            if not self._validate_resolution(cached_metadata):
                cached_res = cached_metadata["generation_params"].get(
                    "resolution", "unknown"
                )
                current_res = (
                    self.config.get("model", {})
                    .get("physical", {})
                    .get("resolution", "unknown")
                )
                reasons.append(
                    f"Resolution mismatch - cached: {cached_res}, current: {current_res}"
                )

            # Check 6: Domain
            if not self._validate_domain(cached_metadata):
                cached_domain = cached_metadata["generation_params"].get(
                    "domain", "unknown"
                )
                current_domain = (
                    self.config.get("model", {})
                    .get("physical", {})
                    .get("domain", "unknown")
                )
                reasons.append(
                    f"Domain mismatch - cached: {cached_domain}, current: {current_domain}"
                )

            # Check 7: dt (timestep)
            if not self._validate_dt(cached_metadata):
                cached_dt = cached_metadata["generation_params"].get("dt", "unknown")
                current_dt = (
                    self.config.get("model", {})
                    .get("physical", {})
                    .get("dt", "unknown")
                )
                reasons.append(
                    f"Timestep (dt) mismatch - cached: {cached_dt}, current: {current_dt}"
                )

        # Cache is valid if no reasons for invalidity were found
        is_valid = len(reasons) == 0
        return is_valid, reasons

    def _is_version_compatible(self, cache_version: str) -> bool:
        """
        Check if cache version is compatible with current version.

        Args:
            cache_version: Version string from cached metadata

        Returns:
            True if compatible, False otherwise
        """
        try:
            # Extract major version
            cache_major = int(cache_version.split(".")[0])
            current_major = 2  # Current cache format version

            # Only accept version 2.x caches
            return cache_major == current_major
        except (ValueError, IndexError):
            # Invalid version format
            return False

    def _validate_field_names(
        self, cached_metadata: Dict[str, Any], field_names: List[str]
    ) -> bool:
        """
        Validate that cached data contains all requested fields.

        Args:
            cached_metadata: Cached metadata dictionary
            field_names: List of field names that should be present

        Returns:
            True if all fields present, False otherwise
        """
        cached_fields = set(cached_metadata.get("field_metadata", {}).keys())
        requested_fields = set(field_names)
        return cached_fields == requested_fields

    def _validate_pde_params(self, cached_metadata: Dict[str, Any]) -> bool:
        """
        Validate that PDE parameters match between cached and current config.

        Args:
            cached_metadata: Cached metadata dictionary

        Returns:
            True if PDE parameters match, False otherwise
        """
        # Get checksums
        cached_hash = cached_metadata.get("checksums", {}).get("pde_params_hash")
        if cached_hash is None:
            # Old cache format without checksums - consider invalid
            return False

        # Compute current hash
        current_params = (
            self.config.get("model", {}).get("physical", {}).get("pde_params", {})
        )
        current_hash = compute_hash(current_params)

        return cached_hash == current_hash

    def _validate_resolution(self, cached_metadata: Dict[str, Any]) -> bool:
        """
        Validate that resolution matches between cached and current config.

        Args:
            cached_metadata: Cached metadata dictionary

        Returns:
            True if resolution matches, False otherwise
        """
        cached_res = cached_metadata.get("generation_params", {}).get("resolution")
        current_res = self.config.get("model", {}).get("physical", {}).get("resolution")

        if cached_res is None or current_res is None:
            return False

        return cached_res == current_res

    def _validate_domain(self, cached_metadata: Dict[str, Any]) -> bool:
        """
        Validate that domain matches between cached and current config.

        Args:
            cached_metadata: Cached metadata dictionary

        Returns:
            True if domain matches, False otherwise
        """
        cached_domain = cached_metadata.get("generation_params", {}).get("domain")
        current_domain = self.config.get("model", {}).get("physical", {}).get("domain")

        if cached_domain is None or current_domain is None:
            return False

        return cached_domain == current_domain

    def _validate_dt(self, cached_metadata: Dict[str, Any]) -> bool:
        """
        Validate that timestep (dt) matches between cached and current config.

        Args:
            cached_metadata: Cached metadata dictionary

        Returns:
            True if dt matches, False otherwise
        """
        cached_dt = cached_metadata.get("generation_params", {}).get("dt")
        current_dt = self.config.get("model", {}).get("physical", {}).get("dt")

        if cached_dt is None or current_dt is None:
            return False

        # Use approximate equality for floating point comparison
        return abs(float(cached_dt) - float(current_dt)) < 1e-9


def compute_hash(obj: Any) -> str:
    """
    Compute a stable SHA256 hash of an object.

    This function converts the object to a JSON string with sorted keys
    to ensure consistent hashing across different runs.

    Args:
        obj: Object to hash (must be JSON-serializable)

    Returns:
        Hexadecimal hash string

    Example:
        >>> hash1 = compute_hash({'a': 1, 'b': 2})
        >>> hash2 = compute_hash({'b': 2, 'a': 1})
        >>> hash1 == hash2
        True
    """
    # Convert OmegaConf DictConfig to regular dict if needed
    if HAS_OMEGACONF and isinstance(obj, DictConfig):
        obj = OmegaConf.to_container(obj, resolve=True)

    json_str = json.dumps(obj, sort_keys=True)
    return hashlib.sha256(json_str.encode()).hexdigest()


def get_cache_version() -> str:
    """
    Get the current cache format version.

    Returns:
        Version string (e.g., '2.0')
    """
    return "2.0"


def get_phiflow_version() -> str:
    """
    Get the PhiFlow version being used.

    Returns:
        Version string, or 'unknown' if PhiFlow not available
    """
    try:
        import phi

        return phi.__version__
    except (ImportError, AttributeError):
        return "unknown"


--- src/evaluation/__init__.py ---

"""
Evaluation module for model performance assessment.

This module provides tools for evaluating trained models, including:
- Inference on test data
- Metric computation (MSE, RMSE, MAE, etc.)
- Visualization generation (animations, error plots, keyframe comparisons)
- Main Evaluator class for orchestrating complete evaluation workflows
"""

from .visualizations import (
    create_comparison_gif,
    create_comparison_gif_from_specs,
    plot_side_by_side_frame,
    plot_error_vs_time,
    plot_error_vs_time_multi_field,
    plot_error_comparison,
    plot_error_heatmap,
    plot_keyframe_comparison,
    plot_keyframe_comparison_multi_field,
    create_evaluation_summary,
)

from .metrics import (
    compute_mse_per_timestep,
    compute_rmse_per_timestep,
    compute_mae_per_timestep,
    compute_relative_error_per_timestep,
    compute_normalized_error_per_timestep,
    compute_all_metrics,
    compute_metrics_per_field,
    aggregate_metrics,
)

from .evaluator import Evaluator

__all__ = [
    # Main Evaluator
    "Evaluator",
    # Visualizations
    "create_comparison_gif",
    "create_comparison_gif_from_specs",
    "plot_side_by_side_frame",
    "plot_error_vs_time",
    "plot_error_vs_time_multi_field",
    "plot_error_comparison",
    "plot_error_heatmap",
    "plot_keyframe_comparison",
    "plot_keyframe_comparison_multi_field",
    "create_evaluation_summary",
    # Metrics
    "compute_mse_per_timestep",
    "compute_rmse_per_timestep",
    "compute_mae_per_timestep",
    "compute_relative_error_per_timestep",
    "compute_normalized_error_per_timestep",
    "compute_all_metrics",
    "compute_metrics_per_field",
    "aggregate_metrics",
]


--- src/evaluation/evaluator.py ---

"""
Main Evaluator class for comprehensive model evaluation.

This module provides the Evaluator class that orchestrates the complete
evaluation workflow: loading models, running inference, computing metrics,
and generating all visualizations.
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from src.data import DataManager
from src.models import ModelRegistry
from .metrics import compute_all_metrics, compute_metrics_per_field, aggregate_metrics
from .visualizations import (
    create_comparison_gif,
    create_comparison_gif_from_specs,
    plot_error_vs_time,
    plot_error_vs_time_multi_field,
    plot_keyframe_comparison,
    plot_keyframe_comparison_multi_field,
    plot_keyframes_as_svgs_multi_field,
    plot_error_heatmap,
    create_evaluation_summary,
    plot_keyframes_as_svgs,
)
from src.utils.logger import get_logger

logger = get_logger(__name__)


class Evaluator:
    """
    Main evaluator class for comprehensive model evaluation.

    This class orchestrates the complete evaluation workflow:
    1. Load trained synthetic model
    2. Run inference on test simulations
    3. Compute error metrics
    4. Generate all visualizations (animations, plots, keyframes)
    5. Save organized results with JSON summaries

    Supports both single and multi-field evaluation with automatic
    organization of results by simulation.

    Attributes:
        config: Configuration dictionary
        device: PyTorch device (CPU or CUDA)
        model: Loaded synthetic model
        data_manager: DataManager for loading test data
        field_specs: Dictionary mapping field names to channel counts
        output_specs: Model output specifications
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the Evaluator from configuration.

        Args:
            config: Configuration dictionary containing:
                - data: Data configuration (fields, dataset name, etc.)
                - model/synthetic: Model configuration (architecture, paths, specs)
                - evaluation_params: Evaluation parameters (test_sim, metrics, etc.)
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        logger.debug(f"\n{'='*60}")
        logger.debug("INITIALIZING EVALUATOR")
        logger.debug(f"{'='*60}")
        logger.debug(f"Device: {self.device}")

        # Extract configuration
        self.data_config = config["data"]
        self.model_config = config["model"]["synthetic"]
        self.eval_config = config.get("evaluation_params", {})

        self.field_names = self.data_config["fields"]
        self.input_specs = {
        field: config['model']['physical']['fields_scheme'].lower().count(field[0].lower())
        for field in config['model']['physical']['fields']
        if field
        }
        self.output_specs = {
        field: config['model']['physical']['fields_scheme'].lower().count(field[0].lower())
        for i, field in enumerate(config['model']['physical']['fields'])
        if field and config['model']['physical']['fields_type'][i].upper() == 'D'
        }


        # Evaluation parameters
        self.test_sim = self.eval_config.get("test_sim", [0])
        self.num_frames = self.eval_config.get("num_frames", 51)
        self.metrics_to_compute = self.eval_config.get(
            "metrics", ["mse", "mae", "rmse"]
        )
        self.num_keyframes = self.eval_config.get("keyframe_count", 5)
        self.animation_fps = self.eval_config.get("animation_fps", 10)
        self.save_animations = self.eval_config.get("save_animations", True)
        self.save_plots = self.eval_config.get("save_plots", True)

        # Initialize components
        self.model = None
        self.data_manager = None

        logger.debug(f"Test simulations: {self.test_sim}")
        logger.debug(f"Evaluation frames: {self.num_frames}")
        logger.debug(f"Metrics to compute: {self.metrics_to_compute}")
        logger.debug(f"{'='*60}\n")

    def load_model(self) -> nn.Module:
        """
        Load the trained synthetic model.

        Returns:
            Loaded model in evaluation mode

        Raises:
            FileNotFoundError: If model checkpoint doesn't exist
        """
        logger.debug("Loading trained model...")

        # Get checkpoint path
        model_path_dir = self.model_config["model_path"]
        model_save_name = self.model_config["model_save_name"]
        checkpoint_path = os.path.join(model_path_dir, f"{model_save_name}.pth")

        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(
                f"Model checkpoint not found at {checkpoint_path}. "
                f"Please train the model first."
            )

        # Create model architecture using registry
        model_name = self.model_config["name"]
        logger.debug(f"Creating model: {model_name}")

        model = ModelRegistry.get_synthetic_model(model_name, config=self.config['model'])
        model = torch.compile(model)
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # Handle different checkpoint formats
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            # Checkpoint is the state dict directly
            model.load_state_dict(checkpoint)

        model = model.to(self.device)
        model.eval()

        self.model = model
        return model

    def setup_data_manager(self) -> DataManager:
        """
        Set up DataManager for loading test data.

        Returns:
            Configured DataManager instance
        """
        logger.debug("Setting up data manager...")

        data_dir = self.data_config["data_dir"]
        dset_name = self.data_config["dset_name"]

        raw_data_dir = os.path.join(data_dir, dset_name)
        cache_dir = os.path.join(data_dir, "cache", f"eval_{dset_name}")

        self.data_manager = DataManager(
            raw_data_dir=raw_data_dir,
            cache_dir=cache_dir,
            config=self.config,  # Pass full config for validation
            auto_clear_invalid=self.data_config.get("auto_clear_invalid", False),
        )

        logger.debug(f"Data manager ready: {dset_name}, Fields: {self.field_names}")

        return self.data_manager

    def run_inference(
        self, sim_index: int, num_rollout_steps: Optional[int] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Run model inference on a simulation.

        Performs autoregressive rollout: use model output as next input.

        Args:
            sim_index: Index of simulation to evaluate
            num_rollout_steps: Number of autoregressive steps (default: num_frames - 1)

        Returns:
            Dictionary with:
                - 'prediction': Model predictions [T, C, H, W]
                - 'ground_truth': True data [T, C, H, W]
                - 'initial_state': Initial condition [C, H, W]
        """
        logger.debug(f"Running inference on simulation {sim_index}...")

        if num_rollout_steps is None:
            num_rollout_steps = self.num_frames - 1

        # Load data
        data = self.data_manager.get_or_load_simulation(
            sim_index=sim_index,
            field_names=self.field_names,
            num_frames=self.num_frames,
        )

        ground_truth = data["tensor_data"]

        # Concatenate all fields into single tensor along channel dimension
        # Handle fields with different channel counts (e.g., density=1, velocity=2)
        field_tensors = []
        for field_name in self.field_names:
            field_tensor = ground_truth[field_name]  # [T, C, H, W]
            field_tensors.append(field_tensor)

        gt_tensor = torch.cat(
            field_tensors, dim=1
        )  # Concatenate along channel dim [T, C_total, H, W]

        # Slice ground truth to match number of frames we're predicting
        # We predict num_frames total (including initial state)
        gt_tensor = gt_tensor[: self.num_frames]

        # Initial state (t=0)
        initial_state = gt_tensor[0:1].to(self.device)  # [1, C, H, W]

        # Autoregressive rollout
        predictions = [initial_state]
        current_state = initial_state

        with torch.no_grad():
            for step in range(num_rollout_steps):
                # Model predicts next state
                next_state = self.model(current_state)
                predictions.append(next_state)

                # Use prediction as next input
                current_state = next_state

        # Stack predictions
        prediction_tensor = torch.cat(predictions, dim=0)  # [T, C, H, W]

        logger.debug(
            f"Rollout complete: {prediction_tensor.shape[0]} frames, Shape: {prediction_tensor.shape}"
        )

        return {
            "prediction": prediction_tensor.cpu(),
            "ground_truth": gt_tensor.cpu(),
            "initial_state": initial_state[0].cpu(),
        }

    def compute_metrics(
        self, prediction: torch.Tensor, ground_truth: torch.Tensor
    ) -> Dict[str, Any]:
        """
        Compute all error metrics.

        Args:
            prediction: Model predictions [T, C, H, W]
            ground_truth: True data [T, C, H, W]

        Returns:
            Dictionary with:
                - Per-field metrics
                - Aggregate statistics
                - Timestep-wise errors
        """
        logger.debug(f"Computing metrics...")

        # Compute per-field metrics
        # Use input_specs to get ALL fields (including static ones)
        # because both prediction and ground_truth contain all fields
        field_specs = {}
        for field_name in self.field_names:
            if field_name in self.input_specs:
                field_specs[field_name] = self.input_specs[field_name]

        field_metrics = compute_metrics_per_field(
            prediction, ground_truth, field_specs, self.metrics_to_compute
        )

        # Compute aggregate statistics for each field/metric
        aggregates = {}
        for field_name, metrics in field_metrics.items():
            aggregates[field_name] = {}
            for metric_name, values in metrics.items():
                aggregates[field_name][metric_name] = aggregate_metrics(values)

        logger.debug(f"Metrics computed for {len(field_specs)} fields")

        return {
            "field_metrics": field_metrics,
            "aggregates": aggregates,
            "field_specs": field_specs,
        }

    def generate_visualizations(
        self,
        prediction: torch.Tensor,
        ground_truth: torch.Tensor,
        sim_index: int,
        save_dir: Union[str, Path],
    ) -> Dict[str, Dict[str, Path]]:
        """
        Generate all visualizations for a simulation.

        Args:
            prediction: Model predictions [T, C, H, W]
            ground_truth: True data [T, C, H, W]
            sim_index: Simulation index
            save_dir: Base directory for saving outputs

        Returns:
            Dictionary mapping visualization types to file paths
        """
        save_dir = Path(save_dir)
        logger.debug(f"Generating visualizations...")

        saved_paths = {
            "animations": {},
            "error_plots": {},
            "keyframes": {},
            "heatmaps": {},
        }

        # Field specifications
        # Use input_specs to get ALL fields (including static ones)
        # because both prediction and ground_truth contain all fields
        field_specs = {}
        for field_name in self.field_names:
            if field_name in self.input_specs:
                field_specs[field_name] = self.input_specs[field_name]

        # 1. Animations
        if self.save_animations:
            logger.debug(f"Creating animations...")
            anim_dir = save_dir / "animations"
            paths = create_comparison_gif_from_specs(
                prediction,
                ground_truth,
                field_specs,
                anim_dir,
                fps=self.animation_fps,
                show_difference=True,
            )
            saved_paths["animations"] = paths

        # 2. Error plots
        if self.save_plots:
            logger.debug(f"Creating error plots...")
            plot_dir = save_dir / "plots"
            paths = plot_error_vs_time_multi_field(
                prediction,
                ground_truth,
                field_specs,
                plot_dir,
                metrics=self.metrics_to_compute,
            )
            saved_paths["error_plots"] = paths

        # 3. Keyframe comparisons
        if self.save_plots:
            logger.debug(f"Creating keyframe comparisons...")
            keyframe_dir = save_dir / "plots"
            paths = plot_keyframe_comparison_multi_field(
                prediction,
                ground_truth,
                field_specs,
                keyframe_dir,
                num_keyframes=self.num_keyframes,
                show_difference=True,
                show_metrics=True,
            )
            saved_paths["keyframes"] = paths
            svg_paths = plot_keyframes_as_svgs_multi_field(
                prediction,
                ground_truth,
                field_specs,
                keyframe_dir,
                num_keyframes=self.num_keyframes,
                show_difference=True,
            )
            saved_paths["keyframes_svg"] = svg_paths

        # 4. Error heatmaps (for multi-channel fields)
        if self.save_plots:
            heatmap_dir = save_dir / "plots"
            channel_idx = 0
            for field_name, num_channels in field_specs.items():
                if num_channels > 1:
                    pred_field = prediction[
                        :, channel_idx : channel_idx + num_channels, :, :
                    ]
                    gt_field = ground_truth[
                        :, channel_idx : channel_idx + num_channels, :, :
                    ]

                    heatmap_path = heatmap_dir / f"{field_name}_error_heatmap.png"
                    plot_error_heatmap(pred_field, gt_field, field_name, heatmap_path)
                    saved_paths["heatmaps"][field_name] = heatmap_path

                channel_idx += num_channels

        logger.debug(f"All visualizations created")

        return saved_paths

    def save_metrics_to_json(
        self, metrics: Dict[str, Any], save_path: Union[str, Path]
    ) -> None:
        """
        Save metrics to JSON file.

        Args:
            metrics: Metrics dictionary
            save_path: Path to JSON file
        """
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # Convert torch tensors to lists for JSON serialization
        json_metrics = {}

        if "aggregates" in metrics:
            json_metrics["aggregates"] = metrics["aggregates"]

        if "field_metrics" in metrics:
            json_metrics["per_timestep"] = {}
            for field_name, field_metrics in metrics["field_metrics"].items():
                json_metrics["per_timestep"][field_name] = {}
                for metric_name, values in field_metrics.items():
                    json_metrics["per_timestep"][field_name][metric_name] = (
                        values.detach().cpu().numpy().tolist()
                    )

        with open(save_path, "w") as f:
            json.dump(json_metrics, f, indent=2)

        logger.debug(f"Metrics saved to {save_path}")

    def evaluate_simulation(
        self, sim_index: int, save_dir: Optional[Union[str, Path]] = None
    ) -> Dict[str, Any]:
        """
        Run complete evaluation on a single simulation.

        Args:
            sim_index: Index of simulation to evaluate
            save_dir: Directory to save results (auto-generated if None)

        Returns:
            Dictionary with results:
                - metrics: Computed error metrics
                - visualizations: Paths to generated visualizations
                - inference_results: Prediction and ground truth tensors
        """
        logger.debug(f"\n{'='*60}")
        logger.debug(f"EVALUATING SIMULATION {sim_index}")
        logger.debug(f"{'='*60}")

        # Set up save directory
        if save_dir is None:
            dset_name = self.data_config["dset_name"]
            model_name = self.model_config["model_save_name"]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = (
                Path("results")
                / "evaluation"
                / f"{dset_name}_{model_name}"
                / f"sim_{sim_index:06d}"
            )
        else:
            save_dir = Path(save_dir)

        save_dir.mkdir(parents=True, exist_ok=True)

        # 1. Run inference
        inference_results = self.run_inference(sim_index)
        prediction = inference_results["prediction"]
        ground_truth = inference_results["ground_truth"]

        # 2. Compute metrics
        metrics = self.compute_metrics(prediction, ground_truth)

        # 3. Generate visualizations
        visualizations = self.generate_visualizations(
            prediction, ground_truth, sim_index, save_dir
        )

        # 4. Save metrics to JSON
        metrics_dir = save_dir / "metrics"
        metrics_dir.mkdir(parents=True, exist_ok=True)
        self.save_metrics_to_json(metrics, metrics_dir / "metrics_summary.json")

        # 5. Summary - compact one-liner with key metrics
        mse_summary = ", ".join(
            [
                f"{name}: {m['mse']['mean']:.4f}"
                for name, m in metrics["aggregates"].items()
            ]
        )
        logger.info(f"Simulation {sim_index} complete - MSE: {mse_summary}")

        # Detailed summary at DEBUG level
        logger.debug(f"\n{'='*60}")
        logger.debug(f"EVALUATION COMPLETE")
        logger.debug(f"{'='*60}")
        logger.debug(f"Results saved to: {save_dir}")
        logger.debug(f"\nMetric Summary:")
        for field_name, field_agg in metrics["aggregates"].items():
            logger.debug(f"\n  {field_name.upper()}:")
            for metric_name, stats in field_agg.items():
                logger.debug(
                    f"    {metric_name.upper()}: mean={stats['mean']:.6f}, std={stats['std']:.6f}"
                )
        logger.debug(f"{'='*60}\n")

        return {
            "metrics": metrics,
            "visualizations": visualizations,
            "inference_results": inference_results,
            "save_dir": save_dir,
        }

    def evaluate(
        self,
        sim_indices: Optional[List[int]] = None,
        base_save_dir: Optional[Union[str, Path]] = None,
    ) -> Dict[int, Dict[str, Any]]:
        """
        Run evaluation on multiple simulations.

        This is the main entry point for evaluation.

        Args:
            sim_indices: List of simulation indices (uses test_sim from config if None)
            base_save_dir: Base directory for all results

        Returns:
            Dictionary mapping sim_index to evaluation results
        """
        if not sim_indices:
            sim_indices = self.test_sim

        logger.info(f"Starting evaluation on {len(sim_indices)} simulation(s)")
        logger.debug(f"\n{'='*60}")
        logger.debug(f"STARTING EVALUATION")
        logger.debug(f"{'='*60}")
        logger.debug(f"Simulations to evaluate: {sim_indices}")
        logger.debug(f"{'='*60}\n")

        # Load model
        if self.model is None:
            self.load_model()

        # Setup data manager
        if self.data_manager is None:
            self.setup_data_manager()

        # Evaluate each simulation
        all_results = {}

        for sim_idx in sim_indices:
            if base_save_dir is not None:
                sim_save_dir = Path(base_save_dir) / f"sim_{sim_idx:06d}"
            else:
                sim_save_dir = None

            results = self.evaluate_simulation(sim_idx, sim_save_dir)
            all_results[sim_idx] = results

        # Create summary across all simulations
        if len(sim_indices) > 1 and base_save_dir is not None:
            self._create_aggregate_summary(all_results, base_save_dir)

        logger.info(f"\n{'='*60}")
        logger.info(f"ALL EVALUATIONS COMPLETE")
        logger.info(f"{'='*60}")
        logger.info(f"Evaluated {len(sim_indices)} simulations")
        if base_save_dir is not None:
            logger.info(f"Results saved to: {base_save_dir}")
        logger.info(f"{'='*60}\n")

        return all_results

    def _create_aggregate_summary(
        self, all_results: Dict[int, Dict[str, Any]], base_save_dir: Union[str, Path]
    ) -> None:
        """
        Create aggregate summary across all evaluated simulations.

        Args:
            all_results: Results from all simulations
            base_save_dir: Base directory for saving summary
        """
        logger.info(f"\nCreating aggregate summary...")

        base_save_dir = Path(base_save_dir)
        summary_dir = base_save_dir / "summary"
        summary_dir.mkdir(parents=True, exist_ok=True)

        # Aggregate metrics across simulations
        aggregate_metrics = {}

        for sim_idx, results in all_results.items():
            metrics = results["metrics"]["aggregates"]

            for field_name, field_metrics in metrics.items():
                if field_name not in aggregate_metrics:
                    aggregate_metrics[field_name] = {
                        metric: [] for metric in field_metrics.keys()
                    }

                for metric_name, stats in field_metrics.items():
                    aggregate_metrics[field_name][metric_name].append(stats["mean"])

        # Compute statistics across simulations
        summary = {}
        for field_name, field_data in aggregate_metrics.items():
            summary[field_name] = {}
            for metric_name, values in field_data.items():
                import numpy as np

                summary[field_name][metric_name] = {
                    "mean": float(np.mean(values)),
                    "std": float(np.std(values)),
                    "min": float(np.min(values)),
                    "max": float(np.max(values)),
                }

        # Save aggregate summary
        with open(summary_dir / "aggregate_metrics.json", "w") as f:
            json.dump(summary, f, indent=2)

        logger.info(f"  [OK] Aggregate summary saved to {summary_dir}")
        logger.info(f"\n  Aggregate Statistics Across {len(all_results)} Simulations:")
        for field_name, field_stats in summary.items():
            logger.info(f"\n    {field_name.upper()}:")
            for metric_name, stats in field_stats.items():
                logger.info(
                    f"      {metric_name.upper()}: mean={stats['mean']:.6f} +/- {stats['std']:.6f}"
                )


--- src/evaluation/metrics.py ---

"""
Metric calculation functions for model evaluation.

This module provides functions to compute various error metrics comparing
model predictions against ground truth data, including per-timestep errors
and aggregate statistics.
"""

from typing import Dict, List, Optional, Tuple, Union
import torch
import numpy as np


def compute_mse_per_timestep(
    prediction: torch.Tensor, ground_truth: torch.Tensor, reduction: str = "mean"
) -> torch.Tensor:
    """
    Compute Mean Squared Error at each time step.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        reduction: How to reduce spatial dimensions:
                  'mean' - average over spatial dims
                  'sum' - sum over spatial dims
                  'none' - keep spatial dimensions

    Returns:
        MSE values, shape depends on reduction:
        - 'mean' or 'sum': [T, C] (per timestep, per channel)
        - 'none': [T, C, H, W] (full spatial error map)

    Example:
        >>> mse = compute_mse_per_timestep(pred, gt, reduction='mean')
        >>> # mse.shape = [50, 2] for 50 timesteps, 2 channels
    """
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    # Compute squared error
    squared_error = (prediction - ground_truth) ** 2

    if reduction == "mean":
        # Average over spatial dimensions [H, W]
        return squared_error.mean(dim=(-2, -1))  # [T, C]
    elif reduction == "sum":
        # Sum over spatial dimensions
        return squared_error.sum(dim=(-2, -1))  # [T, C]
    elif reduction == "none":
        # Keep all dimensions
        return squared_error  # [T, C, H, W]
    else:
        raise ValueError(
            f"Unknown reduction: {reduction}. Use 'mean', 'sum', or 'none'"
        )


def compute_rmse_per_timestep(
    prediction: torch.Tensor, ground_truth: torch.Tensor, reduction: str = "mean"
) -> torch.Tensor:
    """
    Compute Root Mean Squared Error at each time step.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        reduction: How to reduce spatial dimensions ('mean', 'sum', 'none')

    Returns:
        RMSE values, shape [T, C] for 'mean'/'sum', [T, C, H, W] for 'none'
    """
    mse = compute_mse_per_timestep(prediction, ground_truth, reduction)
    return torch.sqrt(mse)


def compute_mae_per_timestep(
    prediction: torch.Tensor, ground_truth: torch.Tensor, reduction: str = "mean"
) -> torch.Tensor:
    """
    Compute Mean Absolute Error at each time step.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        reduction: How to reduce spatial dimensions ('mean', 'sum', 'none')

    Returns:
        MAE values, shape [T, C] for 'mean'/'sum', [T, C, H, W] for 'none'
    """
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    absolute_error = torch.abs(prediction - ground_truth)

    if reduction == "mean":
        return absolute_error.mean(dim=(-2, -1))  # [T, C]
    elif reduction == "sum":
        return absolute_error.sum(dim=(-2, -1))  # [T, C]
    elif reduction == "none":
        return absolute_error  # [T, C, H, W]
    else:
        raise ValueError(f"Unknown reduction: {reduction}")


def compute_relative_error_per_timestep(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    epsilon: float = 1e-8,
    reduction: str = "mean",
) -> torch.Tensor:
    """
    Compute relative error at each time step.

    Relative error = |pred - gt| / (|gt| + epsilon)

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        epsilon: Small constant to avoid division by zero
        reduction: How to reduce spatial dimensions ('mean', 'sum', 'none')

    Returns:
        Relative error values, shape [T, C] for 'mean'/'sum', [T, C, H, W] for 'none'
    """
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    absolute_error = torch.abs(prediction - ground_truth)
    gt_magnitude = torch.abs(ground_truth) + epsilon
    relative_error = absolute_error / gt_magnitude

    if reduction == "mean":
        return relative_error.mean(dim=(-2, -1))  # [T, C]
    elif reduction == "sum":
        return relative_error.sum(dim=(-2, -1))  # [T, C]
    elif reduction == "none":
        return relative_error  # [T, C, H, W]
    else:
        raise ValueError(f"Unknown reduction: {reduction}")


def compute_normalized_error_per_timestep(
    prediction: torch.Tensor, ground_truth: torch.Tensor, reduction: str = "mean"
) -> torch.Tensor:
    """
    Compute normalized error at each time step.

    Normalized error = |pred - gt| / (max(|gt|) - min(|gt|))

    This normalizes by the range of the ground truth, making errors
    comparable across different scales.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        reduction: How to reduce spatial dimensions ('mean', 'sum', 'none')

    Returns:
        Normalized error values, shape [T, C] for 'mean'/'sum', [T, C, H, W] for 'none'
    """
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    absolute_error = torch.abs(prediction - ground_truth)

    # Compute range per channel across all timesteps and spatial locations
    gt_flat = ground_truth.reshape(ground_truth.shape[0], ground_truth.shape[1], -1)
    gt_range = gt_flat.max(dim=-1).values - gt_flat.min(dim=-1).values  # [T, C]
    gt_range = gt_range.unsqueeze(-1).unsqueeze(-1)  # [T, C, 1, 1] for broadcasting

    # Avoid division by zero
    gt_range = torch.clamp(gt_range, min=1e-8)

    normalized_error = absolute_error / gt_range

    if reduction == "mean":
        return normalized_error.mean(dim=(-2, -1))  # [T, C]
    elif reduction == "sum":
        return normalized_error.sum(dim=(-2, -1))  # [T, C]
    elif reduction == "none":
        return normalized_error  # [T, C, H, W]
    else:
        raise ValueError(f"Unknown reduction: {reduction}")


def aggregate_metrics(errors: torch.Tensor, dim: int = 0) -> Dict[str, float]:
    """
    Compute aggregate statistics over error values.

    Args:
        errors: Error tensor, typically shape [T, C]
        dim: Dimension to aggregate over (default: 0 for time)

    Returns:
        Dictionary with aggregate statistics:
        - 'mean': Mean error
        - 'std': Standard deviation
        - 'min': Minimum error
        - 'max': Maximum error
        - 'median': Median error
        - 'q25': 25th percentile
        - 'q75': 75th percentile
    """
    errors_np = errors.detach().cpu().numpy()

    return {
        "mean": float(np.mean(errors_np, axis=dim).mean()),
        "std": float(np.std(errors_np, axis=dim).mean()),
        "min": float(np.min(errors_np)),
        "max": float(np.max(errors_np)),
        "median": float(np.median(errors_np, axis=dim).mean()),
        "q25": float(np.percentile(errors_np, 25, axis=dim).mean()),
        "q75": float(np.percentile(errors_np, 75, axis=dim).mean()),
    }


def compute_all_metrics(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    metrics: Optional[List[str]] = None,
) -> Dict[str, torch.Tensor]:
    """
    Compute multiple metrics at once.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        metrics: List of metric names to compute. If None, computes all.
                Available: 'mse', 'rmse', 'mae', 'relative', 'normalized'

    Returns:
        Dictionary mapping metric names to error tensors [T, C]

    Example:
        >>> metrics = compute_all_metrics(pred, gt, metrics=['mse', 'mae'])
        >>> mse_over_time = metrics['mse'][:, 0]  # First channel
    """
    if metrics is None:
        metrics = ["mse", "rmse", "mae", "relative", "normalized"]

    results = {}

    if "mse" in metrics:
        results["mse"] = compute_mse_per_timestep(prediction, ground_truth)

    if "rmse" in metrics:
        results["rmse"] = compute_rmse_per_timestep(prediction, ground_truth)

    if "mae" in metrics:
        results["mae"] = compute_mae_per_timestep(prediction, ground_truth)

    if "relative" in metrics:
        results["relative"] = compute_relative_error_per_timestep(
            prediction, ground_truth
        )

    if "normalized" in metrics:
        results["normalized"] = compute_normalized_error_per_timestep(
            prediction, ground_truth
        )

    return results


def compute_metrics_per_field(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_specs: Dict[str, int],
    metrics: Optional[List[str]] = None,
) -> Dict[str, Dict[str, torch.Tensor]]:
    """
    Compute metrics for each field separately in multi-field data.

    Args:
        prediction: Model prediction tensor, shape [T, C_total, H, W]
        ground_truth: Ground truth tensor, shape [T, C_total, H, W]
        field_specs: Dictionary mapping field names to channel counts
                    e.g., {'velocity': 2, 'density': 1}
        metrics: List of metric names to compute

    Returns:
        Nested dictionary: field_name -> metric_name -> error_tensor

    Example:
        >>> specs = {'velocity': 2, 'density': 1}
        >>> results = compute_metrics_per_field(pred, gt, specs, ['mse', 'mae'])
        >>> velocity_mse = results['velocity']['mse']  # Shape [T, 2]
        >>> density_mae = results['density']['mae']    # Shape [T, 1]
    """
    results = {}
    channel_idx = 0

    for field_name, num_channels in field_specs.items():
        # Extract field data
        pred_field = prediction[:, channel_idx : channel_idx + num_channels, :, :]
        gt_field = ground_truth[:, channel_idx : channel_idx + num_channels, :, :]

        # Compute metrics for this field
        results[field_name] = compute_all_metrics(pred_field, gt_field, metrics)

        channel_idx += num_channels

    return results


--- src/evaluation/visualizations.py ---

"""
Visualization functions for model evaluation.

This module provides functions to create various visualizations comparing
model predictions against ground truth data, including animations, error plots,
and keyframe comparisons.
"""

import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.gridspec import GridSpec

from src.utils.logger import get_logger
from .metrics import compute_all_metrics, compute_metrics_per_field

logger = get_logger(__name__)


def create_comparison_gif(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_path: Union[str, Path],
    fps: int = 10,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    titles: Optional[Tuple[str, str]] = None,
    show_difference: bool = True,
) -> None:
    """
    Create a side-by-side animated GIF comparing prediction and ground truth.

    This function creates an animation with up to 3 panels:
    1. Ground Truth
    2. Prediction
    3. Absolute Difference (optional)

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W] or [T, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W] or [T, H, W]
        field_name: Name of the field being visualized (e.g., 'velocity', 'density')
        save_path: Path where the GIF will be saved
        fps: Frames per second for the animation
        vmin: Minimum value for color scale (auto-computed if None)
        vmax: Maximum value for color scale (auto-computed if None)
        titles: Custom titles for (ground_truth, prediction) panels
        show_difference: Whether to show the difference panel

    Raises:
        ValueError: If tensor shapes don't match or are invalid
    """
    # Validate inputs
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    # Convert to numpy and move to CPU
    pred_np = prediction.detach().cpu().numpy()
    gt_np = ground_truth.detach().cpu().numpy()

    # Handle different tensor shapes
    if pred_np.ndim == 4:  # [T, C, H, W]
        # For multi-channel data (e.g., 2D velocity), compute magnitude
        if pred_np.shape[1] == 2:
            pred_np = np.sqrt(pred_np[:, 0] ** 2 + pred_np[:, 1] ** 2)
            gt_np = np.sqrt(gt_np[:, 0] ** 2 + gt_np[:, 1] ** 2)
        elif pred_np.shape[1] == 1:
            pred_np = pred_np[:, 0]
            gt_np = gt_np[:, 0]
        else:
            raise ValueError(f"Unsupported channel count: {pred_np.shape[1]}")
    elif pred_np.ndim != 3:  # Should be [T, H, W]
        raise ValueError(f"Expected 3 or 4 dimensions, got {pred_np.ndim}")

    # Transpose spatial dimensions: PhiFlow uses [x, y] but matplotlib expects [y, x] (row, col)
    pred_np = np.transpose(
        pred_np, (0, 2, 1)
    )  # [T, H, W] -> [T, W, H] (swaps last two axes)
    gt_np = np.transpose(gt_np, (0, 2, 1))

    num_frames = pred_np.shape[0]

    # Compute global min/max for consistent color scale
    if vmin is None:
        vmin = min(gt_np.min(), pred_np.min())
    if vmax is None:
        vmax = max(gt_np.max(), pred_np.max())

    # Compute difference
    diff_np = np.abs(gt_np - pred_np)
    diff_vmax = diff_np.max()

    # Set up titles
    if titles is None:
        titles = ("Ground Truth", "Prediction")

    # Create figure
    num_cols = 3 if show_difference else 2
    fig, axes = plt.subplots(1, num_cols, figsize=(6 * num_cols, 5))

    if not show_difference:
        axes = list(axes)
    else:
        axes = list(axes)

    # Initialize images
    im1 = axes[0].imshow(gt_np[0], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower")
    axes[0].set_title(titles[0])
    axes[0].axis("off")
    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)

    im2 = axes[1].imshow(
        pred_np[0], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
    )
    axes[1].set_title(titles[1])
    axes[1].axis("off")
    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)

    if show_difference:
        im3 = axes[2].imshow(
            diff_np[0], cmap="hot", vmin=0, vmax=diff_vmax, origin="lower"
        )
        axes[2].set_title("Absolute Difference")
        axes[2].axis("off")
        plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)

    # Add title with frame counter
    title_text = fig.suptitle(
        f"{field_name.capitalize()} - Frame 0/{num_frames-1}",
        fontsize=14,
        fontweight="bold",
    )

    def update(frame):
        """Update function for animation."""
        im1.set_array(gt_np[frame])
        im2.set_array(pred_np[frame])
        if show_difference:
            im3.set_array(diff_np[frame])
        title_text.set_text(f"{field_name.capitalize()} - Frame {frame}/{num_frames-1}")
        return (
            [im1, im2, im3, title_text] if show_difference else [im1, im2, title_text]
        )

    # Create animation
    anim = animation.FuncAnimation(
        fig, update, frames=num_frames, interval=1000 / fps, blit=True
    )

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    logger.debug(f"Saving animation to {save_path}...")
    anim.save(str(save_path), writer="pillow", fps=fps)
    plt.close(fig)
    logger.debug(f"Animation saved successfully!")


def create_comparison_gif_from_specs(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_specs: Dict[str, int],
    save_dir: Union[str, Path],
    fps: int = 10,
    show_difference: bool = True,
) -> Dict[str, Path]:
    """
    Create comparison GIFs for all fields based on specs.

    This function handles multi-field tensors by splitting them according
    to the provided specs and creating separate animations for each field.

    Args:
        prediction: Model prediction tensor, shape [T, C_total, H, W]
        ground_truth: Ground truth tensor, shape [T, C_total, H, W]
        field_specs: Dictionary mapping field names to channel counts
                    e.g., {'velocity': 2, 'density': 1}
        save_dir: Directory where GIFs will be saved
        fps: Frames per second for animations
        show_difference: Whether to show difference panels

    Returns:
        Dictionary mapping field names to saved file paths

    Example:
        >>> specs = {'velocity': 2, 'density': 1}
        >>> paths = create_comparison_gif_from_specs(pred, gt, specs, 'results/')
        >>> # Creates: results/velocity_comparison.gif, results/density_comparison.gif
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = {}
    channel_idx = 0

    for field_name, num_channels in field_specs.items():
        logger.debug(
            f"Creating animation for '{field_name}' ({num_channels} channels)..."
        )

        # Extract field data
        pred_field = prediction[:, channel_idx : channel_idx + num_channels, :, :]
        gt_field = ground_truth[:, channel_idx : channel_idx + num_channels, :, :]

        # Create animation
        save_path = save_dir / f"{field_name}_comparison.gif"
        create_comparison_gif(
            pred_field,
            gt_field,
            field_name,
            save_path,
            fps=fps,
            show_difference=show_difference,
        )

        saved_paths[field_name] = save_path
        channel_idx += num_channels

    return saved_paths


def plot_side_by_side_frame(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    frame_idx: int,
    field_name: str,
    save_path: Optional[Union[str, Path]] = None,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    show_difference: bool = True,
) -> plt.Figure:
    """
    Plot a single frame side-by-side comparison.

    Useful for debugging or creating static frame comparisons.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W] or [T, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W] or [T, H, W]
        frame_idx: Which time step to visualize
        field_name: Name of the field
        save_path: If provided, save the figure to this path
        vmin: Minimum value for color scale
        vmax: Maximum value for color scale
        show_difference: Whether to show difference panel

    Returns:
        Matplotlib figure object
    """
    # Extract frame
    pred_frame = prediction[frame_idx].detach().cpu().numpy()
    gt_frame = ground_truth[frame_idx].detach().cpu().numpy()

    # Handle multi-channel (compute magnitude)
    if pred_frame.ndim == 3 and pred_frame.shape[0] == 2:
        pred_frame = np.sqrt(pred_frame[0] ** 2 + pred_frame[1] ** 2)
        gt_frame = np.sqrt(gt_frame[0] ** 2 + gt_frame[1] ** 2)
    elif pred_frame.ndim == 3 and pred_frame.shape[0] == 1:
        pred_frame = pred_frame[0]
        gt_frame = gt_frame[0]

    # Transpose spatial dimensions: PhiFlow uses [x, y] but matplotlib expects [y, x]
    pred_frame = pred_frame.T
    gt_frame = gt_frame.T

    # Compute color scale
    if vmin is None:
        vmin = min(gt_frame.min(), pred_frame.min())
    if vmax is None:
        vmax = max(gt_frame.max(), pred_frame.max())

    # Create figure
    num_cols = 3 if show_difference else 2
    fig, axes = plt.subplots(1, num_cols, figsize=(6 * num_cols, 5))

    if num_cols == 2:
        axes = list(axes)

    # Plot ground truth
    im1 = axes[0].imshow(gt_frame, cmap="viridis", vmin=vmin, vmax=vmax, origin="lower")
    axes[0].set_title("Ground Truth")
    axes[0].axis("off")
    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)

    # Plot prediction
    im2 = axes[1].imshow(
        pred_frame, cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
    )
    axes[1].set_title("Prediction")
    axes[1].axis("off")
    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)

    # Plot difference
    if show_difference:
        diff = np.abs(gt_frame - pred_frame)
        im3 = axes[2].imshow(diff, cmap="hot", vmin=0, vmax=diff.max(), origin="lower")
        axes[2].set_title("Absolute Difference")
        axes[2].axis("off")
        plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)

    fig.suptitle(
        f"{field_name.capitalize()} - Frame {frame_idx}", fontsize=14, fontweight="bold"
    )

    plt.tight_layout()

    if save_path is not None:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches="tight")
        logger.debug(f"Frame saved to {save_path}")

    return fig


def plot_error_vs_time(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_path: Union[str, Path],
    metrics: Optional[List[str]] = None,
    channel_names: Optional[List[str]] = None,
    title: Optional[str] = None,
) -> plt.Figure:
    """
    Plot error metrics as a function of time.

    Creates a line plot showing how different error metrics evolve over time.
    Useful for understanding error accumulation in autoregressive predictions.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        field_name: Name of the field being visualized
        save_path: Path where the plot will be saved
        metrics: List of metrics to plot. If None, plots ['mse', 'mae']
        channel_names: Optional names for each channel (e.g., ['x', 'y'] for velocity)
        title: Custom title for the plot

    Returns:
        Matplotlib figure object

    Example:
        >>> plot_error_vs_time(pred, gt, 'velocity', 'error.png', metrics=['mse', 'rmse'])
    """
    if metrics is None:
        metrics = ["mse", "mae"]

    # Compute all requested metrics
    error_dict = compute_all_metrics(prediction, ground_truth, metrics)

    num_timesteps = prediction.shape[0]
    num_channels = prediction.shape[1]
    time_steps = np.arange(num_timesteps)

    # Set up channel names
    if channel_names is None:
        if num_channels == 2:
            channel_names = ["x-component", "y-component"]
        elif num_channels == 1:
            channel_names = ["value"]
        else:
            channel_names = [f"channel_{i}" for i in range(num_channels)]

    # Create subplots - one per metric
    num_metrics = len(metrics)
    fig, axes = plt.subplots(num_metrics, 1, figsize=(10, 4 * num_metrics))

    if num_metrics == 1:
        axes = [axes]

    # Color palette for channels
    colors = plt.cm.tab10(np.linspace(0, 1, num_channels))

    for ax, metric_name in zip(axes, metrics):
        error_values = error_dict[metric_name].detach().cpu().numpy()  # [T, C]

        # Plot each channel
        for c in range(num_channels):
            ax.plot(
                time_steps,
                error_values[:, c],
                label=channel_names[c],
                color=colors[c],
                linewidth=2,
                marker="o" if num_timesteps < 20 else None,
                markersize=4,
            )

        ax.set_xlabel("Time Step", fontsize=12)
        ax.set_ylabel(f"{metric_name.upper()}", fontsize=12)
        ax.set_title(f"{metric_name.upper()} vs Time", fontsize=13, fontweight="bold")
        ax.grid(True, alpha=0.3)
        ax.legend(loc="best")

        # Add mean line for multi-channel
        if num_channels > 1:
            mean_error = error_values.mean(axis=1)
            ax.plot(
                time_steps,
                mean_error,
                label="Mean",
                color="black",
                linewidth=2.5,
                linestyle="--",
                alpha=0.7,
            )

    # Overall title
    if title is None:
        title = f"{field_name.capitalize()} - Error Evolution Over Time"
    fig.suptitle(title, fontsize=14, fontweight="bold", y=0.995)

    plt.tight_layout()

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    logger.debug(f"Error plot saved to {save_path}")

    return fig


def plot_error_vs_time_multi_field(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_specs: Dict[str, int],
    save_dir: Union[str, Path],
    metrics: Optional[List[str]] = None,
) -> Dict[str, Path]:
    """
    Plot error vs time for multiple fields.

    Creates separate error plots for each field in multi-field data.

    Args:
        prediction: Model prediction tensor, shape [T, C_total, H, W]
        ground_truth: Ground truth tensor, shape [T, C_total, H, W]
        field_specs: Dictionary mapping field names to channel counts
        save_dir: Directory where plots will be saved
        metrics: List of metrics to plot

    Returns:
        Dictionary mapping field names to saved file paths
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = {}
    channel_idx = 0

    for field_name, num_channels in field_specs.items():
        logger.debug(f"Creating error plot for '{field_name}'...")

        # Extract field data
        pred_field = prediction[:, channel_idx : channel_idx + num_channels, :, :]
        gt_field = ground_truth[:, channel_idx : channel_idx + num_channels, :, :]

        # Determine channel names
        if num_channels == 2 and field_name == "velocity":
            channel_names = ["x-component", "y-component"]
        elif num_channels == 1:
            channel_names = [field_name]
        else:
            channel_names = [f"{field_name}_{i}" for i in range(num_channels)]

        # Create plot
        save_path = save_dir / f"{field_name}_error_vs_time.png"
        plot_error_vs_time(
            pred_field,
            gt_field,
            field_name,
            save_path,
            metrics=metrics,
            channel_names=channel_names,
        )

        saved_paths[field_name] = save_path
        channel_idx += num_channels

    return saved_paths


def plot_error_comparison(
    errors_dict: Dict[str, torch.Tensor],
    field_name: str,
    save_path: Union[str, Path],
    channel_idx: int = 0,
    title: Optional[str] = None,
) -> plt.Figure:
    """
    Plot multiple error metrics on the same graph for comparison.

    Useful for comparing different error metrics (MSE vs MAE vs RMSE) on
    the same scale to understand their relative behavior.

    Args:
        errors_dict: Dictionary mapping metric names to error tensors [T, C]
        field_name: Name of the field
        save_path: Path where the plot will be saved
        channel_idx: Which channel to plot (for multi-channel fields)
        title: Custom title for the plot

    Returns:
        Matplotlib figure object
    """
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))

    num_timesteps = None
    colors = plt.cm.Set2(np.linspace(0, 1, len(errors_dict)))

    for (metric_name, error_values), color in zip(errors_dict.items(), colors):
        error_np = error_values.detach().cpu().numpy()  # [T, C]
        if num_timesteps is None:
            num_timesteps = error_np.shape[0]

        time_steps = np.arange(num_timesteps)

        # Plot the specified channel
        ax.plot(
            time_steps,
            error_np[:, channel_idx],
            label=metric_name.upper(),
            color=color,
            linewidth=2.5,
            marker="o" if num_timesteps < 20 else None,
            markersize=5,
        )

    ax.set_xlabel("Time Step", fontsize=12)
    ax.set_ylabel("Error", fontsize=12)
    ax.grid(True, alpha=0.3)
    ax.legend(loc="best", fontsize=11)

    if title is None:
        title = f"{field_name.capitalize()} - Error Metric Comparison"
    ax.set_title(title, fontsize=13, fontweight="bold")

    plt.tight_layout()

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    logger.debug(f"Comparison plot saved to {save_path}")

    return fig


def plot_error_heatmap(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_path: Union[str, Path],
    metric: str = "mse",
    max_frames: int = 50,
) -> plt.Figure:
    """
    Create a heatmap showing error evolution across time and channels.

    Useful for multi-channel fields to see which channels accumulate
    more error over time.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        field_name: Name of the field
        save_path: Path where the plot will be saved
        metric: Which metric to visualize ('mse', 'mae', 'rmse')
        max_frames: Maximum number of frames to show (subsamples if exceeded)

    Returns:
        Matplotlib figure object
    """
    # Compute metric
    error_dict = compute_all_metrics(prediction, ground_truth, metrics=[metric])
    error_values = error_dict[metric].detach().cpu().numpy()  # [T, C]

    num_timesteps, num_channels = error_values.shape

    # Subsample if too many frames
    if num_timesteps > max_frames:
        indices = np.linspace(0, num_timesteps - 1, max_frames, dtype=int)
        error_values = error_values[indices, :]
        time_labels = indices
    else:
        time_labels = np.arange(num_timesteps)

    # Create heatmap
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))

    im = ax.imshow(error_values.T, aspect="auto", cmap="hot", origin="lower")

    ax.set_xlabel("Time Step", fontsize=12)
    ax.set_ylabel("Channel", fontsize=12)
    ax.set_title(
        f"{field_name.capitalize()} - {metric.upper()} Heatmap",
        fontsize=13,
        fontweight="bold",
    )

    # Set ticks
    ax.set_yticks(np.arange(num_channels))
    if num_channels == 2:
        ax.set_yticklabels(["x", "y"])
    else:
        ax.set_yticklabels([f"ch{i}" for i in range(num_channels)])

    # Colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label(metric.upper(), fontsize=11)

    plt.tight_layout()

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    logger.debug(f"Heatmap saved to {save_path}")

    return fig


def plot_keyframe_comparison(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_path: Union[str, Path],
    num_keyframes: int = 5,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    show_difference: bool = True,
    show_metrics: bool = True,
) -> plt.Figure:
    """
    Plot evenly-spaced keyframes comparing prediction and ground truth.

    Creates a grid layout with keyframes at t=0, T/4, T/2, 3T/4, T showing
    side-by-side comparison of model prediction vs ground truth.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        field_name: Name of the field being visualized
        save_path: Path where the plot will be saved
        num_keyframes: Number of evenly-spaced frames to show (default: 5)
        vmin: Minimum value for color scale (auto-computed if None)
        vmax: Maximum value for color scale (auto-computed if None)
        show_difference: Whether to show difference column
        show_metrics: Whether to show error metrics for each frame

    Returns:
        Matplotlib figure object

    Example:
        >>> plot_keyframe_comparison(pred, gt, 'velocity', 'keyframes.png')
        # Creates 5×3 grid: [Ground Truth | Prediction | Difference] × 5 frames
    """
    # Validate inputs
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    # Convert to numpy and move to CPU
    pred_np = prediction.detach().cpu().numpy()
    gt_np = ground_truth.detach().cpu().numpy()

    # Handle different tensor shapes
    if pred_np.ndim == 4:  # [T, C, H, W]
        # For multi-channel data (e.g., 2D velocity), compute magnitude
        if pred_np.shape[1] == 2:
            pred_np = np.sqrt(pred_np[:, 0] ** 2 + pred_np[:, 1] ** 2)
            gt_np = np.sqrt(gt_np[:, 0] ** 2 + gt_np[:, 1] ** 2)
            field_label = f"{field_name.capitalize()} Magnitude"
        elif pred_np.shape[1] == 1:
            pred_np = pred_np[:, 0]
            gt_np = gt_np[:, 0]
            field_label = field_name.capitalize()
        else:
            raise ValueError(f"Unsupported channel count: {pred_np.shape[1]}")
    elif pred_np.ndim != 3:  # Should be [T, H, W]
        raise ValueError(f"Expected 3 or 4 dimensions, got {pred_np.ndim}")
    else:
        field_label = field_name.capitalize()

    # Transpose spatial dimensions: PhiFlow uses [x, y] but matplotlib expects [y, x]
    pred_np = np.transpose(pred_np, (0, 2, 1))  # [T, H, W] -> [T, W, H]
    gt_np = np.transpose(gt_np, (0, 2, 1))

    num_frames = pred_np.shape[0]

    # Select evenly-spaced keyframe indices
    if num_frames < num_keyframes:
        # If we have fewer frames than requested, use all of them
        keyframe_indices = list(range(num_frames))
        num_keyframes = num_frames
    else:
        # Evenly space keyframes: 0, T/4, T/2, 3T/4, T
        keyframe_indices = np.linspace(0, num_frames - 1, num_keyframes, dtype=int)

    # Compute global min/max for consistent color scale
    if vmin is None:
        vmin = min(gt_np.min(), pred_np.min())
    if vmax is None:
        vmax = max(gt_np.max(), pred_np.max())

    # Compute difference
    diff_np = np.abs(gt_np - pred_np)
    diff_vmax = diff_np.max()

    # Compute metrics if requested
    metrics_text = []
    if show_metrics:
        pred_tensor = prediction
        gt_tensor = ground_truth

        # Compute MSE and MAE for each keyframe
        for idx in keyframe_indices:
            frame_pred = pred_tensor[idx : idx + 1]
            frame_gt = gt_tensor[idx : idx + 1]

            mse = torch.mean((frame_pred - frame_gt) ** 2).item()
            mae = torch.mean(torch.abs(frame_pred - frame_gt)).item()

            metrics_text.append(f"MSE: {mse:.2e}\nMAE: {mae:.2e}")

    # Create figure
    num_cols = 3 if show_difference else 2
    fig = plt.figure(figsize=(6 * num_cols, 4 * num_keyframes))
    gs = GridSpec(num_keyframes, num_cols, figure=fig, hspace=0.3, wspace=0.3)

    # Column titles
    col_titles = ["Ground Truth", "Prediction"]
    if show_difference:
        col_titles.append("Absolute Difference")

    # Plot each keyframe
    for row_idx, frame_idx in enumerate(keyframe_indices):
        # Ground Truth
        ax_gt = fig.add_subplot(gs[row_idx, 0])
        im_gt = ax_gt.imshow(
            gt_np[frame_idx], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
        )

        if row_idx == 0:
            ax_gt.set_title(col_titles[0], fontsize=13, fontweight="bold")

        # Frame label on the left
        time_label = f"t = {frame_idx}/{num_frames-1}"
        if num_keyframes == 5:
            # Add fraction labels for standard 5-frame layout
            fractions = ["t = 0", "t = T/4", "t = T/2", "t = 3T/4", "t = T"]
            if row_idx < len(fractions):
                time_label = fractions[row_idx]

        ax_gt.set_ylabel(time_label, fontsize=11, fontweight="bold")
        ax_gt.set_xticks([])
        ax_gt.set_yticks([])

        # Add colorbar to first row
        if row_idx == 0:
            plt.colorbar(im_gt, ax=ax_gt, fraction=0.046, pad=0.04)

        # Prediction
        ax_pred = fig.add_subplot(gs[row_idx, 1])
        im_pred = ax_pred.imshow(
            pred_np[frame_idx], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
        )

        if row_idx == 0:
            ax_pred.set_title(col_titles[1], fontsize=13, fontweight="bold")

        ax_pred.set_xticks([])
        ax_pred.set_yticks([])

        # Add metrics text if requested
        if show_metrics and metrics_text:
            ax_pred.text(
                0.02,
                0.98,
                metrics_text[row_idx],
                transform=ax_pred.transAxes,
                fontsize=9,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
            )

        # Add colorbar to first row
        if row_idx == 0:
            plt.colorbar(im_pred, ax=ax_pred, fraction=0.046, pad=0.04)

        # Difference
        if show_difference:
            ax_diff = fig.add_subplot(gs[row_idx, 2])
            im_diff = ax_diff.imshow(
                diff_np[frame_idx], cmap="hot", vmin=0, vmax=diff_vmax, origin="lower"
            )

            if row_idx == 0:
                ax_diff.set_title(col_titles[2], fontsize=13, fontweight="bold")

            ax_diff.set_xticks([])
            ax_diff.set_yticks([])

            # Add colorbar to first row
            if row_idx == 0:
                plt.colorbar(im_diff, ax=ax_diff, fraction=0.046, pad=0.04)

    # Overall title
    fig.suptitle(
        f"{field_label} - Keyframe Comparison", fontsize=15, fontweight="bold", y=0.995
    )

    # Save
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_path, dpi=150, bbox_inches="tight")
    logger.debug(f"Keyframe comparison saved to {save_path}")
    plt.close(fig)

    return fig


def plot_keyframes_as_svgs(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_dir: Union[str, Path],
    num_keyframes: int = 5,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    show_difference: bool = True,
) -> None:
    """
    Plot evenly-spaced keyframes as individual SVG files.

    Creates separate SVG files for ground truth, prediction, and optionally
    the absolute difference for a selection of evenly-spaced keyframes.

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        field_name: Name of the field being visualized
        save_dir: Directory where the SVG files will be saved
        num_keyframes: Number of evenly-spaced frames to save (default: 5)
        vmin: Minimum value for color scale (auto-computed if None)
        vmax: Maximum value for color scale (auto-computed if None)
        show_difference: Whether to save difference plots
    """
    # Validate inputs
    if prediction.shape != ground_truth.shape:
        raise ValueError(
            f"Shape mismatch: prediction {prediction.shape} != ground_truth {ground_truth.shape}"
        )

    # Convert to numpy and move to CPU
    pred_np = prediction.detach().cpu().numpy()
    gt_np = ground_truth.detach().cpu().numpy()

    # Handle different tensor shapes
    if pred_np.ndim == 4:  # [T, C, H, W]
        # For multi-channel data (e.g., 2D velocity), compute magnitude
        if pred_np.shape[1] == 2:
            pred_np = np.sqrt(pred_np[:, 0] ** 2 + pred_np[:, 1] ** 2)
            gt_np = np.sqrt(gt_np[:, 0] ** 2 + gt_np[:, 1] ** 2)
            field_label = f"{field_name.capitalize()} Magnitude"
        elif pred_np.shape[1] == 1:
            pred_np = pred_np[:, 0]
            gt_np = gt_np[:, 0]
            field_label = field_name.capitalize()
        else:
            raise ValueError(f"Unsupported channel count: {pred_np.shape[1]}")
    elif pred_np.ndim != 3:  # Should be [T, H, W]
        raise ValueError(f"Expected 3 or 4 dimensions, got {pred_np.ndim}")
    else:
        field_label = field_name.capitalize()

    # Transpose spatial dimensions: PhiFlow uses [x, y] but matplotlib expects [y, x]
    pred_np = np.transpose(pred_np, (0, 2, 1))  # [T, H, W] -> [T, W, H]
    gt_np = np.transpose(gt_np, (0, 2, 1))

    num_frames = pred_np.shape[0]

    # Select evenly-spaced keyframe indices
    if num_frames < num_keyframes:
        keyframe_indices = list(range(num_frames))
    else:
        keyframe_indices = np.linspace(0, num_frames - 1, num_keyframes, dtype=int)

    # Compute global min/max for consistent color scale
    if vmin is None:
        vmin = min(gt_np.min(), pred_np.min())
    if vmax is None:
        vmax = max(gt_np.max(), pred_np.max())

    # Compute difference
    diff_np = np.abs(gt_np - pred_np)
    diff_vmax = diff_np.max()

    # Create save directory
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # Plot and save each keyframe
    for frame_idx in keyframe_indices:
        # Plot Ground Truth
        fig_gt, ax_gt = plt.subplots(figsize=(6, 4))
        im_gt = ax_gt.imshow(
            gt_np[frame_idx], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
        )
        ax_gt.set_title(f"Ground Truth: {field_label} at t={frame_idx}")
        ax_gt.set_xticks([])
        ax_gt.set_yticks([])
        plt.colorbar(im_gt, ax=ax_gt)
        gt_save_path = save_dir / f"frame_{frame_idx:04d}_gt.svg"
        plt.savefig(gt_save_path, format="svg", bbox_inches="tight")
        plt.close(fig_gt)

        # Plot Prediction
        fig_pred, ax_pred = plt.subplots(figsize=(6, 4))
        im_pred = ax_pred.imshow(
            pred_np[frame_idx], cmap="viridis", vmin=vmin, vmax=vmax, origin="lower"
        )
        ax_pred.set_title(f"Prediction: {field_label} at t={frame_idx}")
        ax_pred.set_xticks([])
        ax_pred.set_yticks([])
        plt.colorbar(im_pred, ax=ax_pred)
        pred_save_path = save_dir / f"frame_{frame_idx:04d}_pred.svg"
        plt.savefig(pred_save_path, format="svg", bbox_inches="tight")
        plt.close(fig_pred)

        # Plot Difference
        if show_difference:
            fig_diff, ax_diff = plt.subplots(figsize=(6, 4))
            im_diff = ax_diff.imshow(
                diff_np[frame_idx], cmap="hot", vmin=0, vmax=diff_vmax, origin="lower"
            )
            ax_diff.set_title(f"Difference: {field_label} at t={frame_idx}")
            ax_diff.set_xticks([])
            ax_diff.set_yticks([])
            plt.colorbar(im_diff, ax=ax_diff)
            diff_save_path = save_dir / f"frame_{frame_idx:04d}_diff.svg"
            plt.savefig(diff_save_path, format="svg", bbox_inches="tight")
            plt.close(fig_diff)

    logger.debug(f"{len(keyframe_indices)} keyframe SVGs saved to {save_dir}")


def plot_keyframes_as_svgs_multi_field(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_specs: Dict[str, int],
    save_dir: Union[str, Path],
    num_keyframes: int = 5,
    show_difference: bool = True,
) -> Dict[str, Path]:
    """
    Create keyframe SVGs for multiple fields.

    Args:
        prediction: Model prediction tensor, shape [T, C_total, H, W]
        ground_truth: Ground truth tensor, shape [T, C_total, H, W]
        field_specs: Dictionary mapping field names to channel counts
        save_dir: Directory where SVG subdirectories will be created
        num_keyframes: Number of evenly-spaced frames to show
        show_difference: Whether to generate difference plots

    Returns:
        Dictionary mapping field names to saved directory paths
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = {}
    channel_idx = 0

    for field_name, num_channels in field_specs.items():
        logger.debug(f"Creating keyframe SVGs for '{field_name}'...")

        # Extract field data
        pred_field = prediction[:, channel_idx : channel_idx + num_channels, :, :]
        gt_field = ground_truth[:, channel_idx : channel_idx + num_channels, :, :]

        # Create subdirectory for this field's SVGs
        field_save_dir = save_dir / f"{field_name}_keyframes_svg"

        # Create plot
        plot_keyframes_as_svgs(
            pred_field,
            gt_field,
            field_name,
            field_save_dir,
            num_keyframes=num_keyframes,
            show_difference=show_difference,
        )

        saved_paths[field_name] = field_save_dir
        channel_idx += num_channels

    return saved_paths


def plot_keyframe_comparison_multi_field(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_specs: Dict[str, int],
    save_dir: Union[str, Path],
    num_keyframes: int = 5,
    show_difference: bool = True,
    show_metrics: bool = True,
) -> Dict[str, Path]:
    """
    Create keyframe comparisons for multiple fields.

    Args:
        prediction: Model prediction tensor, shape [T, C_total, H, W]
        ground_truth: Ground truth tensor, shape [T, C_total, H, W]
        field_specs: Dictionary mapping field names to channel counts
        save_dir: Directory where plots will be saved
        num_keyframes: Number of evenly-spaced frames to show
        show_difference: Whether to show difference column
        show_metrics: Whether to show error metrics

    Returns:
        Dictionary mapping field names to saved file paths
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = {}
    channel_idx = 0

    for field_name, num_channels in field_specs.items():
        logger.debug(f"Creating keyframe comparison for '{field_name}'...")

        # Extract field data
        pred_field = prediction[:, channel_idx : channel_idx + num_channels, :, :]
        gt_field = ground_truth[:, channel_idx : channel_idx + num_channels, :, :]

        # Create plot
        save_path = save_dir / f"{field_name}_keyframes.png"
        plot_keyframe_comparison(
            pred_field,
            gt_field,
            field_name,
            save_path,
            num_keyframes=num_keyframes,
            show_difference=show_difference,
            show_metrics=show_metrics,
        )

        saved_paths[field_name] = save_path
        channel_idx += num_channels

    return saved_paths


def create_evaluation_summary(
    prediction: torch.Tensor,
    ground_truth: torch.Tensor,
    field_name: str,
    save_dir: Union[str, Path],
    metrics_to_compute: Optional[List[str]] = None,
    num_keyframes: int = 5,
    animation_fps: int = 10,
) -> Dict[str, Path]:
    """
    Create a complete evaluation summary with all visualizations.

    This is a convenience function that generates:
    1. Side-by-side comparison animation (GIF)
    2. Error vs time plots
    3. Keyframe comparison
    4. Error heatmap (if multi-channel)

    Args:
        prediction: Model prediction tensor, shape [T, C, H, W]
        ground_truth: Ground truth tensor, shape [T, C, H, W]
        field_name: Name of the field
        save_dir: Directory where all outputs will be saved
        metrics_to_compute: List of metrics for error plots
        num_keyframes: Number of keyframes for comparison
        animation_fps: FPS for animation

    Returns:
        Dictionary mapping output types to file paths

    Example:
        >>> paths = create_evaluation_summary(pred, gt, 'velocity', 'results/')
        >>> # Creates: animation.gif, error_plot.png, keyframes.png, heatmap.png
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    if metrics_to_compute is None:
        metrics_to_compute = ["mse", "mae"]

    saved_paths = {}

    logger.debug(f"\n{'='*60}")
    logger.debug(f"Creating evaluation summary for '{field_name}'")
    logger.debug(f"{'='*60}")

    # 1. Create animation
    logger.debug("\n[1/4] Creating comparison animation...")
    anim_path = save_dir / f"{field_name}_animation.gif"
    create_comparison_gif(
        prediction,
        ground_truth,
        field_name,
        anim_path,
        fps=animation_fps,
        show_difference=True,
    )
    saved_paths["animation"] = anim_path

    # 2. Create error plot
    logger.debug("\n[2/4] Creating error vs time plot...")
    error_path = save_dir / f"{field_name}_error_vs_time.png"
    plot_error_vs_time(
        prediction, ground_truth, field_name, error_path, metrics=metrics_to_compute
    )
    saved_paths["error_plot"] = error_path

    # 3. Create keyframe comparison
    logger.debug("\n[3/4] Creating keyframe comparison...")
    keyframe_path = save_dir / f"{field_name}_keyframes.png"
    plot_keyframe_comparison(
        prediction,
        ground_truth,
        field_name,
        keyframe_path,
        num_keyframes=num_keyframes,
        show_difference=True,
        show_metrics=True,
    )
    saved_paths["keyframes"] = keyframe_path

    # 4. Create heatmap (if multi-channel)
    if prediction.shape[1] > 1:
        logger.debug("\n[4/4] Creating error heatmap...")
        heatmap_path = save_dir / f"{field_name}_error_heatmap.png"
        plot_error_heatmap(prediction, ground_truth, field_name, heatmap_path)
        saved_paths["heatmap"] = heatmap_path
    else:
        logger.debug("\n[4/4] Skipping heatmap (single channel field)")

    logger.debug(f"\n{'='*60}")
    logger.debug(f"Evaluation summary complete!")
    logger.debug(f"{'='*60}")
    logger.debug(f"\nGenerated files:")
    for output_type, path in saved_paths.items():
        logger.debug(f"  - {output_type}: {path}")

    return saved_paths


--- src/factories/__init__.py ---

"""Factories for creating models and trainers."""

from src.factories.model_factory import ModelFactory
from src.factories.trainer_factory import TrainerFactory
from src.factories.dataloader_factory import DataLoaderFactory

__all__ = ["ModelFactory", "TrainerFactory", "DataLoaderFactory"]


--- src/factories/dataloader_factory.py ---

"""
DataLoader Factory

Simplified factory for creating data loaders with minimal configuration.
Replaces complex TrainerFactory data methods with a single, clear creation method.

This factory uses ConfigHelper to extract parameters and creates either
TensorDataset or FieldDataset based on the mode parameter.
"""

from typing import List, Optional, Literal, Union
from pathlib import Path
import torch
from torch.utils.data import DataLoader

from src.data import DataManager, TensorDataset, FieldDataset
from src.config import ConfigHelper
from src.utils.logger import get_logger
from src.data.dataset_utilities import field_collate_fn, tensor_collate_fn

logger = get_logger(__name__)


class DataLoaderFactory:
    """
    Factory for creating data loaders with minimal configuration.

    This replaces the complex TrainerFactory data methods with a
    single, clear creation method that uses ConfigHelper to extract
    all necessary parameters.

    Key simplifications:
    - Single creation method vs 4 separate methods
    - ConfigHelper handles all config extraction
    - Clear mode parameter: 'tensor' or 'field'
    - Consistent interface for all use cases

    Example:
        >>> from src.factories import DataLoaderFactory
        >>>
        >>> # Synthetic training (returns DataLoader)
        >>> loader = DataLoaderFactory.create(
        ...     config,
        ...     mode='tensor',
        ...     shuffle=True
        ... )
        >>>
        >>> # Physical training (returns Dataset)
        >>> dataset = DataLoaderFactory.create(
        ...     config,
        ...     mode='field',
        ...     batch_size=None
        ... )
    """

    @staticmethod
    def create(
        config: dict,
        mode: Literal["tensor", "field"] = "tensor",
        sim_indices: Optional[List[int]] = None,
        batch_size: Optional[int] = None,
        shuffle: bool = True,
        enable_augmentation: Optional[bool] = None,
        num_workers: int = 0,
        percentage_real_data: float = 1.0,
    ) -> Union[DataLoader, FieldDataset]:
        """
        Create a data loader or dataset for training.

        This method:
        1. Extracts configuration using ConfigHelper
        2. Creates DataManager
        3. Determines field specifications
        4. Creates appropriate dataset (Tensor or Field)
        5. Wraps in DataLoader (for tensor mode) or returns Dataset (for field mode)

        Args:
            config: Full configuration dictionary from Hydra
            mode: 'tensor' for synthetic models, 'field' for physical models
            sim_indices: Simulation indices to use (default: from config)
            batch_size: Batch size for DataLoader (default: from config, None for field mode)
            shuffle: Whether to shuffle data (default: True)
            use_sliding_window: Use sliding window (default: from config)
            enable_augmentation: Enable augmentation (default: from config)
            num_workers: Number of DataLoader workers (default: 0)

        Returns:
            - DataLoader: For 'tensor' mode (suitable for synthetic training)
            - FieldDataset: For 'field' mode (suitable for physical training)

        Raises:
            ValueError: If mode is invalid or configuration is invalid

        Example:
            >>> # Tensor mode (synthetic training)
            >>> loader = DataLoaderFactory.create(
            ...     config,
            ...     mode='tensor',
            ...     sim_indices=[0, 1, 2],
            ...     batch_size=16,
            ...     shuffle=True
            ... )
            >>> for initial, targets in loader:
            ...     # initial: [B, C_all, H, W]
            ...     # targets: [B, T, C_dynamic, H, W]
            ...     pass
            >>>
            >>> # Field mode (physical training)
            >>> dataset = DataLoaderFactory.create(
            ...     config,
            ...     mode='field',
            ...     sim_indices=[0, 1, 2],
            ...     batch_size=None,  # Physical models don't use batching
            ... )
            >>> for initial_fields, target_fields in dataset:
            ...     # initial_fields: Dict[str, Field]
            ...     # target_fields: Dict[str, List[Field]]
            ...     pass
        """
        logger.debug(f"Creating data loader (mode={mode})...")

        # === Step 1: Extract configuration using ConfigHelper ===
        cfg = ConfigHelper(config)

        # Validate configuration
        issues = cfg.validate()
        if issues:
            raise ValueError(
                f"Invalid configuration:\n"
                + "\n".join(f"  - {issue}" for issue in issues)
            )

        # Get parameters (use provided values or fall back to config)
        sim_indices = (
            sim_indices if sim_indices is not None else cfg.get_train_sim_indices()
        )
        batch_size = batch_size if batch_size is not None else cfg.get_batch_size()
        enable_augmentation = (
            enable_augmentation
            if enable_augmentation is not None
            else cfg.is_augmentation_enabled()
        )

        num_frames = None
        num_predict_steps = cfg.get_num_predict_steps()

        logger.debug(f"  Simulations: {len(sim_indices)}")
        logger.debug(f"  Batch size: {batch_size}")
        logger.debug(f"  Augmentation: {enable_augmentation}")

        # === Step 2: Create DataManager ===
        data_manager = DataLoaderFactory._create_data_manager(config, cfg)

        # === Step 3: Get field specifications ===
        field_names = cfg.get_field_names()

        # === Step 4: Get augmentation config ===
        augmentation_config = None
        if enable_augmentation:
            augmentation_config = cfg.get_augmentation_config()
            logger.debug(f"  Augmentation mode: {augmentation_config['mode']}")
            logger.debug(f"  Augmentation alpha: {augmentation_config['alpha']}")

        # === Step 5: Create dataset based on mode ===
        if mode == "tensor":

            dataset = TensorDataset(
                data_manager=data_manager,
                sim_indices=sim_indices,
                field_names=field_names,
                num_frames=num_frames,
                num_predict_steps=num_predict_steps,
                augmentation_config=augmentation_config,
                percentage_real_data=percentage_real_data,
            )

            # Wrap in DataLoader for batching
            logger.debug(f"  Created TensorDataset with {len(dataset)} samples")
            logger.debug(
                f"  Creating DataLoader (batch_size={batch_size}, shuffle={shuffle})..."
            )

            data_loader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                collate_fn=tensor_collate_fn,
                num_workers=num_workers,
                pin_memory=torch.cuda.is_available(),
            )

            logger.debug(f"DataLoader created successfully")
            return data_loader

        elif mode == "field":
            # Field mode: for physical (PDE-based) models
            logger.debug(f"  Fields: {field_names}")

            dataset = FieldDataset(
                data_manager=data_manager,
                sim_indices=sim_indices,
                field_names=field_names,
                num_frames=num_frames,
                num_predict_steps=num_predict_steps,
                augmentation_config=augmentation_config,
                percentage_real_data=percentage_real_data,
            )

            # Return dataset directly (no DataLoader for field mode)
            logger.debug(f"  Created FieldDataset with {len(dataset)} samples")
            logger.debug(f"FieldDataset created successfully")

            data_loader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=num_workers,
                collate_fn=field_collate_fn,
            )
            logger.debug(f"DataLoader created successfully")
            return data_loader

        else:
            raise ValueError(f"Unknown mode: {mode}. Must be 'tensor' or 'field'.")

    @staticmethod
    def _create_data_manager(config: dict, cfg: ConfigHelper) -> DataManager:
        """
        Create DataManager from configuration.

        Args:
            config: Full configuration dictionary
            cfg: ConfigHelper instance

        Returns:
            Configured DataManager instance

        Note: Cache creation and validation are always enabled (hardcoded).
        """
        # Get paths
        project_root = cfg.get_project_root()
        raw_data_dir = project_root / cfg.get_raw_data_dir()
        cache_dir = project_root / cfg.get_cache_dir()

        # Get auto-clear setting
        auto_clear_invalid = cfg.should_auto_clear_invalid()

        logger.debug(f"  Raw data: {raw_data_dir}")
        logger.debug(f"  Cache: {cache_dir}")

        return DataManager(
            raw_data_dir=str(raw_data_dir),
            cache_dir=str(cache_dir),
            config=config,
            auto_clear_invalid=auto_clear_invalid,
        )

    @staticmethod
    def create_for_evaluation(
        config: dict,
        mode: Literal["tensor", "field"] = "tensor",
        sim_indices: Optional[List[int]] = None,
    ) -> Union[DataLoader, FieldDataset]:
        """
        Create a data loader/dataset for evaluation.

        Convenience method that uses evaluation-specific defaults:
        - No shuffling
        - No augmentation
        - Uses validation sim indices if not specified

        Args:
            config: Full configuration dictionary
            mode: 'tensor' or 'field'
            sim_indices: Simulation indices (default: validation sims from config)

        Returns:
            DataLoader or FieldDataset configured for evaluation
        """
        cfg = ConfigHelper(config)

        # Use validation sims if not specified
        if sim_indices is None:
            sim_indices = cfg.get_val_sim_indices()
            if not sim_indices:
                logger.warning("No validation sims specified, using train sims")
                sim_indices = cfg.get_train_sim_indices()

        return DataLoaderFactory.create(
            config=config,
            mode=mode,
            sim_indices=sim_indices,
            shuffle=False,  # Don't shuffle for evaluation
            enable_augmentation=False,  # No augmentation for evaluation
        )

    @staticmethod
    def get_info(config: dict) -> dict:
        """
        Get information about what data loader would be created.

        Useful for debugging and validation without actually creating the loader.

        Args:
            config: Full configuration dictionary

        Returns:
            Dictionary with data loader configuration information
        """
        cfg = ConfigHelper(config)

        return {
            "dataset_name": cfg.get_dataset_name(),
            "model_type": cfg.get_model_type(),
            "field_names": cfg.get_field_names(),
            "train_sims": cfg.get_train_sim_indices(),
            "val_sims": cfg.get_val_sim_indices(),
            "batch_size": cfg.get_batch_size(),
            "num_predict_steps": cfg.get_num_predict_steps(),
            "use_sliding_window": cfg.should_use_sliding_window(),
            "augmentation_enabled": cfg.is_augmentation_enabled(),
            "augmentation_config": cfg.get_augmentation_config(),
        }


--- src/factories/model_factory.py ---

"""Factory for creating models."""

from typing import Dict, Any
import torch.nn as nn
from src.models.registry import ModelRegistry


class ModelFactory:
    """Factory for creating model instances."""

    @staticmethod
    def create_physical_model(config: Dict[str, Any]):
        """
        Create physical model from config.

        Args:
            config: Full configuration dictionary

        Returns:
            Physical model instance
        """
        model_config = config["model"]["physical"]
        model_name = model_config["name"]
        return ModelRegistry.get_physical_model(model_name, model_config)

    @staticmethod
    def create_synthetic_model(config: Dict[str, Any]) -> nn.Module:
        """
        Create synthetic model from config.

        Args:
            config: Full configuration dictionary

        Returns:
            Synthetic model instance
        """
        model_config = config["model"]
        model_name = model_config['synthetic']['name']
        return ModelRegistry.get_synthetic_model(model_name, model_config)

    @staticmethod
    def list_available_models():
        """
        List all available models.

        Returns:
            Dictionary with 'physical' and 'synthetic' model lists
        """
        return {
            "physical": ModelRegistry.list_physical_models(),
            "synthetic": ModelRegistry.list_synthetic_models(),
        }


--- src/factories/trainer_factory.py ---

"""Factory for creating trainers with Phase 1 API."""

from typing import Dict, Any, List
from pathlib import Path
import torch
from torch.utils.data import DataLoader

from phi.math import math, Tensor

from src.training.abstract_trainer import AbstractTrainer
from src.training.synthetic.trainer import SyntheticTrainer
from src.training.physical.trainer import PhysicalTrainer

# HybridTrainer imported lazily to avoid circular dependency
from src.data import DataManager, FieldDataset
from src.factories.model_factory import ModelFactory
from src.factories.dataloader_factory import DataLoaderFactory
from src.utils.logger import get_logger
import warnings

logger = get_logger(__name__)


class TrainerFactory:
    """
    Factory for creating trainer instances with Phase 1 API.

    Phase 1: Creates models and data externally, passes to trainers.
    Trainers now receive:
    - SyntheticTrainer(config, model)
    - PhysicalTrainer(config, model, learnable_params)
    - HybridTrainer(config, synthetic_model, physical_model, learnable_params)
    """

    _trainers = {
        "synthetic": SyntheticTrainer,
        "physical": PhysicalTrainer,
        # "hybrid" added lazily to avoid circular import
    }

    @staticmethod
    def list_available_trainers() -> List[str]:
        """Get list of available trainer types."""
        return ["synthetic", "physical", "hybrid"]

    @staticmethod
    def create_trainer(config: Dict[str, Any]) -> AbstractTrainer:
        """
        Create trainer from config with Phase 1 API.

        Args:
            config: Configuration dictionary

        Returns:
            Trainer instance (AbstractTrainer subclass)

        Raises:
            ValueError: If model_type is unknown
        """
        model_type = config["run_params"]["model_type"]

        available = TrainerFactory.list_available_trainers()
        if model_type not in available:
            raise ValueError(
                f"Unknown model_type '{model_type}'. " f"Available: {available}"
            )

        # Create trainer based on type
        if model_type == "synthetic":
            return TrainerFactory._create_synthetic_trainer(config)
        elif model_type == "physical":
            return TrainerFactory._create_physical_trainer(config)
        elif model_type == "hybrid":
            return TrainerFactory.create_hybrid_trainer(config)
        else:
            raise ValueError(f"Unknown model_type '{model_type}'")

    @staticmethod
    def _create_synthetic_trainer(config: Dict[str, Any]) -> SyntheticTrainer:
        """
        Create SyntheticTrainer with external model.

        Args:
            config: Full configuration dictionary

        Returns:
            SyntheticTrainer instance
        """
        # Create model externally
        model = ModelFactory.create_synthetic_model(config)

        # Create trainer with model
        trainer = SyntheticTrainer(config, model)

        return trainer

    # @staticmethod
    def _create_physical_trainer(config: Dict[str, Any]) -> PhysicalTrainer:
        """
        Create PhysicalTrainer with external model and learnable parameters.

        Args:
            config: Full configuration dictionary

        Returns:
            PhysicalTrainer instance
        """
        # Create model externally
        model = ModelFactory.create_physical_model(config)

        # Extract learnable parameters from config
        learnable_params = config["trainer_params"].get(
            "learnable_parameters", []
        )

        if not learnable_params:
            raise ValueError(
                "No 'learnable_parameters' defined in trainer_params for physical training."
            )
        # Create trainer with model and params
        trainer = PhysicalTrainer(config, model, learnable_params)

        return trainer

    @staticmethod
    def create_data_loader_for_synthetic(
        config: Dict[str, Any],
        sim_indices: List[int] = None,
        batch_size: int = None,
        shuffle: bool = True,
        use_sliding_window: bool = True,
    ) -> DataLoader:
        """
        Create DataLoader for synthetic training with optional augmentation.

        DEPRECATED: Use DataLoaderFactory.create(config, mode='tensor') instead.
        This method is kept for backward compatibility but will be removed in a future version.

        Args:
            config: Full configuration dictionary
            sim_indices: Simulation indices to load (defaults to train_sim from config)
            batch_size: Batch size (defaults to config batch_size)
            shuffle: Whether to shuffle data
            use_sliding_window: Whether to use sliding window (default True for Phase 1)

        Returns:
            DataLoader with TensorDataset
        """
        warnings.warn(
            "create_data_loader_for_synthetic is deprecated. "
            "Use DataLoaderFactory.create(config, mode='tensor') instead.",
            DeprecationWarning,
            stacklevel=2,
        )

        # Use new DataLoaderFactory
        return DataLoaderFactory.create(
            config=config,
            mode="tensor",
            sim_indices=sim_indices,
            batch_size=batch_size,
            shuffle=shuffle,
            use_sliding_window=use_sliding_window,
        )

    @staticmethod
    def create_dataset_for_physical(
        config: Dict[str, Any],
        sim_indices: List[int] = None,
        use_sliding_window: bool = True,
    ) -> FieldDataset:
        """
        Create FieldDataset for physical training (returns fields, not tensors).

        DEPRECATED: Use DataLoaderFactory.create(config, mode='field') instead.
        This method is kept for backward compatibility but will be removed in a future version.

        Args:
            config: Full configuration dictionary
            sim_indices: Simulation indices to load (defaults to train_sim from config)
            use_sliding_window: Whether to use sliding window (default True for Phase 1)

        Returns:
            FieldDataset (new simplified version)
        """
        warnings.warn(
            "create_dataset_for_physical is deprecated. "
            "Use DataLoaderFactory.create(config, mode='field') instead.",
            DeprecationWarning,
            stacklevel=2,
        )

        # Use new DataLoaderFactory which returns FieldDataset directly
        return DataLoaderFactory.create(
            config=config,
            mode="field",
            sim_indices=sim_indices,
            use_sliding_window=use_sliding_window,
            batch_size=None,  # Physical training doesn't use batching
        )

    @staticmethod
    def generate_augmented_cache(
        config: Dict[str, Any],
        model: torch.nn.Module,
        model_type: str = "synthetic",
        force_regenerate: bool = False,
    ) -> int:
        """
        Generate and cache augmented predictions for training.

        DEPRECATED: This method is deprecated and not yet updated for the new architecture.
        Cache generation is now handled differently in hybrid training.

        Args:
            config: Full configuration dictionary
            model: Trained model to generate predictions
            model_type: 'synthetic' or 'physical'
            force_regenerate: If True, clear existing cache and regenerate

        Returns:
            Number of samples generated and cached
        """
        warnings.warn(
            "generate_augmented_cache is deprecated and not yet updated for the new architecture. "
            "Cache generation is now handled differently in hybrid training.",
            DeprecationWarning,
            stacklevel=2,
        )
        raise NotImplementedError(
            "generate_augmented_cache is not yet implemented in the new architecture. "
            "Augmentation is now handled directly via augmentation_config in datasets."
        )

    @staticmethod
    def create_hybrid_trainer(config: Dict[str, Any]):
        """
        Create a hybrid trainer that alternates between synthetic and physical training.

        Args:
            config: Full configuration dictionary

        Returns:
            HybridTrainer instance configured with both models
        """
        from src.training.hybrid import HybridTrainer

        logger.info("Creating hybrid trainer...")

        # Create synthetic model
        synthetic_model = ModelFactory.create_synthetic_model(config)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        synthetic_model.to(device)

        # Create physical model and learnable parameters
        physical_model = ModelFactory.create_physical_model(config)

        # Extract learnable parameters from config (same as _create_physical_trainer)
        learnable_params = config["trainer_params"].get(
            "learnable_parameters", {}
        )
        # Create learnable parameter tensors (empty list if none defined, e.g., for advection)

        # Create hybrid trainer
        hybrid_trainer = HybridTrainer(
            config=config,
            synthetic_model=synthetic_model,
            physical_model=physical_model,
            learnable_params=learnable_params,
        )

        logger.info("Hybrid trainer created successfully")
        return hybrid_trainer

    @staticmethod
    def register_trainer(name: str, trainer_class: type):
        """
        Register a new trainer type.

        Args:
            name: Name to register the trainer under
            trainer_class: Trainer class to register
        """
        TrainerFactory._trainers[name] = trainer_class


--- src/models/__init__.py ---

# src/models/__init__.py

"""
Models package initialization.

This file imports all models to trigger their registration with the ModelRegistry.
It also exports the registry for use by trainers and evaluators.
"""

# Import registry first
from .registry import ModelRegistry

# Import physical models (triggers registration via decorators)
from . import physical

# Import synthetic models (triggers registration via decorators)
from . import synthetic

# Export the registry and model classes
__all__ = ["ModelRegistry", "physical", "synthetic"]


--- src/models/physical/__init__.py ---

# src/models/physical/__init__.py

# This file makes your model classes importable

# Import the base class so it can be accessed if needed
from .base import PhysicalModel

# Import your concrete model classes
from .smoke import SmokeModel

# You can add other models here later
from .burgers import BurgersModel


from .advection import AdvectionModel


--- src/models/physical/advection.py ---

# src/models/physical/advection.py

from typing import Dict
import numpy as np

# --- PhiFlow Imports ---
from phi.torch.flow import *
from phi.math import Shape, Tensor, batch, math

# --- Repo Imports ---
from .base import PhysicalModel
from src.models.registry import ModelRegistry


@jit_compile
def _advection_step(
    density: CenteredGrid, velocity: CenteredGrid, advection_coeff: Tensor, dt: float
) -> CenteredGrid:
    """
    Performs one step of pure advection using semi-Lagrangian method.

    Args:
        density (CenteredGrid): The current density field.
        velocity (CenteredGrid): The prescribed velocity field (vector-valued).
        advection_coeff (Tensor): Coefficient to scale the velocity field.
        dt (float): The time step.

    Returns:
        CenteredGrid: The density field at the next time step.
    """
    # Scale velocity by advection coefficient
    velocity = velocity * advection_coeff
    return advect.semi_lagrangian(density, velocity, dt=dt), velocity, advection_coeff


@ModelRegistry.register_physical("AdvectionModel")
class AdvectionModel(PhysicalModel):
    """
    Physical model for pure advection with a donut shaped velocity field.
    The density is transported by a donut shaped velocity field,
    scaled by a learnable advection coefficient.
    """

    # Declare PDE-specific parameters
    PDE_PARAMETERS = {
        "advection_coeff": {
            "type": float,
            "default": 1.0,
        }
    }

    def __init__(self, config: dict):
        """Initialize the advection model."""
        super().__init__(config)

    def get_initial_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Returns a batched initial state with density and static velocity field.

        The velocity field is created once here and will be passed through
        the state dictionary to each step.
        """
        # Create a batch shape
        b = batch(batch=batch_size)

        # Create a nice swirling/rotating velocity field
        def velocity_fn(x, y):
            # Vortex-like pattern: velocity vector depends on position
            center_x = self.domain.size[0] / 2
            center_y = self.domain.size[1] / 2
            dy = y - center_y
            dx = x - center_x
            r = math.sqrt(dx**2 + dy**2 + 1e-6)

            # Circular flow with some variation
            vx = -dy * math.exp(
                -(r**2) / (0.2 * self.domain.size[0]) ** 2
            ) + 0.2 * math.sin(2 * math.pi * y / self.domain.size[1])
            vy = dx * math.exp(
                -(r**2) / (0.2 * self.domain.size[0]) ** 2
            ) + 0.2 * math.cos(2 * math.pi * x / self.domain.size[0])

            return math.stack([vx, vy], channel("vector"))

        # Random line through two random points
        x1 = np.random.uniform(0.2 * self.domain.size[0], 0.8 * self.domain.size[0])
        y1 = np.random.uniform(0.2 * self.domain.size[1], 0.8 * self.domain.size[1])
        x2 = np.random.uniform(0.2 * self.domain.size[0], 0.8 * self.domain.size[0])
        y2 = np.random.uniform(0.2 * self.domain.size[1], 0.8 * self.domain.size[1])

        # Line direction vector
        dx_line = x2 - x1
        dy_line = y2 - y1
        line_length = np.sqrt(dx_line**2 + dy_line**2)

        # Normal vector to the line (perpendicular)
        nx = -dy_line / line_length
        ny = dx_line / line_length

        def density_fn(x, y):
            # Signed distance from point (x,y) to the line
            signed_distance = (x - x1) * nx + (y - y1) * ny
            steepness = 10.0 / max(self.domain.size[0], self.domain.size[1])
            return math.tanh(steepness * signed_distance)

        # Create CenteredGrid for velocity with vector values
        velocity_0 = CenteredGrid(
            velocity_fn,
            extrapolation=extrapolation.PERIODIC,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        velocity_0 = math.expand(velocity_0, b)

        # Create density field with smooth tanh transition
        density_0 = CenteredGrid(
            density_fn,
            extrapolation=extrapolation.ZERO_GRADIENT,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        density_0 = math.expand(density_0, b)

        return {"density": density_0, "velocity": velocity_0}

    def get_random_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Returns a batched initial state with density and static velocity field.

        The velocity field is created once here and will be passed through
        the state dictionary to each step (like smoke's inflow pattern).
        """
        b = batch(batch=batch_size)
        # Create a nice swirling/rotating velocity field
        def velocity_fn(x, y):
            # Vortex-like pattern: velocity vector depends on position
            center_x = self.domain.size[0] / 2
            center_y = self.domain.size[1] / 2
            dy = y - center_y
            dx = x - center_x
            r = math.sqrt(dx**2 + dy**2 + 1e-6)

            # Circular flow with some variation
            vx = -dy * math.exp(
                -(r**2) / (0.2 * self.domain.size[0]) ** 2
            ) + 0.2 * math.sin(2 * math.pi * y / self.domain.size[1])
            vy = dx * math.exp(
                -(r**2) / (0.2 * self.domain.size[0]) ** 2
            ) + 0.2 * math.cos(2 * math.pi * x / self.domain.size[0])

            return math.stack([vx, vy], channel("vector"))

        # Create CenteredGrid for velocity with vector values
        velocity_0 = CenteredGrid(
            velocity_fn,
            extrapolation=extrapolation.PERIODIC,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        velocity_0 = math.expand(velocity_0, b)
        # Create density field with smooth tanh transition
        scale = math.random_uniform(low=1, high=10)
        smoothness = math.random_uniform(low=1.0, high=5.0)
        density_0 = CenteredGrid(
            Noise(scale=scale, smoothness=smoothness),
            extrapolation=extrapolation.ZERO_GRADIENT,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        density_0 = math.tanh(2.0 * density_0)
        density_0 = math.expand(density_0, b)

        return {"density": density_0, "velocity": velocity_0}
    
    def rollout(self, initial_state, num_steps):
        """
        Perform multiple simulation steps starting from the initial state.

        Args:
            initial_state: Dictionary containing initial 'density' and 'velocity' fields.
            num_steps: Number of simulation steps to perform.
        Returns:
            List of states at each timestep.
        """
        density_trj, velocity_trj, _ = iterate(_advection_step, batch(time=num_steps), initial_state["density"], initial_state["velocity"], self.advection_coeff, dt = self.dt)
        return {"density": density_trj, "velocity": velocity_trj}

    def forward(self, current_state: Dict[str, Field]) -> Dict[str, Field]:
        """
        Performs a single simulation step using pure advection.

        Args:
            current_state: Dictionary containing 'density' and 'velocity' fields.

        Returns:
            Dictionary with updated 'density' and unchanged 'velocity'.
        """
        new_density, new_velocity, _ = _advection_step(
            density=current_state["density"], 
            velocity=current_state["velocity"],
            advection_coeff=self.advection_coeff,
            dt=self.dt,
        )
        # Velocity field remains static (like smoke's inflow)
        return {"density": new_density, "velocity": new_velocity}


--- src/models/physical/base.py ---

# src/models/physical/base.py

from abc import ABC, abstractmethod
from phi.flow import Field, Box, math, batch, plot
from phi.math import Shape, spatial
import matplotlib.pyplot as plt
from src.utils.logger import get_logger
from typing import Dict, Any, Callable, Optional, List
import torch
import logging


class PhysicalModel(ABC):
    """
    Abstract Base Class for all physical PDE models.

    This interface guarantees that all physical models can:
    1. Be initialized from a configuration.
    2. Generate a batched initial state (t=0).
    3. Be advanced one time step.

    Child classes should declare their PDE-specific parameters by overriding
    the PDE_PARAMETERS class variable:

    Example:
        class BurgersModel(PhysicalModel):
            PDE_PARAMETERS = {
                'nu': {'type': float, 'default': 0.01}
            }
    """

    # Child classes override this to declare their PDE parameters
    PDE_PARAMETERS: Dict[str, Dict[str, Any]] = {}

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the model from a configuration dictionary.

        Args:
            config (Dict): Configuration dictionary containing:
                - domain: Dict with 'size_x' and 'size_y'
                - resolution: Dict with 'x' and 'y'
                - dt: float time step
                - pde_params: Dict with model-specific parameters
        """
        # Parse common configuration
        self.domain = self._parse_domain(config["domain"])
        self.resolution = self._parse_resolution(config["resolution"])
        self.dt = float(config["dt"])

        # Parse batch_size from pde_params (default: 1)
        pde_params = config["pde_params"]
        # Parse and validate PDE-specific parameters
        self._parse_pde_parameters(pde_params)

        self.logger = get_logger(__name__)
        # Simple string representation to avoid Unicode superscript issues
        self.logger.info(
            f"Initialized {self.__class__.__name__} with resolution={tuple(self.resolution.sizes)}, dt={self.dt}"
        )

    def _parse_domain(self, domain_config: Dict[str, Any]) -> Box:
        """Parse domain configuration into a Box object."""
        size_x = domain_config["size_x"]
        size_y = domain_config["size_y"]
        return Box(x=size_x, y=size_y)

    def _parse_resolution(self, resolution_config: Dict[str, Any]) -> Shape:
        """Parse resolution configuration into a Shape object."""
        x = resolution_config["x"]
        y = resolution_config["y"]
        return spatial(x=x, y=y)

    def _parse_pde_parameters(self, pde_params: Dict[str, Any]):
        """
        Parse and validate PDE-specific parameters based on PDE_PARAMETERS declaration.

        Creates properties for each parameter with automatic getter/setter.
        """
        for param_name, param_spec in self.PDE_PARAMETERS.items():
            # Extract parameter specification
            param_type = param_spec["type"]
            default_value = param_spec["default"]
            # Get value from config or use default
            if param_name in pde_params:
                value = pde_params[param_name]
                # Convert to appropriate type
                if param_type is not None:
                    value = param_type(value)
            elif default_value is not None:
                value = default_value
            else:
                self.logger.error(
                    f"Missing required PDE parameter '{param_name}'"
                )

            # Store as private attribute
            private_name = f"_{param_name}"
            setattr(self, private_name, value)

            # Create property dynamically
            self._create_property(param_name)

    def _create_property(self, param_name: str):
        """
        Dynamically create a property for a PDE parameter.

        This creates a getter and setter that access the private attribute.
        """
        private_name = f"_{param_name}"

        # Create getter and setter functions
        def getter(self):
            return getattr(self, private_name)

        def setter(self, value):
            # Re-validate if validator exists
            setattr(self, private_name, value)

        # Set property on the class (not instance)
        prop = property(getter, setter)
        setattr(self.__class__, param_name, prop)

    @abstractmethod
    def get_initial_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Generates a batched initial state (t=0) for the simulation.

        The batch dimension should be named 'batch'.

        Args:
            batch_size (int): The number of parallel simulations.

        Returns:
            Dict[str, Field]: A dictionary mapping field names to their
                              initial Field values.
        """
        pass

    @abstractmethod
    def get_random_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Generates a random state for the simulation.

        The batch dimension should be named 'batch'.

        Args:
            batch_size (int): The number of parallel simulations.

        Returns:
            Dict[str, Field]: A dictionary mapping field names to their
                              random Field values.
        """
        pass

    @abstractmethod
    def forward(self, current_state: Dict[str, Field]) -> Dict[str, Field]:
        """
        Advances the simulation by one time step (dt).

        Args:
            *current_state (Field): A variable number of fields that
                                    make up the current state, passed in
                                    the same order as returned by
                                    get_initial_state().

        Returns:
            tuple[Field, ...]: A tuple of Fields representing the next state.
        """
        pass

    @abstractmethod
    def rollout(self, initial_state: Dict[str, Field], num_steps: int) -> Dict[str, Field]:
        """
        Roll out the simulation for a specified number of time steps.

        Args:
            initial_state (Dict[str, Field]): The initial state at t=0.
            num_steps (int): The number of time steps to roll out.
        Returns:
            Dict[str, Field]: A dictionary mapping field names to their
                                states after num_steps.
        """
        pass

    def __call__(self, *args) -> tuple[Field, ...]:
        """Convenience wrapper for the forward method."""
        return self.forward(*args)

    @staticmethod
    def _select_proportional_indices(total_count: int, sample_count: int):
        """
        Select indices proportionally across the dataset.

        Ensures diverse sampling rather than just taking the first N samples.
        """
        if sample_count >= total_count:
            return list(range(total_count))
        elif sample_count <= 0:
            return []

        # Calculate step size for proportional sampling
        step = total_count / sample_count

        # Select indices evenly distributed
        indices = [int(i * step) for i in range(sample_count)]

        # Ensure no duplicates and within bounds
        indices = sorted(list(set(indices)))[:sample_count]

        return indices


--- src/models/physical/burgers.py ---

# src/models/physical/burgers.py

from typing import Dict
import numpy as np

# --- PhiFlow Imports ---
from phi.torch.flow import *
from phi.math import Shape, Tensor, batch, math

# --- Repo Imports ---
from .base import PhysicalModel
from src.models.registry import ModelRegistry


@jit_compile
def _burgers_physics_step(
    velocity: CenteredGrid, dt: float, nu: Tensor
) -> CenteredGrid:
    """
    Performs one physics-based Burgers' equation step.

    Args:
        velocity (CenteredGrid): Current velocity field.
        dt (float): Time step.
        nu (Tensor): Viscosity parameter.

    Returns:
        CenteredGrid: new_velocity
    """
    # Advect velocity (self-advection: u * grad(u))
    velocity = advect.semi_lagrangian(velocity, velocity, dt=dt)

    # Diffuse velocity (viscosity: nu * laplace(u))
    velocity = diffuse.explicit(velocity, nu, dt=dt)

    return velocity


# --- Model Class Implementation ---


@ModelRegistry.register_physical("BurgersModel")
class BurgersModel(PhysicalModel):
    """
    Physical model for the Burgers' equation.
    Implements the PhysicalModel interface.
    """

    # Declare PDE-specific parameters
    PDE_PARAMETERS = {
        "nu": {
            "type": float,
            "default": 0.01,
        }
    }

    def __init__(self, config: dict):
        """Initialize the Burgers model."""
        super().__init__(config)

    def get_initial_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Returns an initial state of (noisy velocity).
        We use periodic boundaries as they are common for Burgers.
        """
        b = batch(batch=batch_size)

        temp = StaggeredGrid(
            Noise(scale=20),  # Initialize with noise
            extrapolation.PERIODIC,  # Use periodic boundaries
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )

        velocity_0 = CenteredGrid(
            temp,
            extrapolation.PERIODIC,  # Use periodic boundaries
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        velocity_0 = math.expand(velocity_0, b)
        return {"velocity": velocity_0}

    def get_random_state(self) -> Dict[str, Field]:
        """
        Returns a random initial state of (noisy velocity).
        We use periodic boundaries as they are common for Burgers.
        """
        temp = StaggeredGrid(
            Noise(scale=20),  # Initialize with noise
            extrapolation.PERIODIC,  # Use periodic boundaries
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )

        velocity_0 = CenteredGrid(
            temp,
            extrapolation.PERIODIC,  # Use periodic boundaries
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        return {"velocity": velocity_0}

    def forward(self, current_state: Dict[str, Field]) -> Dict[str, Field]:
        """
        Performs a single simulation step.
        """
        new_velocity = _burgers_physics_step(
            velocity=current_state["velocity"], dt=self.dt, nu=self.nu
        )
        return {"velocity": new_velocity}


--- src/models/physical/smoke.py ---

# src/models/physical/smoke_model.py

from typing import Dict, Any
import numpy as np
import random

# --- PhiFlow Imports ---
from phi.torch.flow import *
from phi.math import Shape, Tensor, batch, math

# --- Repo Imports ---
from .base import PhysicalModel
from src.models.registry import ModelRegistry


# --- JIT-Compiled Physics Function ---
@jit_compile
def _smoke_physics_step(velocity: CenteredGrid, density: CenteredGrid, inflow: CenteredGrid, domain: Box, dt: float, buoyancy_factor: float, nu: float) -> tuple[Field, Field]:
    """
    Performs one physics-based smoke simulation step.

    Args:
        velocity (CenteredGrid): Current velocity field.
        density (CenteredGrid): Current density field.
        inflow (CenteredGrid): Inflow mask.
        domain (Box): Simulation domain.
        dt (float): Time step.
        buoyancy_factor (float): Strength of buoyancy.
        nu (float): Viscosity.

    Returns:
        tuple: (new_velocity, new_density)
    """
    # Advect density and add inflow
    density = advect.mac_cormack(density, velocity, dt=dt) + dt * inflow

    # Apply forces
    buoyancy_force = (density * (0, buoyancy_factor)).at(velocity)
    velocity = velocity + dt * buoyancy_force

    # Advect velocity
    velocity = advect.semi_lagrangian(velocity, velocity, dt=dt)

    velocity = diffuse.explicit(velocity, nu, dt=dt)

    # Make incompressible
    velocity, pressure = fluid.make_incompressible(
        velocity,
        solve=Solve("CG", 1e-3, rank_deficiency=0, suppress=[phi.math.NotConverged]),
    )

    return velocity, density


# --- Model Class Implementation ---


@ModelRegistry.register_physical("SmokeModel")
class SmokeModel(PhysicalModel):
    """
    Physical model for the smoke simulation.
    Implements the PhysicalModel interface.
    """

    # Declare PDE-specific parameters
    PDE_PARAMETERS = {
        "nu": {
            "type": float,
            "default": 0.0,
        },
        "buoyancy": {
            "type": float,
            "default": 1.0,
        },
        "inflow_radius": {
            "type": float,
            "default": 10.0,
        },
        "inflow_rate": {
            "type": float,
            "default": 0.1,
        },
        "inflow_rand_x_range": {
            "type": list,
            "default": [0.2, 0.8],
        },
        "inflow_rand_y_range": {
            "type": list,
            "default": [0.15, 0.25],
        },
    }

    def __init__(self, config: dict):
        """
        Initializes the smoke model.

        Handles special inflow center logic after base initialization.
        """
        # Call parent init to handle standard parameters
        super().__init__(config)

    def get_initial_state(self, batch_size: int = 1) -> Dict[str, Field]:
        """
        Returns an initial state of (zero velocity, zero density).
        """
        # Generate random inflow position within specified ranges
        inflow_center = self._get_inflow_center()

        b = batch(batch=batch_size)

        velocity_0 = CenteredGrid(
            (0, 0),
            extrapolation.ZERO,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        velocity_0 = math.expand(velocity_0, b)

        density_0 = CenteredGrid(
            0,
            extrapolation.BOUNDARY,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        density_0 = math.expand(density_0, b)

        inflow_shape = Sphere(center=inflow_center, radius=self.inflow_radius)
        inflow_0 = self.inflow_rate * CenteredGrid(
            inflow_shape,
            extrapolation.BOUNDARY,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )
        inflow_0 = math.expand(inflow_0, b)

        return {"velocity": velocity_0, "density": density_0, "inflow": inflow_0}

    def get_random_state(self) -> Dict[str, Field]:
        """
        Returns an initial state of (zero velocity, zero density).
        """
        # Generate random inflow position within specified ranges
        inflow_center = self._get_inflow_center()

        velocity_0 = CenteredGrid(
            (0, 0),
            extrapolation.ZERO,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )


        density_0 = CenteredGrid(
            0,
            extrapolation.BOUNDARY,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )

        inflow_shape = Sphere(center=inflow_center, radius=self.inflow_radius)
        inflow_0 = self.inflow_rate * CenteredGrid(
            inflow_shape,
            extrapolation.BOUNDARY,
            x=self.resolution.get_size("x"),
            y=self.resolution.get_size("y"),
            bounds=self.domain,
        )

        return {"velocity": velocity_0, "density": density_0, "inflow": inflow_0}

    def forward(self, current_state: Dict[str, Field]) -> Dict[str, Field]:
        """
        Performs a single simulation step.
        """
        # This is unchanged and will now use the internally created self.inflow
        new_velocity, new_density = _smoke_physics_step(
            velocity=current_state["velocity"],
            density=current_state["density"],
            inflow=current_state["inflow"],
            domain=self.domain,
            dt=self.dt,
            buoyancy_factor=self.buoyancy,
            nu=self.nu,
        )
        return {
            "velocity": new_velocity,
            "density": new_density,
            "inflow": current_state["inflow"],
        }
    
    def _get_inflow_center(self) -> Tensor:
        """
        Computes a random inflow center within the specified ranges.
        """
        rand_x = self.domain.size[0] * (
            self._inflow_rand_x_range[0] + self._inflow_rand_x_range[1] * random.random()
        )
        rand_y = self.domain.size[1] * (
            self._inflow_rand_y_range[0] + self._inflow_rand_y_range[1] * random.random()
        )
        inflow_center = (rand_x, rand_y)
        return math.tensor(inflow_center, channel(vector="x,y"))


--- src/models/registry.py ---

"""
Model Registry for Automatic Model Discovery

This module provides a registry pattern for dynamically registering and
instantiating models. This eliminates hard-coded model instantiation and
makes it easy to add new models without modifying existing code.

Features:
- Automatic model discovery via decorators
- Separate registries for physical and synthetic models
- Clear error messages for missing models
- Easy listing of available models

Usage:
    # Register a model
    @ModelRegistry.register_physical('BurgersModel')
    class BurgersModel(PhysicalModel):
        pass
    
    # Get a model instance
    model = ModelRegistry.get_physical_model('BurgersModel', config)
    
    # List available models
    available = ModelRegistry.list_physical_models()
"""

from typing import Dict, Type, Any, Callable, List
from src.utils.logger import get_logger

logger = get_logger(__name__)


class ModelRegistry:
    """
    Registry for automatic model discovery and instantiation.

    This class maintains separate registries for physical and synthetic models,
    allowing dynamic model creation based on configuration without hard-coding
    model names throughout the codebase.

    Attributes:
        _physical_models: Dictionary mapping model names to physical model classes
        _synthetic_models: Dictionary mapping model names to synthetic model classes
    """

    _physical_models: Dict[str, Type] = {}
    _synthetic_models: Dict[str, Type] = {}

    @classmethod
    def register_physical(cls, name: str) -> Callable:
        """
        Decorator to register a physical model.

        Args:
            name: Name to register the model under (e.g., 'BurgersModel')

        Returns:
            Decorator function that registers the model class

        Example:
            @ModelRegistry.register_physical('BurgersModel')
            class BurgersModel(PhysicalModel):
                pass
        """

        def decorator(model_class: Type) -> Type:
            if name in cls._physical_models:
                logger.warning(f"Overwriting physical model '{name}'")
            cls._physical_models[name] = model_class
            logger.debug(f"Registered physical model: {name}")
            return model_class

        return decorator

    @classmethod
    def register_synthetic(cls, name: str) -> Callable:
        """
        Decorator to register a synthetic model.

        Args:
            name: Name to register the model under (e.g., 'UNet')

        Returns:
            Decorator function that registers the model class

        Example:
            @ModelRegistry.register_synthetic('UNet')
            class UNet(SyntheticModel):
                pass
        """

        def decorator(model_class: Type) -> Type:
            if name in cls._synthetic_models:
                logger.warning(f"Overwriting synthetic model '{name}'")
            cls._synthetic_models[name] = model_class
            logger.debug(f"Registered synthetic model: {name}")
            return model_class

        return decorator

    @classmethod
    def get_physical_model(cls, name: str, config: Dict[str, Any]):
        """
        Get an instance of a physical model.

        Args:
            name: Name of the model to instantiate
            config: Configuration dictionary for the model

        Returns:
            Instance of the requested physical model

        Raises:
            ValueError: If the model name is not registered

        Example:
            config = {'domain': {...}, 'resolution': {...}}
            model = ModelRegistry.get_physical_model('BurgersModel', config)
        """
        if name not in cls._physical_models:
            available = ", ".join(cls._physical_models.keys()) or "none"
            raise ValueError(
                f"Physical model '{name}' not found in registry. "
                f"Available models: {available}"
            )

        model_class = cls._physical_models[name]
        logger.debug(f"Creating physical model: {name}")
        return model_class(config)

    @classmethod
    def get_synthetic_model(cls, name: str, config: Dict[str, Any]):
        """
        Get an instance of a synthetic model.

        Args:
            name: Name of the model to instantiate
            config: Configuration dictionary for the model

        Returns:
            Instance of the requested synthetic model

        Raises:
            ValueError: If the model name is not registered

        Example:
            config = {'input_specs': {...}, 'output_specs': {...}}
            model = ModelRegistry.get_synthetic_model('UNet', config)
        """
        if name not in cls._synthetic_models:
            available = ", ".join(cls._synthetic_models.keys()) or "none"
            raise ValueError(
                f"Synthetic model '{name}' not found in registry. "
                f"Available models: {available}"
            )

        model_class = cls._synthetic_models[name]
        logger.debug(f"Creating synthetic model: {name}")
        return model_class(config)

    @classmethod
    def list_physical_models(cls) -> List[str]:
        """
        List all registered physical models.

        Returns:
            List of physical model names

        Example:
            >>> ModelRegistry.list_physical_models()
            ['BurgersModel', 'SmokeModel', 'HeatModel']
        """
        return sorted(cls._physical_models.keys())

    @classmethod
    def list_synthetic_models(cls) -> List[str]:
        """
        List all registered synthetic models.

        Returns:
            List of synthetic model names

        Example:
            >>> ModelRegistry.list_synthetic_models()
            ['UNet', 'ResNet', 'FNO']
        """
        return sorted(cls._synthetic_models.keys())

    @classmethod
    def is_physical_model_registered(cls, name: str) -> bool:
        """
        Check if a physical model is registered.

        Args:
            name: Model name to check

        Returns:
            True if model is registered, False otherwise
        """
        return name in cls._physical_models

    @classmethod
    def is_synthetic_model_registered(cls, name: str) -> bool:
        """
        Check if a synthetic model is registered.

        Args:
            name: Model name to check

        Returns:
            True if model is registered, False otherwise
        """
        return name in cls._synthetic_models

    @classmethod
    def clear_registry(cls):
        """
        Clear all registered models.

        This is mainly useful for testing purposes.
        """
        cls._physical_models.clear()
        cls._synthetic_models.clear()
        logger.debug("Cleared model registry")


--- src/models/synthetic/__init__.py ---

# src/models/synthetic/__init__.py

# This file makes your model classes importable

# Import the base class so it can be accessed if needed
from .base import SyntheticModel

# Import your concrete model classes
from .unet import UNet
from .resnet import ResNet
from .convnet import ConvNet


--- src/models/synthetic/base.py ---

# In src/models/synthetic/base.py

from abc import ABC
from typing import Dict, Any, List

import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import logging


class SyntheticModel(nn.Module, ABC):
    """
    Abstract base class for all synthetic models (neural networks).

    This class handles the boilerplate logic for:
    1.  Pre-processing: Converting a state dict of Phiflow Fields (including
        StaggeredGrids) into a single, multi-channel CenteredGrid tensor.
    2.  Post-processing: Converting the network's output CenteredGrid back
        into a state dict of individual Fields, restoring original
        StaggeredGrid types where appropriate.

    Subclasses are only required to implement the `_predict` method.
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the synthetic model.

        Args:
            config: A dictionary containing model-specific configurations.
                    Expected to contain 'input_specs' and 'output_specs'
                    dictionaries, e.g., {'density': 1, 'velocity': 2}.
        """
        super().__init__()
        self.config = config
        self.logger = logging.getLogger(__name__)

        self.input_specs = {
        field: config['physical']['fields_scheme'].lower().count(field[0].lower())
        for field in config['physical']['fields']
        if field
        }
        self.output_specs = {
        field: config['physical']['fields_scheme'].lower().count(field[0].lower())
        for i, field in enumerate(config['physical']['fields'])
        if field and config['physical']['fields_type'][i].upper() == 'D'
        }
        # Calculate channel counts for padding
        self.num_dynamic_channels = sum(self.output_specs.values())
        self.num_static_channels = sum(self.input_specs.values()) - self.num_dynamic_channels
        self._dynamic_slice = slice(0, self.num_dynamic_channels)
        


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the synthetic model using an additive residual.

        This implementation assumes that dynamic fields are ordered first in the
        channel dimension, followed by static fields.

        It predicts the residual for dynamic fields, pads it with zeros for the
        static fields, and adds the result to the input tensor `x`.

        Args:
            x: Input tensor. Expected shape:
               - BVTS: [B, V, T, H, W] where V==channels per-frame and T is time.

        Returns:
            Output tensor in BVTS layout [B, V, T, H, W] representing the
            next state.
        """
        # Guaranteed BVTS: [B, V, T, H, W]
        B, V, T, H, W = x.shape

        # Move time into batch: [B*T, V, H, W]
        reshaped_in = x.permute(0, 2, 1, 3, 4).reshape(B * T, V, H, W)

        # Run through network which expects [batch, channels, H, W]
        dynamic_out = self.net(reshaped_in)

        # dynamic_out: [B*T, num_dynamic_channels, H, W]
        out_frames = torch.empty_like(reshaped_in)
        out_frames[:, self._dynamic_slice] = dynamic_out
        out_frames[:, self.num_dynamic_channels:] = reshaped_in[:, self.num_dynamic_channels:]

        # Reshape back to BVTS: [B, V, T, H, W]
        out = out_frames.reshape(B, T, V, H, W).permute(0, 2, 1, 3, 4)
        return out


    # In src/models/synthetic/base.py

    @torch.no_grad()
    def generate_predictions(
        self,
        trajectories: List[Dict[str, torch.Tensor]],
        device: str = "cuda",
        batch_size: int = 1,
    ):
        """
        Generate one-step predictions from physical model trajectories.
        
        NEW BEHAVIOR:
        - Takes physical trajectories directly (not through dataset)
        - For each trajectory, generates one-step predictions autoregressively
        - Returns as trajectory format: [initial_real, pred_from_real, pred_from_pred, ...]
        - This allows the same windowing logic via indexing
        
        Args:
            trajectories: List of trajectory dicts in cache format
                        Format: [{'tensor_data': {field_name: tensor[C, T, H, W]}}]
            device: Device to run predictions on
            batch_size: Batch size for inference (currently unused, could batch multiple trajectories)
            
        Returns:
            List of prediction trajectories in BVTS format [1, V, T, H, W]
        """
        self.eval()
        self.to(device)
        
        prediction_trajectories = []
        
        for tensor_data in trajectories:
            # Concatenate all fields along channel dimension
            # Each field: [C, T, H, W]
            field_tensors = [tensor_data[field_name] for field_name in sorted(tensor_data.keys())]
            full_trajectory = torch.cat(field_tensors, dim=0)  # [C_all, T, H, W]
            
            # Move to device
            full_trajectory = full_trajectory.to(device, non_blocking=True)
            
            # Get trajectory length
            num_steps = full_trajectory.shape[1] 

            trajectory_frames = [full_trajectory[:, 0:1, :, :]] 
            
            with torch.amp.autocast(enabled=True, device_type=device):
                # Generate predictions for remaining timesteps
                for t in range(num_steps-1):
                    # One-step prediction
                    next_state = self(full_trajectory[:, t:t+1, :, :].unsqueeze(0)).squeeze(0)
                    # Store prediction (squeeze time dim)
                    trajectory_frames.append(next_state)  
            
            trajectory_tensor = torch.cat(trajectory_frames, dim=1)
            # Store as CPU tensor
            prediction_trajectories.append(trajectory_tensor.cpu())
        
        
        return prediction_trajectories

    @staticmethod
    def _select_proportional_indices(total_count: int, sample_count: int):
        """
        Select indices proportionally across the dataset.

        Ensures diverse sampling rather than just taking the first N samples.
        """
        if sample_count >= total_count:
            return list(range(total_count))

        # Calculate step size for proportional sampling
        step = total_count / sample_count

        # Select indices evenly distributed
        indices = [int(i * step) for i in range(sample_count)]

        # Ensure no duplicates and within bounds
        indices = sorted(list(set(indices)))[:sample_count]

        return indices


--- src/models/synthetic/convnet.py ---

# src/models/synthetic/convnet.py

from typing import Dict, Any
import torch
import torch.nn as nn
from phiml.nn import conv_net
from src.models.registry import ModelRegistry
from src.models.synthetic.base import SyntheticModel


@ModelRegistry.register_synthetic("ConvNet")
class ConvNet(SyntheticModel):
    """
    Tensor-based ConvNet for efficient training.

    Works directly with PyTorch tensors in [batch, channels, height, width] format.
    All Field conversions are handled by DataManager before training.

    Handles static vs dynamic fields:
    - Input contains all fields (static + dynamic)
    - Model predicts only dynamic fields
    - Static fields are automatically preserved and re-attached to output
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the ConvNet model.

        Args:
            config: Model configuration containing:
                - input_specs: Dict[field_name, num_channels] - all input fields
                - output_specs: Dict[field_name, num_channels] - fields to predict
                - architecture: Dict with levels, filters, batch_norm
        """
        # Call parent constructor to set up base attributes
        super().__init__(config)

        # Get architecture params (with defaults for backwards compatibility)
        layers = config["synthetic"]['architecture']['layers']

        # Build the ConvNet using PhiML's conv_net
        self.net = conv_net(
            in_channels=sum(self.input_specs.values()),
            out_channels=sum(self.output_specs.values()),
            layers=layers,
            batch_norm=True,
        )


--- src/models/synthetic/resnet.py ---

# src/models/synthetic/resnet.py

from typing import Dict, Any
import torch
import torch.nn as nn
from phiml.nn import res_net
from src.models.registry import ModelRegistry
from src.models.synthetic.base import SyntheticModel


@ModelRegistry.register_synthetic("ResNet")
class ResNet(SyntheticModel):
    """
    Tensor-based ResNet for efficient training.

    Works directly with PyTorch tensors in [batch, channels, height, width] format.
    All Field conversions are handled by DataManager before training.

    Handles static vs dynamic fields:
    - Input contains all fields (static + dynamic)
    - Model predicts only dynamic fields
    - Static fields are automatically preserved and re-attached to output
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the ResNet model.

        Args:
            config: Model configuration containing:
                - input_specs: Dict[field_name, num_channels] - all input fields
                - output_specs: Dict[field_name, num_channels] - fields to predict
                - architecture: Dict with levels, filters, batch_norm
        """
        # Call parent constructor to set up base attributes
        super().__init__(config)

        # Get architecture params
        layers = config["synthetic"]['architecture']["layers"]

        # Build the ResNet using PhiML's res_net
        self.net = res_net(
            in_channels=sum(self.input_specs.values()),
            out_channels=sum(self.output_specs.values()),
            layers=layers,
            batch_norm=True,
        )

    

--- src/models/synthetic/unet.py ---

# src/models/synthetic/unet.py

from typing import Dict, Any
import torch
import torch.nn as nn
from phiml.nn import u_net
from src.models.registry import ModelRegistry
from src.models.synthetic.base import SyntheticModel


@ModelRegistry.register_synthetic("UNet")
class UNet(SyntheticModel):
    """
    Tensor-based U-Net for efficient training.

    Works directly with PyTorch tensors in [batch, channels, height, width] format.
    All Field conversions are handled by DataManager before training.

    Handles static vs dynamic fields:
    - Input contains all fields (static + dynamic)
    - Model predicts only dynamic fields
    - Static fields are automatically preserved and re-attached to output
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the U-Net model.

        Args:
            config: Model configuration containing:
                - input_specs: Dict[field_name, num_channels] - all input fields
                - output_specs: Dict[field_name, num_channels] - fields to predict
                - architecture: Dict with levels, filters, batch_norm
        """
        # Call parent constructor to set up base attributes
        super().__init__(config)

        # Get architecture params (with defaults for backwards compatibility)
        levels = config["synthetic"]['architecture']["levels"]
        filters = config["synthetic"]['architecture']["filters"]

        # Build the U-Net using PhiML's u_net
        self.net = u_net(
            in_channels=sum(self.input_specs.values()),
            out_channels=sum(self.output_specs.values()),
            levels=levels,
            filters=filters,
            batch_norm=True,
        )


--- src/tests/migration/test_bvts_io.py ---

import torch
from src.utils.field_conversion.bvts import to_bvts, from_bvts, assert_bvts


def test_to_from_bvts_roundtrip_4d():
    # time-major [T, C, H, W]
    t = torch.randn(10, 3, 16, 16)
    bv = to_bvts(t)
    assert bv.shape == (1, 3, 10, 16, 16)
    # roundtrip
    back = from_bvts(bv)
    assert back.shape == t.shape


def test_to_bvts_single_frame_channel():
    t = torch.randn(3, 16, 16)
    bv = to_bvts(t)
    assert bv.shape == (1, 3, 1, 16, 16)


def test_assert_bvts_accepts_5d():
    bv = torch.randn(2, 3, 10, 8, 8)
    assert_bvts(bv)


def test_from_bvts_requires_batch1():
    bv = torch.randn(2, 3, 10, 8, 8)
    try:
        _ = from_bvts(bv)
        assert False, "Expected ValueError for batch>1"
    except ValueError:
        pass


--- src/tests/test_dataset_builder.py ---

import torch
from src.data.dataset_utilities import DatasetBuilder


class FakeDataManager:
    def __init__(self, tensor):
        self._tensor = tensor

    def get_or_load_simulation(self, sim_index, field_names=None, num_frames=None):
        return {"tensor_data": {field_names[0]: self._tensor}}
    
    def is_cached(self, sim_index: int) -> bool:
        """Minimal cached check for tests: pretend the simulation is already cached.

        DatasetBuilder only queries this to decide whether to call
        get_or_load_simulation for caching. Returning True keeps the test focused
        on frame detection logic.
        """
        return True


def test_setup_cache_detects_bvts_layout():
    # BVTS layout: [B, C, T, H, W]
    B, C, T, H, W = 1, 3, 10, 8, 8
    tensor = torch.zeros((B, C, T, H, W))
    dm = FakeDataManager(tensor)
    builder = DatasetBuilder(dm)

    num_frames = builder.setup_cache([0], ["field"], num_frames=None, num_predict_steps=2)
    assert num_frames == T
 


--- src/tests/test_field_dataset.py ---

import torch

import src.utils.field_conversion as fc
import src.data.field_dataset as fd
from src.data.field_dataset import FieldDataset


class DummyConverter:
    def tensor_to_field(self, tensor_t, field_meta=None, time_slice=0):
        # Return a simple marker containing the incoming tensor shape
        return {"shape": tuple(tensor_t.shape)}


def make_fake_field_dataset(field_tensor: torch.Tensor, total_frames: int):
    # Create an uninitialized FieldDataset instance and set minimal attributes
    ds = FieldDataset.__new__(FieldDataset)
    ds.field_names = ["field"]
    ds.num_frames = total_frames
    return ds


def test_tensors_to_fields_bvts_only():
    # Prepare BVTS input: [B, C, T, H, W]
    B, C, T, H, W = 1, 3, 5, 8, 8
    bvts = torch.zeros((B, C, T, H, W))

    # Patch make_converter in the field_dataset module to return our DummyConverter
    old_make_converter = fd.make_converter
    fd.make_converter = lambda meta: DummyConverter()

    try:
    # BVTS case
        ds_bvts = make_fake_field_dataset(bvts, T)
        data = {"tensor_data": {"field": bvts}}
        fields_bvts = FieldDataset._tensors_to_fields(ds_bvts, data, {"field": None}, 1, 3)
        assert "field" in fields_bvts
        assert len(fields_bvts["field"]) == 2  # frames 1..2
        assert all(isinstance(x, dict) and "shape" in x for x in fields_bvts["field"])

    finally:
        # Restore
        fd.make_converter = old_make_converter


--- src/tests/test_tensor_dataset.py ---

import torch
from src.data.tensor_dataset import TensorDataset


class FakeDataManager:
    def __init__(self, tensor_map):
        self._tensor_map = tensor_map

    def get_or_load_simulation(self, sim_index, field_names=None, num_frames=None):
        return {"tensor_data": {name: self._tensor_map[name] for name in field_names}}

    def is_cached(self, sim_index: int) -> bool:
        return True


def test_tensor_dataset_shapes():
    # Build BVTS tensors for two fields
    B, C1, C2, T, H, W = 1, 1, 2, 6, 8, 8

    f1 = torch.zeros((B, C1, T, H, W))
    f2 = torch.zeros((B, C2, T, H, W))

    dm = FakeDataManager({"f1": f1, "f2": f2})

    ds = TensorDataset(data_manager=dm, sim_indices=[0], field_names=["f1", "f2"], num_frames=None, num_predict_steps=2, augmentation_config=None)

    initial_state, rollout_targets = ds[0]

    # After concatenation C_all = C1 + C2
    C_all = C1 + C2

    assert isinstance(initial_state, torch.Tensor)
    assert isinstance(rollout_targets, torch.Tensor)

    # initial_state expected shape: [C_all, 1, H, W]
    assert initial_state.shape == (C_all, 1, H, W)

    # rollout_targets expected shape: [C_all, num_predict_steps, H, W]
    assert rollout_targets.shape == (C_all, ds.num_predict_steps, H, W)


--- src/training/__init__.py ---

"""
Training Module

This module provides the trainer hierarchy for HYCO-PhiFlow:

- AbstractTrainer: Minimal common interface for all trainers
- TensorTrainer: Base class for PyTorch tensor-based trainers
- FieldTrainer: Base class for PhiFlow field-based trainers

Concrete implementations:
- SyntheticTrainer: Trains synthetic (data-driven) models using tensors
- PhysicalTrainer: Trains physical models using field optimization
- HybridTrainer: Alternates between synthetic and physical training with augmentation
"""

from src.training.abstract_trainer import AbstractTrainer
from src.training.tensor_trainer import TensorTrainer
from src.training.field_trainer import FieldTrainer
from src.training.hybrid import HybridTrainer

__all__ = [
    "AbstractTrainer",
    "TensorTrainer",
    "FieldTrainer",
    "HybridTrainer",
]


--- src/training/abstract_trainer.py ---

"""
Abstract Trainer

This module provides a minimal abstract base class that all trainers must implement.
This is the foundation of the new trainer hierarchy that separates concerns between
tensor-based (PyTorch) and field-based (PhiFlow) trainers.

Key Principle:
- Only includes functionality that ALL trainers need
- No PyTorch-specific code (that goes in TensorTrainer)
- No PhiFlow-specific code (that goes in FieldTrainer)
"""

from abc import ABC, abstractmethod
from typing import Dict, Any


class AbstractTrainer(ABC):
    """
    Minimal interface that all trainers must implement.

    This abstract class defines only the essential contract that every trainer
    must fulfill, regardless of whether it's tensor-based, field-based, or hybrid.

    Design Philosophy:
    - Keep it minimal - only what's truly common
    - No assumptions about model type (PyTorch vs PhiFlow)
    - No assumptions about training paradigm (epochs vs optimization runs)
    - Provide clean extension points for different trainer types

    Attributes:
        config: Full configuration dictionary containing all settings
        project_root: Root directory of the project

    Subclasses:
        TensorTrainer: For PyTorch tensor-based models
        FieldTrainer: For PhiFlow field-based models
        HybridTrainer: For combined training strategies
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize abstract trainer.

        Args:
            config: Full configuration dictionary containing all settings.
                   This should include data, model, and trainer parameters.
        """
        self._config = config
        self._project_root = config.get("project_root", ".")

    @abstractmethod
    def train(self) -> Dict[str, Any]:
        """
        Execute training and return results.

        This is the main entry point for training. Each trainer type implements
        its own training logic appropriate to its paradigm:
        - TensorTrainer: Epoch-based gradient descent
        - FieldTrainer: Optimization-based parameter inference
        - HybridTrainer: Coordinated training of multiple models

        Returns:
            Dictionary containing training results, metrics, and any other
            relevant information. The exact contents depend on the trainer type,
            but should typically include:
            - 'loss': Final or average loss value(s)
            - 'epochs' or 'iterations': Number of training steps completed
            - Any trainer-specific metrics
        """
        pass

    @property
    def config(self) -> Dict[str, Any]:
        """
        Return configuration used for this trainer.

        Useful for logging, debugging, and ensuring reproducibility.

        Returns:
            Configuration dictionary
        """
        return self._config
    
    @config.setter
    def config(self, new_config: Dict[str, Any]):
        """
        Set a new configuration for the trainer.

        Args:
            new_config: New configuration dictionary to replace the existing one.
        """
        self._config = new_config

    @property
    def project_root(self) -> str:
        """
        Get the project root directory.

        Returns:
            Project root path as string
        """
        return self._project_root
    
    @project_root.setter
    def project_root(self, new_root: str):
        """
        Set a new project root directory.

        Args:
            new_root: New project root path as string
        """
        self._project_root = new_root


--- src/training/field_trainer.py ---

"""
Field Trainer

This module provides the base class for PhiFlow field-based trainers.
All PhiFlow-specific functionality lives here, including:
- Field-based data management
- Optimization-based parameter inference
- Physical model simulation
- Result saving and visualization

This separates field-based concerns from tensor-based concerns,
providing a cleaner architecture for physical model training.
"""

from abc import abstractmethod
from typing import Dict, Any, List, Optional
from pathlib import Path
import torch
from torch.utils.data import DataLoader
from phi.field import Field
from phi.math import Solve, Tensor, minimize
from tqdm import tqdm
import time
import os
from phi.torch.flow import *
from src.training.abstract_trainer import AbstractTrainer
from src.models.physical.base import PhysicalModel
from src.utils.logger import get_logger

logger = get_logger(__name__)


class FieldTrainer(AbstractTrainer):
    """
    Base class for PhiFlow field-based trainers.

    NEW ARCHITECTURE (Phase 1):
    - Model and learnable_params are passed in __init__, not created internally
    - Data is passed to train() method, not held internally
    - Trainers are persistent across training calls
    - Optimizer state is preserved

    Provides all PhiFlow-specific functionality:
    - Field-based optimization
    - Physical model simulation and evaluation
    - Result saving in appropriate formats

    Unlike TensorTrainer which uses epoch-based training, FieldTrainer
    uses sample-by-sample iteration (since field operations don't batch well).

    Subclasses should implement:
    - _train_sample(): Train on a single sample

    The train() method accepts data explicitly and should not be overridden
    in most cases.

    Attributes:
        config: Full configuration dictionary
        model: PhysicalModel instance (passed in __init__)
        learnable_params: List of parameters to optimize (passed in __init__)
        optimizer: PhiML/PyTorch optimizer
    """

    def __init__(
        self,
        config: Dict[str, Any],
        model: Any,  # PhysicalModel instance
        learnable_params: List[Dict[str, Tensor]],
    ):
        """
        Initialize field trainer with model and learnable parameters.

        Args:
            config: Full configuration dictionary containing all settings
            model: Pre-created PhysicalModel instance
            learnable_params: List of parameters to optimize
        """
        super().__init__(config)

        # --- Derive all parameters from config ---
        self.data_config = config["data"]
        self.model_config = config["model"]["physical"]
        self.trainer_config = config["trainer_params"]

        # --- Data specifications ---
        self.field_names: List[str] = self.data_config["fields"]
        self.dset_name = self.data_config["dset_name"]
        self.data_dir = self.data_config["data_dir"]

        # # --- Checkpoint path ---
        # model_save_name = self.model_config["model_save_name"]
        # model_path_dir = self.model_config["model_path"]
        # self.checkpoint_path = Path(model_path_dir) / f"{model_save_name}.pth"
        # os.makedirs(model_path_dir, exist_ok=True)

        # -- Store model and parameters ---
        self.model = model
        self.learnable_params = [p['initial_guess'] for p in learnable_params]
        self.param_names = [p["name"] for p in learnable_params]

        # Create optimizer
        self.optimizer = self._create_optimizer()
 
        # Results storage
        self.final_loss: float = 0.0
        self.training_history: List[float] = []

        self.best_val_loss = float("inf")
        self.best_epoch = 0

        # --- Get parameters ---
        self.num_predict_steps= self.trainer_config["num_predict_steps"]

    def _create_optimizer(self):
        """
        Create optimizer for learnable parameters.

        Returns:
            PhiFlow optimizer instance
        """
        method = self.trainer_config['method']
        abs_tol = self.trainer_config['abs_tol']
        max_iterations = self.trainer_config['max_iterations']

        optimizer = math.Solve(
            method=method,
            abs_tol=abs_tol,
            x0=self.learnable_params,
            max_iterations=max_iterations,
            suppress=(math.NotConverged,),
        )
        return optimizer

    @abstractmethod
    def _train_epoch(
        self, data_source: DataLoader
    ) -> float:
        """
        Train on a single sample.

        This method should:
        1. Run simulation from initial_fields
        2. Compute loss against target_fields
        3. Perform backward pass and optimization step
        4. Return loss value

        Args:
            initial_fields: Dict[field_name, Field] for initial state
            target_fields: Dict[field_name, List[Field]] for target trajectory

        Returns:
            Loss value for this sample
        """
        pass

    def train(self, data_source: DataLoader, num_epochs: int, verbose: bool = True) -> Dict[str, Any]:
        """
        Execute training for specified number of epochs with provided data.

        NEW SIGNATURE: Data is passed explicitly, not held internally.

        Args:
            data_source: Iterable yielding (initial_fields, target_fields) tuples
                        Note: NO weights - all samples treated equally!
            num_epochs: Number of epochs to train

        Returns:
            Dictionary with training results including losses and metrics
        """

        results = {
            "train_losses": [],
            "epochs": [],
            "num_epochs": num_epochs,
            "best_epoch": 0,
            "best_val_loss": float("inf"),
        }

        disable_tqdm = not verbose
        pbar = tqdm(
            range(num_epochs), desc="Training", unit="epoch", disable=disable_tqdm
        )

        for epoch in pbar:
            start_time = time.time()
            train_loss = self._train_epoch(data_source)

            results["train_losses"].append(train_loss)
            results["epochs"].append(epoch + 1)

            if train_loss < self.best_val_loss:
                self.best_val_loss = train_loss
                self.best_epoch = epoch + 1
                results["best_epoch"] = self.best_epoch
                results["best_val_loss"] = self.best_val_loss

                # self.save_checkpoint(
                #     epoch=epoch,
                #     loss=train_loss,
                #     optimizer_state=(
                #         self.optimizer.state_dict() if self.optimizer else None
                #     )
                # )

            epoch_time = time.time() - start_time

            # Update progress bar
            postfix_dict = {
                "train_loss": f"{train_loss:.6f}",
                "time": f"{epoch_time:.2f}s",
            }

            postfix_dict["best_epoch"] = self.best_epoch

            pbar.set_postfix(postfix_dict)

        final_loss = results["train_losses"][-1]
        results["final_loss"] = final_loss

        return results

    # =========================================================================
    # PhiFlow-Specific Utilities (kept for backward compatibility)
    # =========================================================================

    def update_optimizer_params(self, new_params: List[Tensor]):
        """
        Update optimizer with new parameter values.

        This is useful for continuing optimization with updated initial guesses.

        Args:
            new_params: New parameter values
        """
        self.learnable_params = new_params
        self.optimizer.x0 = new_params

    def get_current_params(self) -> Dict[str, float]:
        """
        Get current parameter values.

        Returns:
            Dictionary mapping parameter names to values
        """
        return {
            name: self._tensor_to_float(param)
            for name, param in zip(self.param_names, self.learnable_params)
        }
    
    @staticmethod
    def _tensor_to_float(tensor: Tensor) -> float:
        """
        Extract Python float from PhiFlow tensor.

        Args:
            tensor: PhiFlow Tensor

        Returns:
            Python float value
        """
        return float(tensor)


--- src/training/hybrid/__init__.py ---

"""
Hybrid Trainer for HYCO

This module implements the hybrid training approach that alternates between
synthetic and physical model training with cross-model data augmentation.
"""

from .trainer import HybridTrainer

__all__ = ["HybridTrainer"]


--- src/training/hybrid/trainer.py ---

"""
REFACTORED HybridTrainer - Clean API with new dataset methods

Key Changes:
- Uses set_augmented_trajectories() for TensorDataset (physical → synthetic training)
- Uses set_augmented_predictions() for FieldDataset (synthetic → physical training)
- Clear distinction between trajectory-based and prediction-based augmentation
- Datasets maintain source tracking internally
"""

import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, Any, List, Tuple
from tqdm import tqdm
import gc
from contextlib import contextmanager

from src.training.abstract_trainer import AbstractTrainer
from src.training.synthetic.trainer import SyntheticTrainer
from src.training.physical.trainer import PhysicalTrainer
from src.data import TensorDataset, FieldDataset
from src.utils.logger import get_logger, logging
from src.config import ConfigHelper
from phi.math import Tensor
from phi.flow import Field
from torch.utils.data import DataLoader
from src.factories.dataloader_factory import DataLoaderFactory

logger = get_logger(__name__)


class HybridTrainer(AbstractTrainer):
    """
    Hybrid trainer with clean separation of concerns.
    
    Training Flow:
    1. Physical model generates trajectories (Fields) → TensorDataset augmentation
    2. Synthetic model trains on real + physical trajectories
    3. Synthetic model generates predictions (tensors) → FieldDataset augmentation
    4. Physical model trains on real + synthetic predictions
    """

    def __init__(
        self,
        config: Dict[str, Any],
        synthetic_model: nn.Module,
        physical_model,
        learnable_params: Dict[str, Tensor],
    ):
        """Initialize hybrid trainer with both models."""
        super().__init__(config)

        # Store models
        self.synthetic_model = synthetic_model
        self.physical_model = physical_model
        self.learnable_params = learnable_params

        # Parse configuration
        self._parse_config()

        # Setup device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Pre-create base datasets
        self._base_tensor_dataset = None
        self._base_field_dataset = None
        self._initialize_datasets()

        # Create component trainers
        self.synthetic_trainer = SyntheticTrainer(config, synthetic_model)
        self.physical_trainer = PhysicalTrainer(config, physical_model, learnable_params)

        # Training state
        self.current_cycle = 0
        self.best_synthetic_loss = float("inf")
        self.best_physical_loss = float("inf")

    def _parse_config(self):
        """Extract all configuration parameters."""
        self.trainer_config = self.config["trainer_params"]
        self.hybrid_config = self.trainer_config["hybrid"]
        self.aug_config = self.trainer_config["augmentation"]
        self.generation_config = self.config["generation_params"]
        
        self.num_cycles = self.hybrid_config["num_cycles"]
        self.synthetic_epochs_per_cycle = self.hybrid_config["synthetic_epochs_per_cycle"]
        self.physical_epochs_per_cycle = self.hybrid_config["physical_epochs_per_cycle"]
        self.warmup_synthetic_epochs = self.hybrid_config["warmup_synthetic_epochs"]
        
        self.alpha = self.aug_config["alpha"]
        self.real_data_access = self.hybrid_config["real_data_access"]
        self._validate_real_data_access()

    def _validate_real_data_access(self):
        """Validate real_data_access parameter."""
        valid_options = ["both", "synthetic_only", "physical_only", "neither"]
        if self.real_data_access not in valid_options:
            raise ValueError(
                f"Invalid real_data_access: '{self.real_data_access}'. "
                f"Must be one of {valid_options}"
            )

    def _initialize_datasets(self):
        """Pre-create base datasets that will be reused."""
        logger.info("Initializing reusable datasets...")

        train_sim_indices = self.trainer_config["train_sim"]
        percentage_real_data = self.trainer_config["percentage_real_data"]
        
        # Create base datasets (without augmentation)
        tensor_result = self._create_dataset(
            train_sim_indices,
            return_fields=False,
            percentage_real_data=percentage_real_data
        )
        self._base_tensor_dataset = tensor_result.dataset
        
        field_result = self._create_dataset(
            train_sim_indices,
            return_fields=True,
            percentage_real_data=percentage_real_data
        )
        self._base_field_dataset = field_result.dataset

    def train(self):
        """Main training loop."""
        if self.warmup_synthetic_epochs > 0:
            self._run_warmup()
        
        pbar = tqdm(range(self.num_cycles), desc="Hybrid Cycles", unit="cycle")
        
        for cycle in pbar:
            self.current_cycle = cycle
            
            synthetic_loss, physical_loss = self._run_cycle()
            
            pbar.set_postfix({
                'syn_loss': f"{synthetic_loss:.6f}",
                'phy_loss': f"{physical_loss:.6f}",
                'params': f"{self.physical_trainer.get_current_params()}"
            })
            self._save_if_best(synthetic_loss, physical_loss)

    def _run_warmup(self):
        """Warmup phase - train synthetic model on real data only."""
        logger.info(f"Running warmup for {self.warmup_synthetic_epochs} epochs...")
        
        self._base_tensor_dataset.access_policy = "real_only"
        
        warmup_dataloader = DataLoader(
            self._base_tensor_dataset,
            batch_size=self.trainer_config["batch_size"],
            shuffle=True,
            num_workers=self.trainer_config.get("num_workers", 0),
            pin_memory=True
        )
        
        self.synthetic_trainer.train(
            data_source=warmup_dataloader,
            num_epochs=self.warmup_synthetic_epochs,
            verbose=True
        )

        self._clear_gpu_memory()

    def _run_cycle(self) -> Tuple[float, float]:
        """Execute one complete hybrid training cycle."""
        logger.debug(f"\n{'='*60}")
        logger.debug(f"CYCLE {self.current_cycle + 1}/{self.num_cycles}")
        logger.debug(f"{'='*60}")
        
        # Phase 1: Generate physical trajectories (as Field rollouts)
        logger.debug("Phase 1: Generating physical trajectories...")
        physical_rollouts = self._generate_physical_rollouts()
        
        # Phase 2: Add to tensor dataset and train synthetic model
        logger.debug("Phase 2: Training synthetic model on physical trajectories...")
        # Ensure dataset access policy is set before we recompute totals in the dataset
        access_policy = self._get_access_policy(for_synthetic=True)
        self._base_tensor_dataset.access_policy = access_policy

        logger.debug(f"  Set TensorDataset access_policy={access_policy} before adding augmented trajectories")
        self._base_tensor_dataset.set_augmented_trajectories(physical_rollouts)
        synthetic_loss = self._train_synthetic_model()
        
        # Phase 3: Generate synthetic predictions (as tensor windows)
        logger.debug("Phase 3: Generating synthetic predictions...")
        synthetic_predictions = self._generate_synthetic_predictions()
        
        # Phase 4: Add to field dataset and train physical model
        logger.debug("Phase 4: Training physical model on synthetic predictions...")
        physical_loss = self._train_physical_model(synthetic_predictions)
        
        # Cleanup
        del physical_rollouts
        del synthetic_predictions
        self._clear_gpu_memory()
        
        return synthetic_loss, physical_loss
    
    # ==================== PHASE 1: PHYSICAL ROLLOUT GENERATION ====================
    
    def _generate_physical_rollouts(self) -> List[Dict[str, Field]]:
        """
        Generate physical model rollouts as Field trajectories.
        
        Returns:
            List of rollout dictionaries: [{'field_name': Field[time, x, y]}]
        """
        with self.managed_memory_phase("Physical Generation"):
            if hasattr(self.physical_model, 'to'):
                self.physical_model.to(self.device)
            
            # Calculate requirements
            num_real_samples = self._calculate_num_real_samples()
            num_synthetic_samples = int(num_real_samples * self.alpha)
            
            trajectory_length = self.generation_config["total_steps"]
            samples_per_trajectory = trajectory_length - self.trainer_config["num_predict_steps"]
            num_trajectories = max(1, (num_synthetic_samples + samples_per_trajectory - 1) // samples_per_trajectory)
            
            logger.debug(
                f"  Generating {num_trajectories} trajectories "
                f"(~{num_synthetic_samples} samples after windowing)"
            )
            
            # Generate batched rollout
            initial_state = self.physical_model.get_random_state(batch_size=num_trajectories)
            rollout = self.physical_model.rollout(initial_state, num_steps=trajectory_length)
            
            # Split into list of individual trajectories
            field_names = ConfigHelper(self.config).get_field_names()
            rollouts = []
            
            for traj_idx in range(num_trajectories):
                trajectory = {}
                for field_name in field_names:
                    trajectory[field_name] = rollout[field_name].batch[traj_idx]
                rollouts.append(trajectory)
            
            logger.debug(f"  Generated {len(rollouts)} physical trajectories")
            return rollouts
    
    # ==================== PHASE 2: SYNTHETIC MODEL TRAINING ====================
    
    def _train_synthetic_model(self) -> float:
        """
        Train synthetic model on real + physical trajectories.
        
        TensorDataset handles windowing of physical trajectories internally.
        """
        with self.managed_memory_phase("Synthetic Training", clear_cache=False):
            # Set access policy
            access_policy = self._get_access_policy(for_synthetic=True)
            self._base_tensor_dataset.access_policy = access_policy
            
            logger.debug(
                f"  TensorDataset: {self._base_tensor_dataset.num_real} real + "
                f"{self._base_tensor_dataset.num_augmented} augmented = "
                f"{len(self._base_tensor_dataset)} total samples"
            )
            
            # Create dataloader
            dataloader = DataLoader(
                self._base_tensor_dataset,
                batch_size=self.trainer_config["batch_size"],
                shuffle=True,
                num_workers=self.trainer_config.get("num_workers", 0),
                pin_memory=True
            )
            
            # Train
            result = self.synthetic_trainer.train(
                data_source=dataloader,
                num_epochs=self.synthetic_epochs_per_cycle,
                verbose=False
            )
            
            logger.debug(f"  Synthetic loss: {result['final_loss']:.6f}")
            return result['final_loss']
    
    # ==================== PHASE 3: SYNTHETIC PREDICTION GENERATION ====================
    
# In src/training/hybrid/trainer.py
# Update the _generate_synthetic_predictions method

# In src/training/hybrid/trainer.py
# Update the _generate_synthetic_predictions method

    def _generate_synthetic_predictions(self) -> List[torch.Tensor]:
        """
        Generate windowed synthetic predictions as trajectories.
        
        NEW BEHAVIOR:
        - Passes physical trajectories directly to synthetic model
        - No dataset indexing involved - uses raw augmented_samples
        - Returns trajectories in BVTS format that can be windowed
        - Format: [1, V, T, H, W] where T is trajectory length
        
        Returns:
            List of prediction trajectory tensors in BVTS format
        """
        with self.managed_memory_phase("Synthetic Prediction"):
            self.synthetic_model.to(self.device)
            self.synthetic_model.eval()
            
            # Get physical trajectories directly from TensorDataset
            # These are stored in augmented_samples as cache-format dicts
            physical_trajectories = self._base_tensor_dataset.augmented_samples
            
            if not physical_trajectories:
                logger.warning("No physical trajectories available for synthetic prediction")
                return []
            
            logger.debug(f"  Using {len(physical_trajectories)} physical trajectories as input")
            
            # Use model's built-in generation method
            # Pass trajectories directly, not through dataset indexing
            prediction_trajectories = self.synthetic_model.generate_predictions(
                trajectories=physical_trajectories,
                device=str(self.device),
                batch_size=1,  # Could batch multiple trajectories if needed
            )
            
            logger.debug(f"  Generated {len(prediction_trajectories)} synthetic prediction trajectories")
            return prediction_trajectories


    def _train_physical_model(
        self,
        synthetic_predictions: List[torch.Tensor]
    ) -> float:
        """
        Train physical model on real + synthetic predictions.
        
        UPDATED: Now handles prediction trajectories instead of pre-windowed samples.
        FieldDataset will handle windowing using set_augmented_trajectories.
        """
        if len(self.physical_trainer.learnable_params) == 0:
            logger.info("No learnable parameters, skipping physical training")
            return 0.0

        with self.managed_memory_phase("Physical Training", clear_cache=False):
            # Set access policy
            access_policy = self._get_access_policy(for_synthetic=False)
            
            # NEW: Convert BVTS trajectories to cache-format dicts for FieldDataset
            cache_format_trajectories = []
            cfg_helper = ConfigHelper(self.config)
            field_names = cfg_helper.get_field_names()
            
            for traj_tensor in synthetic_predictions:
                # traj_tensor: [1, V, T, H, W] in BVTS format
                # Squeeze batch dimension
                traj_tensor = traj_tensor.squeeze(0)  # [V, T, H, W]
                
                # Split into per-field tensors based on channel counts
                field_trajectories = {}
                channel_idx = 0
                
                for field_name in field_names:
                    # Get number of channels for this field
                    if field_name in self.synthetic_model.output_specs:
                        num_channels = self.synthetic_model.output_specs[field_name]
                    else:
                        num_channels = self.synthetic_model.input_specs[field_name]
                    
                    # Extract field channels
                    field_tensor = traj_tensor[channel_idx:channel_idx + num_channels]  # [C, T, H, W]
                    field_trajectories[field_name] = field_tensor
                    channel_idx += num_channels
                
                # Create cache-format dict
                cache_format_trajectories.append({'tensor_data': field_trajectories})
            
            # Set augmented trajectories (now in proper cache format)
            self._base_field_dataset.set_augmented_trajectories(cache_format_trajectories)
            self._base_field_dataset.access_policy = access_policy
            
            logger.debug(
                f"  FieldDataset: {self._base_field_dataset.num_real} real + "
                f"{self._base_field_dataset.num_augmented} augmented = "
                f"{len(self._base_field_dataset)} total samples"
            )
            
            # Create dataloader
            from src.data.dataset_utilities import field_collate_fn
            dataloader = DataLoader(
                self._base_field_dataset,
                batch_size=self.trainer_config["batch_size"],
                shuffle=True,
                num_workers=0,
                collate_fn=field_collate_fn
            )
            
            # Train
            result = self.physical_trainer.train(
                data_source=dataloader,
                num_epochs=self.physical_epochs_per_cycle,
                verbose=False
            )
            
            logger.debug(f"  Physical loss: {result['final_loss']:.6f}")
            return float(result['final_loss'])
        
    # ==================== UTILITIES ====================
    
    def _calculate_num_real_samples(self) -> int:
        """
        Calculate number of truly real samples (excluding any augmentation).
        
        For TensorDataset: Need to count only the original real data, not
        any previously added physical trajectories.
        """
        # Check if dataset has original real count stored
        if hasattr(self._base_tensor_dataset, '_num_real_only'):
            # This exists if we've added trajectories before
            return self._base_tensor_dataset._num_real_only
        
        # Otherwise, num_real is the true real count
        return self._base_tensor_dataset.num_real
    
    def _create_dataset(
        self,
        sim_indices: List[int],
        return_fields: bool = False,
        percentage_real_data: float = 1.0
    ):
        """Create dataset using DataLoaderFactory."""
        mode = "field" if return_fields else "tensor"
        
        result = DataLoaderFactory.create(
            config=self.config,
            mode=mode,
            sim_indices=sim_indices,
            enable_augmentation=False,
            batch_size=self.trainer_config["batch_size"],
            percentage_real_data=percentage_real_data
        )
        
        return result
    
    def _get_access_policy(self, for_synthetic: bool) -> str:
        """Determine data access policy based on configuration."""
        if for_synthetic:
            # Synthetic model training
            if self.real_data_access in ["both", "synthetic_only"]:
                return "both"  # Use real + physical trajectories
            else:
                return "generated_only"  # Only physical trajectories
        else:
            # Physical model training
            if self.real_data_access in ["both", "physical_only"]:
                return "both"  # Use real + synthetic predictions
            else:
                return "generated_only"  # Only synthetic predictions
    
    @staticmethod
    @contextmanager
    def managed_memory_phase(phase_name: str, clear_cache: bool = True):
        """Context manager for memory-efficient training phases."""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        try:
            yield
        finally:
            if torch.cuda.is_available() and clear_cache:
                gc.collect()
                torch.cuda.empty_cache()
    
    def _clear_gpu_memory(self):
        """Force GPU memory cleanup."""
        if torch.cuda.is_available():
            gc.collect()
            torch.cuda.empty_cache()
    
    def _save_if_best(self, synthetic_loss: float, physical_loss: float):
        """Save checkpoints if losses improved."""
        if synthetic_loss < self.best_synthetic_loss:
            self.best_synthetic_loss = synthetic_loss
            checkpoint_path = Path(self.synthetic_trainer.checkpoint_path)
            checkpoint_path = (
                checkpoint_path.parent
                / f"{checkpoint_path.stem}_hybrid_best{checkpoint_path.suffix}"
            )
            torch.save(self.synthetic_model.state_dict(), checkpoint_path)
            logger.debug(f"  Saved best synthetic model (loss: {synthetic_loss:.6f})")
        
        if physical_loss < self.best_physical_loss:
            self.best_physical_loss = physical_loss
            logger.debug(f"  New best physical loss: {physical_loss:.6f}")

--- src/training/physical/__init__.py ---

"""Physical training module."""

from src.training.physical.trainer import PhysicalTrainer

__all__ = ["PhysicalTrainer"]


--- src/training/physical/trainer.py ---

# In src/training/physical/trainer.py

import os
import sys
import time
import logging
from typing import Dict, Any, List, Optional, Tuple

# --- PhiFlow Imports ---
from phi.torch.flow import *
from phi.field import l2_loss
from phi.math import math, Tensor

# --- Repo Imports ---
from src.models import ModelRegistry
from src.training.field_trainer import FieldTrainer
from src.utils.logger import get_logger
from torch.utils.data import DataLoader

logger = get_logger(__name__)


class PhysicalTrainer(FieldTrainer):
    """
    Solves an inverse problem for a PhysicalModel using cached data
    from DataManager/FieldDataset.

    This trainer uses math.minimize for optimization and leverages
    the efficient DataLoader pipeline with field conversion.

    Inherits from FieldTrainer to get PhiFlow-specific functionality.

    Phase 1 Migration: Now receives model and learnable params externally,
    data passed via train(). Always uses sliding window.
    """

    def __init__(self, config: Dict[str, Any], model, learnable_params: Dict[str, Tensor]):
        """
        Initializes the trainer with external model and learnable parameters.

        Args:
            config: Full configuration dictionary
            model: Pre-created physical model
            learnable_params: List of PhiFlow Tensors for learnable parameters
        """
        # Initialize base trainer with model and params
        super().__init__(config, model, learnable_params)
        
        # Placeholder
        self.batch_size = 1000

    def _train_epoch(self, data_source: DataLoader) -> float:
        """
        Train using a DataLoader that provides pre-collated batches.
        """
        total_loss = 0.0

        # The data_source is now a DataLoader
        for stacked_initial, stacked_targets in data_source:
            # Optimize over the entire batch
            batch_loss = self._optimize_batch(stacked_initial, stacked_targets)

            total_loss += batch_loss


        return total_loss

    def _optimize_batch(
        self,
        initial_fields: Dict[str, Field],
        target_fields: Dict[str, Field],
    ) -> float:
        """
        Optimize parameters over a batch of samples.
        
        Key insight: The loss function computes loss for ALL samples in parallel,
        and math.minimize finds parameters that minimize the AVERAGE loss.
        
        Args:
            initial_fields: Fields with shape [samples, x, y, ...]
            target_fields: Fields with shape [samples, time, x, y, ...]
        
        Returns:
            Average loss across batch
        """
        
        def loss_function(*learnable_tensors: Tensor) -> Tensor:
            """
            Compute loss across entire batch.
            
            Returns:
                Scalar loss (averaged over batch dimension)
            """
            # Update model parameters
            self._update_model_params(learnable_tensors)
            
            # Initialize loss accumulator (scalar)
            total_loss = math.tensor(0.0)
            
            # Run batched simulation
            # initial_fields already has batch dimension [samples, x, y]
            current_state = initial_fields
            
            for step in range(self.num_predict_steps):
                # Forward step operates on batch dimension automatically!
                # current_state: [samples, x, y] → [samples, x, y]
                current_state = self.model.forward(current_state)
                
                # Compute loss for this timestep
                step_loss = self._compute_step_loss(
                    current_state, target_fields, step
                )
                total_loss += step_loss
            
            # Average over timesteps
            avg_loss = total_loss / self.num_predict_steps
            # Loss is already averaged over batch dimension by PhiML operations
            return avg_loss
        
        # Run optimization
        try:
            estimated_params = minimize(loss_function, self.optimizer)
        except Exception as e:
            logger.error(f"Batched optimization failed: {e}")
            estimated_params = tuple(self.learnable_params)
        
        # Compute final loss
        final_loss = loss_function(*estimated_params)
        # Update parameters
        self.update_optimizer_params(list(estimated_params))
        return self._tensor_to_float(final_loss)

    def _compute_step_loss(
        self,
        current_state: Dict[str, Field],
        target_fields: Dict[str, Field],
        step: int,
    ) -> Tensor:
        """
        Compute L2 loss for a timestep across batch.
        
        Args:
            current_state: Current predictions [samples, x, y]
            target_fields: Ground truth [samples, time, x, y]
            step: Current timestep index
        
        Returns:
            Scalar loss (automatically averaged over batch)
        """
        step_loss = math.tensor(0.0)
        
        for field_name, gt_field in target_fields.items():
            # Extract target for this timestep
            # gt_field has shape [samples, time, x, y]
            # We want [samples, x, y]
            target = gt_field.time[step]
            
            # Current prediction
            prediction = current_state[field_name]
            
            # Compute difference
            # Both have shape [samples, x, y]
            diff = prediction - target
            
            # L2 loss (automatically handles batch dimension)
            field_loss = l2_loss(diff)
            
            # Sum over spatial dimensions, average over samples
            # PhiML automatically reduces over batch dimensions in math.sum
            field_loss = math.mean(field_loss, dim="batch,time")
            step_loss += field_loss
        return step_loss    

    
    def _update_model_params(self, learnable_tensors: Tuple[Tensor, ...]):
        """
        Update model's learnable parameters.

        Args:
            learnable_tensors: Current parameter values from optimizer
        """
        for param_name, param_value in zip(self.param_names, learnable_tensors):
            setattr(self.model, param_name, param_value)
    
    def _run_optimization(self, loss_function: callable) -> Tuple[Tensor, ...]:
        """
        Run PhiML optimization with error handling.

        Args:
            loss_function: Loss function to minimize

        Returns:
            Tuple of optimized parameter tensors
        """
        try:
            # Run optimization with optional monitoring
            estimated_params = minimize(loss_function, self.optimizer)
            return estimated_params

        except Exception as e:
            logger.error(f"Optimization failed: {e}")
            # Return initial parameters as fallback
            return tuple(self.learnable_params)


--- src/training/synthetic/__init__.py ---

"""Synthetic training module."""

from src.training.synthetic.trainer import SyntheticTrainer

__all__ = ["SyntheticTrainer"]


--- src/training/synthetic/trainer.py ---

# src/training/synthetic/trainer.py

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from typing import Dict, Any, List

# Import tensor trainer (new hierarchy)
from src.training.tensor_trainer import TensorTrainer
from torch.utils.data import DataLoader


# Import logging
from src.utils.logger import get_logger

logger = get_logger(__name__)


class SyntheticTrainer(TensorTrainer):
    """
    Tensor-based trainer for synthetic models using DataManager pipeline.

    Uses TensorDataset for efficient cached data loading with no runtime
    Field conversions. All conversions happen once during caching.

    Inherits from TensorTrainer to get PyTorch-specific functionality.

    Phase 1 Migration: Now receives model externally, data passed via train().
    """

    def __init__(self, config: Dict[str, Any], model: nn.Module):
        """
        Initializes the trainer with external model.

        Args:
            config: Full configuration dictionary
            model: Pre-created synthetic model (e.g., UNet)
        """
        # Initialize base trainer with model
        super().__init__(config, model)

        # --- Loss function ---
        self.loss_fn = nn.MSELoss()  # Simple MSE for tensor-based training

    def _train_epoch(self, data_source: DataLoader) -> float:
        """
        OPTIMIZED: Removed redundant time tracking, cleaner structure.
        """
        self.model.train()
        total_loss = 0.0

        # Optional: Add progress bar
        # pbar = tqdm(data_source, desc="Training", leave=False)
        
        for batch in data_source:
            self.optimizer.zero_grad(set_to_none=True)  # More efficient

            # Compute loss using shared method
            loss = self._compute_batch_loss(batch)

            # AMP backward pass
            self.scaler.scale(loss).backward()
            
            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

        # Update scheduler (moved to base class logic if needed)
        if self.scheduler is not None:
            self.scheduler.step()

        return total_loss / len(data_source)

    def _compute_batch_loss(self, batch) -> torch.Tensor:
        """
        OPTIMIZED: Better memory management, clearer autoregressive loop.
        """
        initial_state, rollout_targets = batch
        
        # Non-blocking transfer
        initial_state = initial_state.to(self.device, non_blocking=True)
        rollout_targets = rollout_targets.to(self.device, non_blocking=True)

        # Enforce BVTS-only inputs: initial_state and rollout_targets must be
        # BVTS-shaped tensors: [B, V, 1, H, W] and [B, V, T, H, W]. If not,
        # raise a descriptive error to guide migration.
        if not (initial_state.dim() == 5 and rollout_targets.dim() == 5):
            raise ValueError(
                f"Expected BVTS tensors for SyntheticTrainer (initial: 5D, targets: 5D), "
                f"got initial.dim()={initial_state.dim()}, targets.dim()={rollout_targets.dim()}. "
                "Ensure datasets and DataManager produce BVTS-shaped tensors [B,V,T,H,W]."
            )

        num_steps = rollout_targets.shape[2]

        # BVTS autoregressive loop: current_state is [B, V, 1, H, W]
        with torch.amp.autocast(enabled=True, device_type=self.device.type):
            current_state = initial_state
            total_loss = 0.0

            for t in range(num_steps):
                # Predict next frame as BVTS [B, V, 1, H, W]
                prediction = self.model(current_state)

                # prediction: [B, V, 1, H, W] -> squeeze time dim for comparison
                pred_frame = prediction[:, :, 0]

                # target frame: rollout_targets[:, :, t] -> [B, V, H, W]
                target_frame = rollout_targets[:, :, t]

                # Accumulate loss (compare per-channel tensors)
                total_loss += self.loss_fn(pred_frame, target_frame)

                # Next input is the predicted frame (keep as [B,V,1,H,W])
                current_state = prediction

            avg_loss = total_loss / float(num_steps)

        return avg_loss


--- src/training/tensor_trainer.py ---

"""
Tensor Trainer

This module provides the base class for PyTorch tensor-based trainers.
All PyTorch-specific functionality lives here, including:
- Device management (CPU/GPU)
- Model checkpointing
- Parameter counting
- Epoch-based training loop structure
"""

from abc import abstractmethod
from typing import Dict, Any, Optional, List
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
import os

from src.training.abstract_trainer import AbstractTrainer
from src.utils.logger import get_logger

logger = get_logger(__name__)


class TensorTrainer(AbstractTrainer):
    """
    Base class for PyTorch tensor-based trainers.

    NEW ARCHITECTURE (Phase 1):
    - Model is passed in __init__, not created internally
    - Data is passed to train() method, not held internally
    - Trainers are persistent across training calls
    - Optimizer state is preserved

    Provides all PyTorch-specific functionality:
    - Device management (CPU/GPU)
    - Model checkpoint saving/loading
    - Model parameter counting and summary
    - Common epoch-based training loop structure
    - Validation support (optional)

    Subclasses should implement:
    - _train_epoch_with_data(): Train for one epoch using provided data_source
    - _compute_batch_loss(): Compute loss for a single batch (optional, for validation)

    The train() method accepts data explicitly and should not be overridden
    in most cases.

    Attributes:
        config: Full configuration dictionary
        device: PyTorch device (CPU or CUDA)
        model: The PyTorch neural network model (passed in __init__)
        optimizer: PyTorch optimizer
        checkpoint_path: Path to model checkpoint file
        best_val_loss: Best validation loss seen so far
    """

    def __init__(self, config: Dict[str, Any], model: nn.Module):
        """
        Initialize tensor trainer with model.

        Args:
            config: Full configuration dictionary containing all settings
            model: Pre-created PyTorch model
        """
        super().__init__(config)

        # --- Derive all parameters from config ---
        self.data_config = config["data"]
        self.model_config = config["model"]["synthetic"]
        self.trainer_config = config["trainer_params"]

        # --- Data specifications ---
        self.field_names: List[str] = self.data_config["fields"]
        self.dset_name = self.data_config["dset_name"]
        self.data_dir = self.data_config["data_dir"]

        # --- Checkpoint path ---
        model_save_name = self.model_config["model_save_name"]
        model_path_dir = self.model_config["model_path"]
        self.checkpoint_path = Path(model_path_dir) / f"{model_save_name}.pth"
        os.makedirs(model_path_dir, exist_ok=True)

        # PyTorch-specific initialization
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Store model and move to device
        self.model = model.to(self.device)

        # Create optimizer for the model
        self.optimizer = self._create_optimizer()

        # --- AMP ---
        self._setup_amp_scaler(enabled=True)

        # --- Scheduler ---
        epochs = self.trainer_config["epochs"]
        self._setup_scheduler('cosine', T_max=epochs)

        # Validation state tracking
        self.best_val_loss = float("inf")
        self.best_epoch = 0

    def _create_optimizer(self) -> torch.optim.Optimizer:
        """
        Create optimizer for the model.

        Can be overridden by subclasses for custom optimizer configuration.
        Default: Adam with learning rate from config.

        Returns:
            PyTorch optimizer instance
        """
        learning_rate = self.config['trainer_params']['learning_rate']
        return torch.optim.Adam(self.model.parameters(), lr=learning_rate)

    @abstractmethod
    def _train_epoch(self, data_source: DataLoader) -> float:
        """
        Train for one epoch using provided data source.

        This method should:
        1. Iterate through batches from data_source
        2. Perform forward pass
        3. Compute loss
        4. Perform backward pass and optimization
        5. Return average epoch loss

        Args:
            data_source: DataLoader yielding (input, target) tuples

        Returns:
            Average loss for the epoch
        """
        pass

    def train(self, data_source: DataLoader, num_epochs: int, verbose: bool = True) -> Dict[str, Any]:
        """
        Execute training for specified number of epochs with provided data.

        Args:
            data_source: PyTorch DataLoader yielding (input, target) tuples
            num_epochs: Number of epochs to train

        Returns:
            Dictionary with training results including losses and metrics
        """
        results = {
            "train_losses": [],
            "epochs": [],
            "num_epochs": num_epochs,
            "best_epoch": 0,
            "best_val_loss": float("inf"),
        }

        # Only log if not suppressed in config
        if verbose:
            logger.info(f"Training on {self.device} for {num_epochs} epochs")

        # Create progress bar for epochs (disable if suppress_training_logs is True)
        disable_tqdm = not verbose
        pbar = tqdm(
            range(num_epochs), desc="Training", unit="epoch", disable=disable_tqdm
        )

        for epoch in pbar:
            start_time = time.time()

            # Training
            train_loss = self._train_epoch(data_source)
            results["train_losses"].append(train_loss)
            results["epochs"].append(epoch + 1)

            # Track best model (based on train loss if no validation)
            if train_loss < self.best_val_loss:
                self.best_val_loss = train_loss
                self.best_epoch = epoch + 1
                results["best_epoch"] = self.best_epoch
                results["best_val_loss"] = self.best_val_loss

                self.save_checkpoint(
                    epoch=epoch,
                    loss=train_loss,
                    optimizer_state=(
                        self.optimizer.state_dict() if self.optimizer else None
                    )
                )

            epoch_time = time.time() - start_time

            # Update progress bar
            postfix_dict = {
                "train_loss": f"{train_loss:.6f}",
                "time": f"{epoch_time:.2f}s",
            }

            postfix_dict["best_epoch"] = self.best_epoch

            pbar.set_postfix(postfix_dict)

        final_loss = results["train_losses"][-1]

        # Only log if not suppressed in config
        if verbose:
            logger.info(
                f"Training Complete! Best Epoch: {results['best_epoch']}, Final Loss: {final_loss:.6f}"
            )

        results["final_loss"] = final_loss
        return results
    
    # In TensorTrainer, add these methods:

    def _validate_epoch(self, data_source: DataLoader) -> float:
        """
        Run validation on provided data source.
        Subclasses must implement _compute_batch_loss().
        """
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in data_source:
                loss = self._compute_batch_loss(batch)
                total_loss += loss.item()
        
        self.model.train()
        return total_loss / len(data_source)

    @abstractmethod
    def _compute_batch_loss(self, batch) -> torch.Tensor:
        """
        Compute loss for a single batch.
        Must be implemented by subclasses.
        """
        pass

    def _setup_amp_scaler(self, enabled: bool = True):
        """Setup automatic mixed precision scaler."""
        self.scaler = torch.amp.GradScaler(enabled=enabled)
        
    def _setup_scheduler(self, scheduler_type: str = 'cosine', **kwargs):
        """Setup learning rate scheduler."""
        if scheduler_type == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, **kwargs
            )
        # Add other scheduler types as needed

    # =========================================================================
    # PyTorch-Specific Utilities
    # =========================================================================

    def save_checkpoint(
        self,
        epoch: int,
        loss: float,
        optimizer_state: Optional[Dict] = None,
        additional_info: Optional[Dict] = None,
    ):
        """
        Save PyTorch model checkpoint.

        Args:
            epoch: Current epoch number
            loss: Current loss value
            optimizer_state: Optimizer state dict (optional)
            additional_info: Any additional information to save (optional)
            is_best: If True, also save as 'best.pth'

        Raises:
            ValueError: If checkpoint_path is not set
            RuntimeError: If model is not initialized
        """

        # Build checkpoint dictionary
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "loss": loss,
            "config": self.config,
        }

        if optimizer_state is not None:
            checkpoint["optimizer_state_dict"] = optimizer_state

        if additional_info is not None:
            checkpoint.update(additional_info)

        # Save regular checkpoint
        torch.save(checkpoint, self.checkpoint_path)
        logger.debug(f"Saved checkpoint to {self.checkpoint_path}")

    def load_checkpoint(
        self, path: Optional[Path] = None, strict: bool = True
    ) -> Dict[str, Any]:
        """
        Load PyTorch model checkpoint.

        Args:
            path: Path to checkpoint. If None, uses self.checkpoint_path
            strict: Whether to strictly enforce that keys in checkpoint match
                   keys in model (passed to model.load_state_dict)

        Returns:
            Checkpoint dictionary containing epoch, loss, config, etc.

        Raises:
            ValueError: If no checkpoint path provided
            FileNotFoundError: If checkpoint file doesn't exist
            RuntimeError: If model is not initialized
        """

        if path is None:
            path = self.checkpoint_path

        if not Path(path).exists():
            raise FileNotFoundError(f"Checkpoint not found: {path}")

        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint["model_state_dict"], strict=strict)

        return checkpoint

    def get_parameter_count(self) -> int:
        """
        Get total number of model parameters.

        Returns:
            Total parameter count, or 0 if model not initialized
        """
        if self.model is None:
            return 0
        return sum(p.numel() for p in self.model.parameters())

    def get_trainable_parameter_count(self) -> int:
        """
        Get number of trainable parameters.

        Returns:
            Trainable parameter count, or 0 if model not initialized
        """
        if self.model is None:
            return 0
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)

    def print_model_summary(self):
        """
        Print model summary information.

        Displays:
        - Model type
        - Total parameters
        - Trainable parameters
        - Non-trainable parameters
        - Device (CPU/CUDA)
        """
        if self.model is None:
            logger.warning("Model not initialized")
            return

        total_params = self.get_parameter_count()
        trainable_params = self.get_trainable_parameter_count()

        logger.info("=" * 60)
        logger.info("MODEL SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Model type: {self.model.__class__.__name__}")
        logger.info(f"Total parameters: {total_params:,}")
        logger.info(f"Trainable parameters: {trainable_params:,}")
        logger.info(f"Non-trainable parameters: {total_params - trainable_params:,}")
        logger.info(f"Device: {self.device}")
        logger.info("=" * 60)

    def move_model_to_device(self):
        """Move model to the configured device (CPU or CUDA)."""
        if self.model is not None:
            self.model = self.model.to(self.device)
            logger.info(f"Model moved to {self.device}")

    def set_train_mode(self):
        """Set model to training mode."""
        if self.model is not None:
            self.model.train()

    def set_eval_mode(self):
        """Set model to evaluation mode."""
        if self.model is not None:
            self.model.eval()


--- src/utils/__init__.py ---

"""Utility modules for HYCO-PhiFlow."""

from .logger import setup_logger, get_logger

__all__ = ["setup_logger", "get_logger"]


--- src/utils/gpu_memory_profiler.py ---

"""
GPU Memory Profiler for Training

Monitors GPU memory during actual training runs.
This script provides instructions and tools to profile GPU memory usage.
"""

import torch
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.memory_monitor import MemoryMonitor


def add_memory_monitoring_to_training():
    """
    Instructions for adding memory monitoring to actual training.

    This provides code snippets to add to the trainer for profiling.
    """
    print("=" * 80)
    print("GPU MEMORY PROFILING INSTRUCTIONS")
    print("=" * 80)

    print("\nTo profile GPU memory during training, add this code to your trainer:")
    print("\n1. In src/training/tensor_trainer.py, add at the top:")
    print("-" * 80)
    print(
        """
from src.utils.memory_monitor import MemoryMonitor
import torch
    """
    )

    print("\n2. In the _train_epoch method, add memory tracking:")
    print("-" * 80)
    print(
        """
def _train_epoch(self):
    '''Train for one epoch with memory monitoring.'''
    self.model.train()
    epoch_loss = 0.0
    
    # Print memory at epoch start
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        MemoryMonitor.print_memory_usage("Epoch start: ")
    
    for batch_idx, (initial_state, rollout_targets) in enumerate(self.train_loader):
        # Move to device
        initial_state = initial_state.to(self.device)
        rollout_targets = rollout_targets.to(self.device)
        
        # Print memory for first few batches
        if batch_idx < 5:
            MemoryMonitor.print_memory_usage(f"  Batch {batch_idx} after data to GPU: ")
        
        # Forward pass
        self.optimizer.zero_grad()
        predictions = self._rollout(initial_state, rollout_targets.shape[1])
        
        if batch_idx < 5:
            MemoryMonitor.print_memory_usage(f"  Batch {batch_idx} after forward: ")
        
        # Loss
        loss = self._compute_loss(predictions, rollout_targets)
        
        # Backward
        loss.backward()
        
        if batch_idx < 5:
            MemoryMonitor.print_memory_usage(f"  Batch {batch_idx} after backward: ")
        
        # Optimizer step
        self.optimizer.step()
        
        if batch_idx < 5:
            MemoryMonitor.print_memory_usage(f"  Batch {batch_idx} after optimizer: ")
            print(f"    Loss: {loss.item():.6f}")
        
        epoch_loss += loss.item()
    
    # Print peak memory
    if torch.cuda.is_available():
        peak = torch.cuda.max_memory_allocated() / 1024 / 1024
        MemoryMonitor.print_memory_usage("Epoch end: ")
        print(f"  Peak GPU memory this epoch: {peak:.1f} MB")
    
    return epoch_loss / len(self.train_loader)
    """
    )

    print("\n3. Alternative: Quick profiler script")
    print("=" * 80)
    print("\nOr run training normally with CUDA memory profiling:")
    print(
        """
# Before training:
torch.cuda.memory._record_memory_history(enabled=True)

# After training:
torch.cuda.memory._dump_snapshot("memory_snapshot.pickle")

# Then analyze with:
# python -m torch.cuda.memory viz trace memory_snapshot.pickle
    """
    )

    print("\n4. Quick memory check during training:")
    print("=" * 80)
    print(
        """
# Add this to your training loop to print memory every N batches:
if batch_idx % 10 == 0:
    allocated = torch.cuda.memory_allocated() / 1024**2
    reserved = torch.cuda.memory_reserved() / 1024**2
    print(f"Batch {batch_idx}: GPU {allocated:.0f}MB allocated, {reserved:.0f}MB reserved")
    """
    )


def estimate_memory_requirements():
    """
    Estimate memory requirements for typical training scenario.
    """
    print("\n" + "=" * 80)
    print("MEMORY REQUIREMENT ESTIMATION")
    print("=" * 80)

    # Typical UNet configuration from burgers_experiment
    print("\nTypical UNet (4 levels, 64 filters):")
    print("-" * 80)

    # Rough parameter count for UNet
    # Level 0: 64 filters, ~100K params
    # Level 1: 128 filters, ~400K params
    # Level 2: 256 filters, ~1.6M params
    # Level 3: 512 filters, ~6.4M params
    # Total: ~8.5M parameters

    model_params = 8_500_000
    bytes_per_param = 4  # float32

    model_size_mb = model_params * bytes_per_param / (1024**2)
    gradients_mb = model_size_mb  # Same size as model
    optimizer_mb = model_size_mb * 2  # Adam keeps 2 states per parameter

    print(f"  Model parameters: {model_params:,}")
    print(f"  Model weights: {model_size_mb:.1f} MB")
    print(f"  Gradients: {gradients_mb:.1f} MB")
    print(f"  Optimizer state (Adam): {optimizer_mb:.1f} MB")
    print(
        f"  Total model+training overhead: {model_size_mb + gradients_mb + optimizer_mb:.1f} MB"
    )

    print("\nData/Activation Memory (depends on batch size):")
    print("-" * 80)

    # Burgers: 2 channels (velocity components), 128x128 resolution
    spatial_size = 128 * 128
    channels = 2
    batch_size = 4
    num_predict_steps = 4

    # Input batch
    input_mb = batch_size * channels * spatial_size * bytes_per_param / (1024**2)

    # Target batch (multiple timesteps)
    target_mb = (
        batch_size
        * num_predict_steps
        * channels
        * spatial_size
        * bytes_per_param
        / (1024**2)
    )

    # Activations (rough estimate: ~10x model size for deep networks)
    activations_mb = model_size_mb * 10

    print(f"  Input batch ({batch_size}x{channels}x{spatial_size}): {input_mb:.1f} MB")
    print(
        f"  Target batch ({batch_size}x{num_predict_steps}x{channels}x{spatial_size}): {target_mb:.1f} MB"
    )
    print(f"  Intermediate activations (estimate): {activations_mb:.1f} MB")
    print(f"  Total data/activation: {input_mb + target_mb + activations_mb:.1f} MB")

    print("\nTotal Estimated GPU Memory:")
    print("=" * 80)
    total_mb = (
        model_size_mb
        + gradients_mb
        + optimizer_mb
        + input_mb
        + target_mb
        + activations_mb
    )
    print(f"  {total_mb:.1f} MB ({total_mb/1024:.2f} GB)")

    print("\nPyTorch Memory Overhead:")
    print("-" * 80)
    print("  PyTorch caching allocator reserves extra memory for efficiency")
    print("  Typical overhead: +20-30% of allocated memory")
    total_with_overhead = total_mb * 1.25
    print(
        f"  Total with overhead: {total_with_overhead:.1f} MB ({total_with_overhead/1024:.2f} GB)"
    )

    print("\nRECOMMENDATIONS:")
    print("=" * 80)
    if total_with_overhead / 1024 > 8:
        print("  ⚠️  Estimated usage exceeds 8GB!")
        print("  Consider:")
        print("    - Reduce batch size")
        print("    - Use gradient accumulation")
        print("    - Use mixed precision training (FP16)")
        print("    - Reduce model size (fewer filters/levels)")
    elif total_with_overhead / 1024 > 6:
        print("  ⚠️  Estimated usage is high (6-8GB)")
        print("  Should work but close to limit. Monitor carefully.")
    else:
        print("  ✓ Estimated usage should be comfortable (<6GB)")


if __name__ == "__main__":
    add_memory_monitoring_to_training()
    estimate_memory_requirements()

    print("\n" + "=" * 80)
    print("NEXT STEPS:")
    print("=" * 80)
    print("\nTo actually profile GPU memory during training:")
    print("1. Modify src/training/tensor_trainer.py with the code above")
    print("2. Run: python run.py --config-name=burgers_experiment")
    print("3. Watch the memory output during training")
    print("\nOR")
    print("1. Run your training normally")
    print("2. Use nvidia-smi in another terminal to monitor GPU usage")
    print("3. Add simple memory prints to your training loop")
    print("=" * 80)


--- src/utils/logger.py ---

"""
Logging configuration for HYCO-PhiFlow.

This module sets up a centralized logging system with:
- Console output with color coding
- File output with rotation
- Different log levels for different modules
- Structured logging with context
"""

import logging
import sys
from pathlib import Path
from typing import Optional
from datetime import datetime


class ColoredFormatter(logging.Formatter):
    """Formatter that adds color to console output."""

    COLORS = {
        "DEBUG": "\033[36m",  # Cyan
        "INFO": "\033[32m",  # Green
        "WARNING": "\033[33m",  # Yellow
        "ERROR": "\033[31m",  # Red
        "CRITICAL": "\033[35m",  # Magenta
    }
    RESET = "\033[0m"

    def format(self, record):
        # Shorten module name to last component
        name_parts = record.name.split(".")
        if len(name_parts) > 2:
            # Keep only the last 2 parts (e.g., training.synthetic_trainer)
            record.name = ".".join(name_parts[-2:])

        log_color = self.COLORS.get(record.levelname, self.RESET)
        record.levelname = f"{log_color}{record.levelname}{self.RESET}"
        return super().format(record)


def setup_logger(
    name: str,
    log_dir: Optional[Path] = None,
    level: int = logging.INFO,
    log_to_file: bool = True,
    log_to_console: bool = True,
) -> logging.Logger:
    """
    Set up a logger with file and console handlers.

    Args:
        name: Logger name (usually __name__ of the module)
        log_dir: Directory for log files (default: logs/)
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_to_file: Whether to log to file
        log_to_console: Whether to log to console

    Returns:
        Configured logger instance

    Example:
        >>> logger = setup_logger(__name__)
        >>> logger.info("Training started")
        >>> logger.error("Model failed to load", exc_info=True)
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.propagate = False  # Don't propagate to root logger (avoids Hydra duplicate)

    # Avoid duplicate handlers
    if logger.handlers:
        return logger

    # Console handler with color
    if log_to_console:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        # Simplified format: just [module] LEVEL: message
        console_format = ColoredFormatter("[%(name)s] %(levelname)s: %(message)s")
        console_handler.setFormatter(console_format)
        logger.addHandler(console_handler)

    # File handler with rotation
    if log_to_file:
        if log_dir is None:
            log_dir = Path("logs")
        log_dir.mkdir(parents=True, exist_ok=True)

        # Create log file with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f'{name.replace(".", "_")}_{timestamp}.log'

        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)  # File gets all messages
        file_format = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        file_handler.setFormatter(file_format)
        logger.addHandler(file_handler)

        logger.debug(f"Logging to file: {log_file}")

    return logger


def get_logger(name: str, level: Optional[int] = None) -> logging.Logger:
    """
    Get or create a logger with default configuration.

    Args:
        name: Logger name (use __name__)
        level: Override default logging level

    Returns:
        Logger instance
    """
    if level is None:
        level = logging.INFO

    return setup_logger(name, level=level)


--- src/utils/memory_monitor.py ---

"""
Memory and Performance Monitoring Utilities

Provides tools for monitoring CPU and GPU memory usage, as well as execution time
during training. Useful for diagnosing memory issues and performance bottlenecks.

Includes decorators and context managers for non-intrusive monitoring.
"""

import torch
import psutil
import os
import time
from typing import Optional, Dict, Callable, Any, List
from functools import wraps
from contextlib import contextmanager
from dataclasses import dataclass, field
from collections import defaultdict

from .logger import get_logger

logger = get_logger(__name__)


class MemoryMonitor:
    """Monitor CPU and GPU memory usage during training."""

    @staticmethod
    def get_cpu_memory_mb() -> float:
        """
        Get current process CPU memory usage in MB.

        Returns:
            Memory usage in megabytes
        """
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024

    @staticmethod
    def get_gpu_memory_mb(device: int = 0) -> float:
        """
        Get current GPU memory usage in MB.

        Args:
            device: GPU device index (default: 0)

        Returns:
            Memory usage in megabytes, or 0.0 if CUDA not available
        """
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated(device) / 1024 / 1024
        return 0.0

    @staticmethod
    def get_gpu_memory_reserved_mb(device: int = 0) -> float:
        """
        Get reserved GPU memory (allocated by PyTorch cache) in MB.

        Args:
            device: GPU device index (default: 0)

        Returns:
            Reserved memory in megabytes, or 0.0 if CUDA not available
        """
        if torch.cuda.is_available():
            return torch.cuda.memory_reserved(device) / 1024 / 1024
        return 0.0

    @staticmethod
    def get_memory_summary(device: int = 0) -> Dict[str, float]:
        """
        Get comprehensive memory usage summary.

        Args:
            device: GPU device index (default: 0)

        Returns:
            Dictionary with memory usage statistics in MB
        """
        summary = {
            "cpu_memory_mb": MemoryMonitor.get_cpu_memory_mb(),
            "gpu_memory_allocated_mb": MemoryMonitor.get_gpu_memory_mb(device),
            "gpu_memory_reserved_mb": MemoryMonitor.get_gpu_memory_reserved_mb(device),
        }

        if torch.cuda.is_available():
            summary["gpu_memory_free_mb"] = (
                torch.cuda.get_device_properties(device).total_memory / 1024 / 1024
                - summary["gpu_memory_reserved_mb"]
            )

        return summary

    @staticmethod
    def print_memory_usage(prefix: str = "", device: int = 0):
        """
        Print current memory usage to console.

        Args:
            prefix: Optional prefix string for the output
            device: GPU device index (default: 0)
        """
        cpu_mem = MemoryMonitor.get_cpu_memory_mb()
        gpu_mem = MemoryMonitor.get_gpu_memory_mb(device)
        gpu_reserved = MemoryMonitor.get_gpu_memory_reserved_mb(device)

        if torch.cuda.is_available():
            logger.info(
                f"{prefix}CPU: {cpu_mem:.1f} MB | GPU Allocated: {gpu_mem:.1f} MB | GPU Reserved: {gpu_reserved:.1f} MB"
            )
        else:
            logger.info(f"{prefix}CPU: {cpu_mem:.1f} MB | GPU: Not available")

    @staticmethod
    def log_memory_usage(logger, prefix: str = "", device: int = 0):
        """
        Log memory usage to a logger.

        Args:
            logger: Logger instance (e.g., from logging module)
            prefix: Optional prefix string for the log message
            device: GPU device index (default: 0)
        """
        cpu_mem = MemoryMonitor.get_cpu_memory_mb()
        gpu_mem = MemoryMonitor.get_gpu_memory_mb(device)
        gpu_reserved = MemoryMonitor.get_gpu_memory_reserved_mb(device)

        if torch.cuda.is_available():
            logger.info(
                f"{prefix}CPU: {cpu_mem:.1f} MB | GPU Allocated: {gpu_mem:.1f} MB | GPU Reserved: {gpu_reserved:.1f} MB"
            )
        else:
            logger.info(f"{prefix}CPU: {cpu_mem:.1f} MB | GPU: Not available")


class MemoryTracker:
    """
    Track memory usage over time for analysis.

    Usage:
        tracker = MemoryTracker()
        tracker.record("start")
        # ... do some work ...
        tracker.record("after_loading")
        # ... more work ...
        tracker.record("after_training")
        tracker.print_summary()
    """

    def __init__(self, device: int = 0):
        """
        Initialize memory tracker.

        Args:
            device: GPU device index to monitor (default: 0)
        """
        self.device = device
        self.records = []

    def record(self, label: str):
        """
        Record current memory usage with a label.

        Args:
            label: Label for this memory snapshot
        """
        summary = MemoryMonitor.get_memory_summary(self.device)
        summary["label"] = label
        self.records.append(summary)

    def print_summary(self):
        """Print summary of all recorded memory snapshots."""
        if not self.records:
            logger.info("No memory records available")
            return

        logger.info("\n" + "=" * 80)
        logger.info("Memory Usage Summary")
        logger.info("=" * 80)
        logger.info(
            f"{'Label':<30} {'CPU (MB)':<12} {'GPU Alloc (MB)':<15} {'GPU Reserved (MB)':<15}"
        )
        logger.info("-" * 80)

        for record in self.records:
            label = record["label"]
            cpu = record["cpu_memory_mb"]
            gpu_alloc = record["gpu_memory_allocated_mb"]
            gpu_reserved = record["gpu_memory_reserved_mb"]
            logger.info(
                f"{label:<30} {cpu:>10.1f}   {gpu_alloc:>13.1f}   {gpu_reserved:>17.1f}"
            )

        logger.info("=" * 80 + "\n")

    def get_peak_memory(self) -> Dict[str, float]:
        """
        Get peak memory usage across all records.

        Returns:
            Dictionary with peak CPU and GPU memory usage
        """
        if not self.records:
            return {"cpu_peak_mb": 0.0, "gpu_peak_mb": 0.0}

        cpu_peak = max(r["cpu_memory_mb"] for r in self.records)
        gpu_peak = max(r["gpu_memory_allocated_mb"] for r in self.records)

        return {"cpu_peak_mb": cpu_peak, "gpu_peak_mb": gpu_peak}

    def clear(self):
        """Clear all recorded memory snapshots."""
        self.records = []


# ============================================================================
# DECORATORS AND CONTEXT MANAGERS
# ============================================================================


@contextmanager
def track_memory(label: str = "operation", device: int = 0, print_output: bool = True):
    """
    Context manager to track memory usage for a code block.

    Usage:
        with track_memory("data loading"):
            data = load_large_dataset()

    Args:
        label: Label for this operation
        device: GPU device index
        print_output: Whether to print memory changes
    """
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats(device)

    start_cpu = MemoryMonitor.get_cpu_memory_mb()
    start_gpu = MemoryMonitor.get_gpu_memory_mb(device)

    try:
        yield
    finally:
        end_cpu = MemoryMonitor.get_cpu_memory_mb()
        end_gpu = MemoryMonitor.get_gpu_memory_mb(device)

        cpu_delta = end_cpu - start_cpu
        gpu_delta = end_gpu - start_gpu

        if print_output:
            logger.info(
                f"[Memory] {label}: CPU {cpu_delta:+.1f} MB, GPU {gpu_delta:+.1f} MB"
            )
            if torch.cuda.is_available():
                peak_gpu = torch.cuda.max_memory_allocated(device) / 1024 / 1024
                logger.info(f"         Peak GPU: {peak_gpu:.1f} MB")


def monitor_memory(label: Optional[str] = None, device: int = 0, verbose: bool = True):
    """
    Decorator to monitor memory usage of a function.

    Usage:
        @monitor_memory("training epoch")
        def train_epoch(self):
            # training code
            pass

    Args:
        label: Label for this operation (defaults to function name)
        device: GPU device index
        verbose: Whether to print memory changes
    """

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            func_label = label or func.__name__

            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats(device)

            start_cpu = MemoryMonitor.get_cpu_memory_mb()
            start_gpu = MemoryMonitor.get_gpu_memory_mb(device)

            result = func(*args, **kwargs)

            end_cpu = MemoryMonitor.get_cpu_memory_mb()
            end_gpu = MemoryMonitor.get_gpu_memory_mb(device)

            if verbose:
                cpu_delta = end_cpu - start_cpu
                gpu_delta = end_gpu - start_gpu
                logger.info(
                    f"[Memory] {func_label}: CPU {cpu_delta:+.1f} MB, GPU {gpu_delta:+.1f} MB"
                )
                if torch.cuda.is_available():
                    peak_gpu = torch.cuda.max_memory_allocated(device) / 1024 / 1024
                    logger.info(f"         Peak GPU: {peak_gpu:.1f} MB")

            return result

        return wrapper

    return decorator


class EpochMemoryMonitor:
    """
    Lightweight memory monitor for training loops.

    Usage in trainer:
        self.memory_monitor = EpochMemoryMonitor(enabled=True, verbose_batches=5)

        # In training loop:
        for epoch in range(epochs):
            self.memory_monitor.on_epoch_start()

            for batch_idx, batch in enumerate(dataloader):
                self.memory_monitor.on_batch_start(batch_idx)
                # ... training code ...
                self.memory_monitor.on_batch_end(batch_idx)

            self.memory_monitor.on_epoch_end()
    """

    def __init__(self, enabled: bool = True, verbose_batches: int = 5, device: int = 0):
        """
        Initialize epoch memory monitor.

        Args:
            enabled: Whether monitoring is active
            verbose_batches: Number of batches to print detailed info for
            device: GPU device index
        """
        self.enabled = enabled
        self.verbose_batches = verbose_batches
        self.device = device
        self._batch_count = 0

    def on_epoch_start(self):
        """Call at the start of each epoch."""
        if not self.enabled:
            return

        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats(self.device)

        self._batch_count = 0

    def on_batch_start(self, batch_idx: int):
        """Call at the start of each batch."""
        if not self.enabled or batch_idx >= self.verbose_batches:
            return

        # Store starting memory for this batch
        self._batch_start_gpu = MemoryMonitor.get_gpu_memory_mb(self.device)

    def on_batch_end(self, batch_idx: int, loss: Optional[float] = None):
        """Call at the end of each batch."""
        if not self.enabled or batch_idx >= self.verbose_batches:
            return

        gpu_mem = MemoryMonitor.get_gpu_memory_mb(self.device)
        cpu_mem = MemoryMonitor.get_cpu_memory_mb()

        loss_str = f", loss={loss:.6f}" if loss is not None else ""
        logger.info(
            f"  Batch {batch_idx}: GPU {gpu_mem:.1f} MB, CPU {cpu_mem:.1f} MB{loss_str}"
        )

        self._batch_count = batch_idx + 1

    def on_epoch_end(self):
        """Call at the end of each epoch."""
        if not self.enabled:
            return

        if torch.cuda.is_available():
            peak_gpu = torch.cuda.max_memory_allocated(self.device) / 1024 / 1024
            current_gpu = MemoryMonitor.get_gpu_memory_mb(self.device)
            reserved_gpu = MemoryMonitor.get_gpu_memory_reserved_mb(self.device)

            logger.info(
                f"  Epoch end: GPU {current_gpu:.1f} MB allocated, "
                f"{reserved_gpu:.1f} MB reserved, peak {peak_gpu:.1f} MB"
            )


class BatchMemoryProfiler:
    """
    Detailed memory profiler for individual training steps.

    Usage:
        profiler = BatchMemoryProfiler(enabled=True)

        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 5:  # Only profile first 5 batches
                profiler.disable()

            profiler.checkpoint("data_loaded")

            batch = batch.to(device)
            profiler.checkpoint("data_to_gpu")

            output = model(batch)
            profiler.checkpoint("forward")

            loss = criterion(output, target)
            loss.backward()
            profiler.checkpoint("backward")

            optimizer.step()
            profiler.checkpoint("optimizer")

            profiler.print_summary()
            profiler.reset()
    """

    def __init__(self, enabled: bool = True, device: int = 0):
        """
        Initialize batch memory profiler.

        Args:
            enabled: Whether profiling is active
            device: GPU device index
        """
        self.enabled = enabled
        self.device = device
        self.checkpoints = []
        self._last_label = "start"
        self._last_gpu = (
            0.0
            if not torch.cuda.is_available()
            else MemoryMonitor.get_gpu_memory_mb(device)
        )

    def checkpoint(self, label: str):
        """Record a checkpoint with current memory usage."""
        if not self.enabled:
            return

        gpu_mem = MemoryMonitor.get_gpu_memory_mb(self.device)
        gpu_delta = gpu_mem - self._last_gpu

        self.checkpoints.append(
            {
                "label": label,
                "gpu_mb": gpu_mem,
                "gpu_delta": gpu_delta,
                "from": self._last_label,
            }
        )

        self._last_label = label
        self._last_gpu = gpu_mem

    def print_summary(self):
        """Print summary of all checkpoints."""
        if not self.enabled or not self.checkpoints:
            return

        logger.info(f"    Memory profile:")
        for cp in self.checkpoints:
            delta_str = f"{cp['gpu_delta']:+.1f}" if cp["gpu_delta"] != 0 else " 0.0"
            logger.info(
                f"      {cp['from']} → {cp['label']}: "
                f"GPU {cp['gpu_mb']:.1f} MB ({delta_str} MB)"
            )

    def reset(self):
        """Reset checkpoints for next batch."""
        self.checkpoints = []
        self._last_label = "start"
        self._last_gpu = MemoryMonitor.get_gpu_memory_mb(self.device)

    def disable(self):
        """Disable profiling."""
        self.enabled = False

    def enable(self):
        """Enable profiling."""
        self.enabled = True


# ============================================================================
# PERFORMANCE MONITORING (Time + Memory)
# ============================================================================


@dataclass
class PerformanceMetrics:
    """Container for performance metrics."""

    operation: str
    duration_seconds: float
    cpu_start_mb: float
    cpu_end_mb: float
    cpu_delta_mb: float
    gpu_start_mb: float
    gpu_end_mb: float
    gpu_delta_mb: float
    gpu_peak_mb: float = 0.0
    call_count: int = 1

    def __str__(self):
        return (
            f"{self.operation}: {self.duration_seconds:.3f}s, "
            f"CPU {self.cpu_delta_mb:+.1f}MB, GPU {self.gpu_delta_mb:+.1f}MB (peak {self.gpu_peak_mb:.1f}MB)"
        )


class PerformanceMonitor:
    """
    Monitor both execution time and memory usage for operations.

    Usage:
        monitor = PerformanceMonitor()

        with monitor.track("data loading"):
            data = load_data()

        with monitor.track("forward pass"):
            output = model(data)

        monitor.print_summary()
    """

    def __init__(self, enabled: bool = True, device: int = 0):
        """
        Initialize performance monitor.

        Args:
            enabled: Whether monitoring is active
            device: GPU device index (use 0 for default GPU, ignored if CUDA unavailable)
        """
        self.enabled = enabled
        # Validate device parameter
        if device < 0:
            device = 0  # Default to device 0 if invalid
        if torch.cuda.is_available() and device >= torch.cuda.device_count():
            device = 0  # Default to device 0 if out of range
        self.device = device
        self.metrics: List[PerformanceMetrics] = []
        self.aggregate_metrics: Dict[str, List[float]] = defaultdict(list)

    @contextmanager
    def track(self, operation: str, aggregate: bool = False):
        """
        Context manager to track performance of an operation.

        Args:
            operation: Name of the operation being tracked
            aggregate: If True, accumulate stats for this operation name

        Usage:
            with monitor.track("data loading"):
                data = load_data()
        """
        if not self.enabled:
            yield
            return

        # Reset peak stats (initialize CUDA if needed)
        if torch.cuda.is_available():
            try:
                # This will initialize CUDA if not already initialized
                torch.cuda.set_device(self.device)
                torch.cuda.reset_peak_memory_stats(self.device)
            except RuntimeError:
                # Device might be invalid, skip reset
                pass

        # Start measurements
        start_time = time.perf_counter()
        cpu_start = MemoryMonitor.get_cpu_memory_mb()
        gpu_start = MemoryMonitor.get_gpu_memory_mb(self.device)

        try:
            yield
        finally:
            # End measurements
            end_time = time.perf_counter()
            cpu_end = MemoryMonitor.get_cpu_memory_mb()
            gpu_end = MemoryMonitor.get_gpu_memory_mb(self.device)

            duration = end_time - start_time
            cpu_delta = cpu_end - cpu_start
            gpu_delta = gpu_end - gpu_start

            gpu_peak = 0.0
            if torch.cuda.is_available():
                gpu_peak = torch.cuda.max_memory_allocated(self.device) / 1024 / 1024

            metrics = PerformanceMetrics(
                operation=operation,
                duration_seconds=duration,
                cpu_start_mb=cpu_start,
                cpu_end_mb=cpu_end,
                cpu_delta_mb=cpu_delta,
                gpu_start_mb=gpu_start,
                gpu_end_mb=gpu_end,
                gpu_delta_mb=gpu_delta,
                gpu_peak_mb=gpu_peak,
            )

            self.metrics.append(metrics)

            if aggregate:
                self.aggregate_metrics[operation].append(duration)

    def print_summary(self, show_aggregate: bool = True):
        """
        Print summary of all tracked operations.

        Args:
            show_aggregate: Whether to show aggregated statistics
        """
        if not self.metrics:
            logger.info("No performance metrics recorded")
            return

        logger.info("\n" + "=" * 90)
        logger.info("PERFORMANCE SUMMARY")
        logger.info("=" * 90)
        logger.info(
            f"{'Operation':<30} {'Time (s)':<12} {'CPU (MB)':<15} {'GPU (MB)':<15} {'GPU Peak':<12}"
        )
        logger.info("-" * 90)

        for m in self.metrics:
            cpu_str = f"{m.cpu_delta_mb:+.1f}".rjust(7)
            gpu_str = f"{m.gpu_delta_mb:+.1f}".rjust(7)
            logger.info(
                f"{m.operation:<30} {m.duration_seconds:>10.3f}  {cpu_str:>13}  {gpu_str:>13}  {m.gpu_peak_mb:>10.1f}"
            )

        # Total time
        total_time = sum(m.duration_seconds for m in self.metrics)
        logger.info("-" * 90)
        logger.info(f"{'TOTAL':<30} {total_time:>10.3f}s")

        # Aggregate statistics
        if show_aggregate and self.aggregate_metrics:
            logger.info("\n" + "=" * 90)
            logger.info("AGGREGATE STATISTICS (for repeated operations)")
            logger.info("=" * 90)
            logger.info(
                f"{'Operation':<30} {'Count':<8} {'Total (s)':<12} {'Mean (s)':<12} {'Min (s)':<12} {'Max (s)':<12}"
            )
            logger.info("-" * 90)

            for op_name, durations in self.aggregate_metrics.items():
                count = len(durations)
                total = sum(durations)
                mean = total / count
                min_dur = min(durations)
                max_dur = max(durations)

                logger.info(
                    f"{op_name:<30} {count:<8} {total:<12.3f} {mean:<12.3f} {min_dur:<12.3f} {max_dur:<12.3f}"
                )

        logger.info("=" * 90 + "\n")

    def get_metrics(self) -> List[PerformanceMetrics]:
        """Get all recorded metrics."""
        return self.metrics

    def get_total_time(self) -> float:
        """Get total time across all operations."""
        return sum(m.duration_seconds for m in self.metrics)

    def get_operation_time(self, operation: str) -> float:
        """Get total time for a specific operation."""
        return sum(m.duration_seconds for m in self.metrics if m.operation == operation)

    def clear(self):
        """Clear all metrics."""
        self.metrics = []
        self.aggregate_metrics = defaultdict(list)


@contextmanager
def track_performance(operation: str, print_result: bool = True, device: int = 0):
    """
    Standalone context manager for quick performance tracking.

    Usage:
        with track_performance("data loading"):
            data = load_data()

    Args:
        operation: Name of the operation
        print_result: Whether to print the result immediately
        device: GPU device index (use 0 for default GPU, ignored if CUDA unavailable)
    """
    # Validate device parameter
    if device < 0:
        device = 0
    if torch.cuda.is_available() and device >= torch.cuda.device_count():
        device = 0

    if torch.cuda.is_available():
        try:
            torch.cuda.set_device(device)
            torch.cuda.reset_peak_memory_stats(device)
        except RuntimeError:
            pass

    start_time = time.perf_counter()
    cpu_start = MemoryMonitor.get_cpu_memory_mb()
    gpu_start = MemoryMonitor.get_gpu_memory_mb(device)

    try:
        yield
    finally:
        end_time = time.perf_counter()
        cpu_end = MemoryMonitor.get_cpu_memory_mb()
        gpu_end = MemoryMonitor.get_gpu_memory_mb(device)

        duration = end_time - start_time
        cpu_delta = cpu_end - cpu_start
        gpu_delta = gpu_end - gpu_start

        if print_result:
            logger.info(
                f"[Performance] {operation}: {duration:.3f}s, CPU {cpu_delta:+.1f}MB, GPU {gpu_delta:+.1f}MB"
            )
            if torch.cuda.is_available():
                gpu_peak = torch.cuda.max_memory_allocated(device) / 1024 / 1024
                logger.info(f"              GPU peak: {gpu_peak:.1f}MB")


def monitor_performance(operation: Optional[str] = None, device: int = 0):
    """
    Decorator for monitoring function performance.

    Usage:
        @monitor_performance("model training")
        def train_model():
            # training code
            pass

    Args:
        operation: Name of the operation (defaults to function name)
        device: GPU device index (use 0 for default GPU, ignored if CUDA unavailable)
    """
    # Validate device parameter
    validated_device = device
    if validated_device < 0:
        validated_device = 0
    if torch.cuda.is_available() and validated_device >= torch.cuda.device_count():
        validated_device = 0

    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            op_name = operation or func.__name__

            if torch.cuda.is_available():
                try:
                    torch.cuda.set_device(validated_device)
                    torch.cuda.reset_peak_memory_stats(validated_device)
                except RuntimeError:
                    pass

            start_time = time.perf_counter()
            cpu_start = MemoryMonitor.get_cpu_memory_mb()
            gpu_start = MemoryMonitor.get_gpu_memory_mb(validated_device)

            result = func(*args, **kwargs)

            end_time = time.perf_counter()
            cpu_end = MemoryMonitor.get_cpu_memory_mb()
            gpu_end = MemoryMonitor.get_gpu_memory_mb(validated_device)

            duration = end_time - start_time
            cpu_delta = cpu_end - cpu_start
            gpu_delta = gpu_end - gpu_start

            logger.info(
                f"[Performance] {op_name}: {duration:.3f}s, CPU {cpu_delta:+.1f}MB, GPU {gpu_delta:+.1f}MB"
            )
            if torch.cuda.is_available():
                gpu_peak = torch.cuda.max_memory_allocated(device) / 1024 / 1024
                logger.info(f"              GPU peak: {gpu_peak:.1f}MB")

            return result

        return wrapper

    return decorator


class EpochPerformanceMonitor:
    """
    Combined time and memory monitor for training epochs.

    Extends EpochMemoryMonitor with timing capabilities.

    Usage:
        monitor = EpochPerformanceMonitor(enabled=True)

        monitor.on_epoch_start()
        for batch_idx, batch in enumerate(dataloader):
            monitor.on_batch_start(batch_idx)
            # ... training ...
            monitor.on_batch_end(batch_idx, loss)
        monitor.on_epoch_end()
    """

    def __init__(self, enabled: bool = True, verbose_batches: int = 5, device: int = 0):
        """
        Initialize epoch performance monitor.

        Args:
            enabled: Whether monitoring is active
            verbose_batches: Number of batches to print details for
            device: GPU device index (use 0 for default GPU, ignored if CUDA unavailable)
        """
        self.enabled = enabled
        self.verbose_batches = verbose_batches
        # Validate device parameter
        if device < 0:
            device = 0
        if torch.cuda.is_available() and device >= torch.cuda.device_count():
            device = 0
        self.device = device
        self._batch_count = 0
        self._epoch_start_time = 0.0
        self._batch_start_time = 0.0
        self._batch_times: List[float] = []
        self._epoch_times: List[float] = []

    def on_epoch_start(self):
        """Call at the start of each epoch."""
        if not self.enabled:
            return

        if torch.cuda.is_available():
            try:
                torch.cuda.set_device(self.device)
                torch.cuda.reset_peak_memory_stats(self.device)
            except RuntimeError:
                pass

        self._epoch_start_time = time.perf_counter()
        self._batch_count = 0
        self._batch_times = []

    def on_batch_start(self, batch_idx: int):
        """Call at the start of each batch."""
        if not self.enabled:
            return

        self._batch_start_time = time.perf_counter()

    def on_batch_end(self, batch_idx: int, loss: Optional[float] = None):
        """Call at the end of each batch."""
        if not self.enabled:
            return

        batch_time = time.perf_counter() - self._batch_start_time
        self._batch_times.append(batch_time)

        if batch_idx < self.verbose_batches:
            gpu_mem = MemoryMonitor.get_gpu_memory_mb(self.device)
            cpu_mem = MemoryMonitor.get_cpu_memory_mb()

            loss_str = f", loss={loss:.6f}" if loss is not None else ""
            logger.info(
                f"  Batch {batch_idx}: {batch_time:.3f}s, GPU {gpu_mem:.1f}MB, CPU {cpu_mem:.1f}MB{loss_str}"
            )

        self._batch_count = batch_idx + 1

    def on_epoch_end(self):
        """Call at the end of each epoch."""
        if not self.enabled:
            return

        epoch_time = time.perf_counter() - self._epoch_start_time
        self._epoch_times.append(epoch_time)

        if torch.cuda.is_available():
            peak_gpu = torch.cuda.max_memory_allocated(self.device) / 1024 / 1024
            current_gpu = MemoryMonitor.get_gpu_memory_mb(self.device)
            reserved_gpu = MemoryMonitor.get_gpu_memory_reserved_mb(self.device)

            logger.info(
                f"  Epoch end: {epoch_time:.3f}s total, GPU {current_gpu:.1f}MB allocated, "
                f"{reserved_gpu:.1f}MB reserved, peak {peak_gpu:.1f}MB"
            )
        else:
            logger.info(f"  Epoch end: {epoch_time:.3f}s total")

        # Batch statistics
        if self._batch_times:
            avg_batch = sum(self._batch_times) / len(self._batch_times)
            min_batch = min(self._batch_times)
            max_batch = max(self._batch_times)
            logger.info(
                f"  Batch timing: avg={avg_batch:.3f}s, min={min_batch:.3f}s, max={max_batch:.3f}s"
            )

    def get_statistics(self) -> Dict[str, Any]:
        """Get timing statistics."""
        if not self._batch_times:
            return {}

        return {
            "total_epoch_time": sum(self._epoch_times) if self._epoch_times else 0.0,
            "avg_epoch_time": (
                sum(self._epoch_times) / len(self._epoch_times)
                if self._epoch_times
                else 0.0
            ),
            "total_batch_time": sum(self._batch_times),
            "avg_batch_time": sum(self._batch_times) / len(self._batch_times),
            "min_batch_time": min(self._batch_times),
            "max_batch_time": max(self._batch_times),
            "num_batches": len(self._batch_times),
            "num_epochs": len(self._epoch_times),
        }


--- tests/test_bvts_compliance.py ---

"""Lightweight BVTS compliance tests.

These tests avoid heavy project fixtures and instead validate the
validation helpers operate as expected.
"""
import pytest
import torch

from src.utils.field_conversion.validation import (
    assert_bvts_format,
    BVTSValidationError,
    validate_bvts_dataset,
    requires_bvts,
)


def test_assert_bvts_accepts_valid_tensor():
    t = torch.randn(1, 2, 1, 8, 8)
    # Should not raise
    assert_bvts_format(t)


def test_assert_bvts_rejects_invalid_shape():
    bad = torch.randn(3, 4, 5)
    with pytest.raises(BVTSValidationError):
        assert_bvts_format(bad)


def test_validate_bvts_dataset_accepts_good_dataset():
    class GoodDataset:
        def __getitem__(self, idx):
            return (torch.randn(1, 2, 1, 8, 8), torch.randn(1, 2, 1, 8, 8))

    ds = GoodDataset()
    # Should not raise
    validate_bvts_dataset(ds)


def test_validate_bvts_dataset_rejects_bad_dataset():
    class BadDataset:
        def __getitem__(self, idx):
            return torch.randn(4, 5, 6)

    ds = BadDataset()
    with pytest.raises(Exception):
        validate_bvts_dataset(ds)


def test_requires_bvts_decorator_validates_input_and_output():
    @requires_bvts
    def identity(x: torch.Tensor) -> torch.Tensor:
        return x

    good = torch.randn(1, 1, 1, 4, 4)
    # Should not raise
    out = identity(good)
    assert out.shape == good.shape

    bad = torch.randn(3, 4, 5)
    with pytest.raises(BVTSValidationError):
        identity(bad)
