# Data Loading Simplification Proposal

**Project**: HYCO-PhiFlow  
**Document Version**: 1.0  
**Date**: November 3, 2025  
**Status**: Proposal - Awaiting Discussion

**Companion Document**: [DATA_LOADING_ARCHITECTURE.md](./DATA_LOADING_ARCHITECTURE.md)

---

## Executive Summary

This document proposes a **major simplification** of the data loading system to reduce complexity from **8/10 to 4/10** while maintaining all critical functionality.

### Current State vs Proposed State

| Metric | Current | Proposed | Improvement |
|--------|---------|----------|-------------|
| **Abstraction Layers** | 5 | 3 | â¬‡ï¸ 40% |
| **Total Classes** | 10 | 6 | â¬‡ï¸ 40% |
| **Public Methods** | 60+ | 35 | â¬‡ï¸ 42% |
| **Config Parameters** | 15+ | 8 | â¬‡ï¸ 47% |
| **Factory Methods** | 4 | 1 | â¬‡ï¸ 75% |
| **Code Duplication** | Medium | Low | â¬‡ï¸ 70% |
| **Overall Complexity** | 8/10 | 4/10 | â¬‡ï¸ 50% |

### Key Changes

1. âœ… **Eliminate dual-mode dataset** - Separate classes for tensor/field data
2. âœ… **Reduce abstraction layers** - 5 layers â†’ 3 layers
3. âœ… **Simplify factory** - Single creation method with clear parameters
4. âœ… **Merge augmentation classes** - Unified interface
5. âœ… **Remove strategy wrapper** - Direct dataset selection
6. âœ… **Extract config helpers** - Reduce coupling

### Benefits

- ðŸŽ¯ **Easier to understand** - Clear, linear data flow
- ðŸŽ¯ **Easier to debug** - Fewer layers to trace
- ðŸŽ¯ **Easier to extend** - Less interdependencies
- ðŸŽ¯ **Easier to test** - Smaller, focused components
- ðŸŽ¯ **Less code** - ~30% reduction in LOC

---

## Table of Contents

1. [Proposed Architecture](#1-proposed-architecture)
2. [Refactoring Steps](#2-refactoring-steps)
3. [Before & After Comparisons](#3-before--after-comparisons)
4. [Migration Guide](#4-migration-guide)
5. [Risk Assessment](#5-risk-assessment)
6. [Implementation Plan](#6-implementation-plan)

---

## 1. Proposed Architecture

### 1.1 New Component Hierarchy

**Before** (5 layers):
```
TrainerFactory
  â†’ AdaptiveAugmentedDataLoader (strategy selector)
    â†’ AugmentedTensorDataset / CachedAugmentedDataset
      â†’ HybridDataset (dual-mode: tensors OR fields)
        â†’ DataManager
```

**After** (3 layers):
```
DataLoaderFactory (simplified)
  â†’ TensorDataset / FieldDataset (separate, focused)
    â†’ DataManager (unchanged)
```

### 1.2 New Class Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DataLoaderFactory                          â”‚
â”‚  Single creation method with clear parameters               â”‚
â”‚  - create_dataloader(config, mode='tensor'|'field', ...)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  AbstractDataset     â”‚
                   â”‚  (base class)        â”‚
                   â”‚  - Common logic      â”‚
                   â”‚  - Template methods  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â†“                   â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ TensorDataset   â”‚   â”‚ FieldDataset    â”‚
          â”‚ (for synthetic) â”‚   â”‚ (for physical)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   DataManager    â”‚
                   â”‚   (unchanged)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Augmentation Integration

**Before**: Separate wrapper layers

**After**: Built into datasets

```python
class TensorDataset(Dataset):
    """Tensor dataset with optional augmentation."""
    
    def __init__(
        self,
        data_manager,
        sim_indices,
        augmentation_config=None,  # â† Built-in
    ):
        self.real_data = ...
        
        if augmentation_config:
            self.augmented_data = self._load_augmentation(
                augmentation_config
            )
        else:
            self.augmented_data = None
    
    def _load_augmentation(self, config):
        """Load augmented data based on config."""
        if config['mode'] == 'memory':
            return self._load_from_memory(config['data'])
        elif config['mode'] == 'cache':
            return self._load_from_cache(config['cache_dir'])
        else:
            return None
```

---

## 2. Refactoring Steps

### Step 1: Create Abstract Base Dataset

**Issue**: Common logic will be duplicated between `TensorDataset` and `FieldDataset`

**Solution**: Create abstract base class with shared functionality (similar to trainer hierarchy)

#### 1.0 Create `AbstractDataset`

**File**: `src/data/abstract_dataset.py` (NEW)

```python
"""
Abstract Dataset Base Class

Provides common functionality for all dataset types, similar to AbstractTrainer.
Reduces code duplication and ensures consistent behavior.
"""

from typing import List, Optional, Dict, Any, Tuple, Union
from abc import ABC, abstractmethod
from pathlib import Path
import torch
from torch.utils.data import Dataset
from functools import lru_cache

from .data_manager import DataManager


class AbstractDataset(Dataset, ABC):
    """
    Abstract base class for all HYCO datasets.
    
    This class provides:
    - Common initialization logic
    - Augmentation management
    - Sliding window indexing
    - LRU caching for simulations
    - Cache validation
    - Sample routing (real vs augmented)
    
    Subclasses must implement:
    - _load_simulation_uncached(): Load and process simulation data
    - _get_real_sample(): Convert loaded data to final format
    
    Args:
        data_manager: DataManager instance
        sim_indices: List of simulation indices
        field_names: List of field names to load
        num_frames: Number of frames per simulation
        num_predict_steps: Number of prediction steps
        use_sliding_window: Create multiple samples per simulation
        augmentation_config: Optional augmentation configuration
        max_cached_sims: LRU cache size
        
    Returns:
        Implementation-specific format (tensors or Fields)
    """
    
    def __init__(
        self,
        data_manager: DataManager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: int,
        num_predict_steps: int,
        use_sliding_window: bool = False,
        augmentation_config: Optional[Dict[str, Any]] = None,
        max_cached_sims: int = 5,
    ):
        self.data_manager = data_manager
        self.sim_indices = sim_indices
        self.field_names = field_names
        self.num_frames = num_frames
        self.num_predict_steps = num_predict_steps
        self.use_sliding_window = use_sliding_window
        self.max_cached_sims = max_cached_sims
        
        # Validate cache
        self._validate_cache()
        
        # Create LRU cache for simulations
        self._cached_load_simulation = lru_cache(maxsize=max_cached_sims)(
            self._load_simulation_uncached
        )
        
        # Build sliding window index
        self.sample_index = []
        if use_sliding_window:
            self._build_sliding_window_index()
        
        # Load augmentation if configured
        self.augmented_samples = []
        if augmentation_config:
            self._load_augmentation(augmentation_config)
        
        self.num_real = (
            len(self.sample_index) if use_sliding_window 
            else len(sim_indices)
        )
        self.num_augmented = len(self.augmented_samples)
    
    # ==================== Cache Management ====================
    
    def _validate_cache(self):
        """Validate that all required simulations are cached."""
        for sim_idx in self.sim_indices:
            if not self.data_manager.is_simulation_cached(sim_idx):
                raise ValueError(
                    f"Simulation {sim_idx} not found in cache. "
                    f"Please run cache generation first."
                )
    
    # ==================== Sliding Window ====================
    
    def _build_sliding_window_index(self):
        """
        Build index for sliding window samples.
        
        Creates list of (sim_idx, start_frame) tuples for all valid windows.
        """
        self.sample_index = []
        
        for sim_idx in self.sim_indices:
            metadata = self.data_manager.get_simulation_metadata(sim_idx)
            total_frames = metadata.get("num_frames", 0)
            
            # Calculate valid starting positions
            required_frames = self.num_predict_steps + 1
            if total_frames >= required_frames:
                max_start = total_frames - required_frames
                for start_frame in range(max_start + 1):
                    self.sample_index.append((sim_idx, start_frame))
    
    # ==================== Augmentation ====================
    
    def _load_augmentation(self, config: Dict[str, Any]):
        """Load augmented samples based on configuration."""
        mode = config.get('mode', 'cache')
        alpha = config.get('alpha', 0.1)
        
        if mode == 'memory':
            # Pre-loaded augmented data provided
            self.augmented_samples = config['data']
        
        elif mode == 'cache':
            # Load from disk cache
            cache_dir = config['cache_dir']
            expected_count = int(self.num_real * alpha)
            self.augmented_samples = self._load_from_cache(
                cache_dir, expected_count
            )
        
        else:
            raise ValueError(f"Unknown augmentation mode: {mode}")
    
    def _load_from_cache(self, cache_dir: str, expected_count: int):
        """Load augmented samples from cache directory."""
        cache_path = Path(cache_dir)
        
        if not cache_path.exists():
            raise FileNotFoundError(f"Cache not found: {cache_dir}")
        
        cache_files = sorted(cache_path.glob("sample_*.pt"))
        
        if len(cache_files) == 0:
            raise ValueError(f"No cached samples in {cache_dir}")
        
        # Load all cached samples
        samples = []
        for file_path in cache_files:
            data = torch.load(file_path, map_location='cpu')
            if isinstance(data, dict):
                samples.append((data['input'], data['target']))
            else:
                samples.append(data)
        
        return samples
    
    # ==================== Dataset Interface ====================
    
    def __len__(self) -> int:
        """Total samples: real + augmented."""
        return self.num_real + self.num_augmented
    
    def __getitem__(self, idx: int):
        """
        Get sample by index.
        
        Routes to real or augmented sample based on index.
        """
        # Route to real or augmented
        if idx < self.num_real:
            return self._get_real_sample(idx)
        else:
            aug_idx = idx - self.num_real
            return self.augmented_samples[aug_idx]
    
    # ==================== Abstract Methods ====================
    
    @abstractmethod
    def _load_simulation_uncached(self, sim_idx: int) -> Dict[str, Any]:
        """
        Load simulation data from cache.
        
        This method is wrapped with LRU cache automatically.
        
        Args:
            sim_idx: Simulation index
            
        Returns:
            Dictionary mapping field names to loaded data
            (format depends on subclass implementation)
        """
        pass
    
    @abstractmethod
    def _get_real_sample(self, idx: int):
        """
        Get real (non-augmented) sample.
        
        Args:
            idx: Sample index (0 to num_real-1)
            
        Returns:
            Sample in format specific to subclass
            (tensors for TensorDataset, Fields for FieldDataset)
        """
        pass
    
    # ==================== Utility Methods ====================
    
    def get_simulation_and_frame(self, idx: int) -> Tuple[int, int]:
        """
        Get simulation index and frame for a sample index.
        
        Args:
            idx: Sample index
            
        Returns:
            Tuple of (sim_idx, start_frame)
        """
        if idx >= self.num_real:
            raise ValueError(f"Index {idx} is augmented sample")
        
        if self.use_sliding_window:
            return self.sample_index[idx]
        else:
            return self.sim_indices[idx], 0
    
    def clear_cache(self):
        """Clear the LRU cache of loaded simulations."""
        self._cached_load_simulation.cache_clear()
    
    def get_cache_info(self):
        """Get cache statistics."""
        return self._cached_load_simulation.cache_info()
```

**Benefits**:
- âœ… Eliminates code duplication
- âœ… Consistent behavior across dataset types
- âœ… Similar pattern to `AbstractTrainer` hierarchy
- âœ… Easy to extend with new dataset types
- âœ… Template method pattern for customization

---

### Step 2: Split HybridDataset

**Issue**: Dual-mode operation (`return_fields` parameter) adds complexity

**Solution**: Create two focused classes that inherit from `AbstractDataset`

#### 2.1 Create `TensorDataset`

**File**: `src/data/tensor_dataset.py` (NEW)

```python
"""
Tensor Dataset for Synthetic Model Training

Returns PyTorch tensors in format suitable for neural network training.
Inherits common functionality from AbstractDataset.
"""

from typing import List, Optional, Dict, Any, Tuple
import torch

from .abstract_dataset import AbstractDataset


class TensorDataset(AbstractDataset):
    """
    PyTorch Dataset that returns tensors for synthetic training.
    
    Inherits from AbstractDataset:
    - Lazy loading with LRU cache
    - Sliding window support
    - Optional augmentation (built-in)
    
    Additional features:
    - Static/dynamic field separation
    - Pin memory for GPU transfer
    - Tensor concatenation and slicing
    
    Args:
        data_manager: DataManager instance
        sim_indices: List of simulation indices
        field_names: List of field names to load
        num_frames: Number of frames per simulation
        num_predict_steps: Number of prediction steps
        dynamic_fields: Fields that change over time
        static_fields: Fields that don't change
        use_sliding_window: Create multiple samples per simulation
        augmentation_config: Optional augmentation configuration
        max_cached_sims: LRU cache size
        pin_memory: Pin tensors for GPU transfer
        
    Returns:
        Tuple[torch.Tensor, torch.Tensor]:
            - initial_state: [C_all, H, W] - all fields
            - rollout_targets: [T, C_dynamic, H, W] - dynamic fields only
    """
    
    def __init__(
        self,
        data_manager,
        sim_indices: List[int],
        field_names: List[str],
        num_frames: int,
        num_predict_steps: int,
        dynamic_fields: List[str],
        static_fields: List[str] = None,
        use_sliding_window: bool = False,
        augmentation_config: Optional[Dict[str, Any]] = None,
        max_cached_sims: int = 5,
        pin_memory: bool = True,
    ):
        # Call parent constructor (handles common initialization)
        super().__init__(
            data_manager=data_manager,
            sim_indices=sim_indices,
            field_names=field_names,
            num_frames=num_frames,
            num_predict_steps=num_predict_steps,
            use_sliding_window=use_sliding_window,
            augmentation_config=augmentation_config,
            max_cached_sims=max_cached_sims,
        )
        
        # TensorDataset-specific attributes
        self.dynamic_fields = dynamic_fields
        self.static_fields = static_fields or []
        self.pin_memory = pin_memory and torch.cuda.is_available()
    
    def _get_real_sample(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get real sample (not augmented)."""
        # Determine sim and frame
        if self.use_sliding_window:
            sim_idx, start_frame = self.sample_index[idx]
        else:
            sim_idx = self.sim_indices[idx]
            start_frame = 0
        
        # Load simulation data
        sim_data = self._cached_load_simulation(sim_idx)
        
        # Concatenate fields
        all_field_tensors = [sim_data[name] for name in self.field_names]
        all_data = torch.cat(all_field_tensors, dim=1)  # [T, C_all, H, W]
        
        # Extract initial state and targets
        initial_state = all_data[start_frame]  # [C_all, H, W]
        
        dynamic_field_tensors = [sim_data[name] for name in self.dynamic_fields]
        dynamic_data = torch.cat(dynamic_field_tensors, dim=1)
        
        target_start = start_frame + 1
        target_end = start_frame + 1 + self.num_predict_steps
        rollout_targets = dynamic_data[target_start:target_end]  # [T, C_dynamic, H, W]
        
        return initial_state, rollout_targets
    
    # ... (rest of methods similar to current HybridDataset)
```

#### 2.2 Create `FieldDataset`

**File**: `src/data/field_dataset.py` (NEW)

```python
"""
Field Dataset for Physical Model Training

Returns PhiFlow Fields suitable for physics-based simulation.
Inherits common functionality from AbstractDataset.
"""

from typing import List, Optional, Dict, Any, Tuple
import torch
from phi.field import Field

from .abstract_dataset import AbstractDataset


class FieldDataset(AbstractDataset):
    """
    Dataset that returns PhiFlow Fields for physical training.
    
    Features:
    - Lazy loading with LRU cache
    - Sliding window support
    - Optional augmentation (built-in)
    - Field reconstruction from cached tensors
    
    Args:
        data_manager: DataManager instance
        sim_indices: List of simulation indices
        field_names: List of field names to load
        num_frames: Number of frames per simulation
        num_predict_steps: Number of prediction steps
        use_sliding_window: Create multiple samples per simulation
        augmentation_config: Optional augmentation configuration
        max_cached_sims: LRU cache size
        
    
    # ==================== Implementation of Abstract Methods ====================
    
    def _load_simulation_uncached(self, sim_idx: int) -> Dict[str, torch.Tensor]:
        """
        Load simulation tensors from cache.
        
        Returns:
            Dictionary mapping field names to tensors [T, C, H, W]
        """
        sim_data = {}
        for field_name in self.field_names:
            tensor = self.data_manager.load_field(sim_idx, field_name)
            sim_data[field_name] = tensor
        return sim_data
    
    def _get_real_sample(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get real sample (not augmented).
        
        Returns:
            Tuple of (initial_state, rollout_targets)
        """
        # Get simulation and frame (inherited method)
        sim_idx, start_frame = self.get_simulation_and_frame(idx)
        
        # Load simulation data (uses LRU cache)
        sim_data = self._cached_load_simulation(sim_idx)
        
        # Concatenate all fields for initial state
        all_field_tensors = [sim_data[name] for name in self.field_names]
        all_data = torch.cat(all_field_tensors, dim=1)  # [T, C_all, H, W]
        
        # Extract initial state
        initial_state = all_data[start_frame]  # [C_all, H, W]
        
        # Concatenate only dynamic fields for targets
        dynamic_field_tensors = [sim_data[name] for name in self.dynamic_fields]
        dynamic_data = torch.cat(dynamic_field_tensors, dim=1)  # [T, C_dynamic, H, W]
        
        # Extract target rollout
        target_start = start_frame + 1
        target_end = start_frame + 1 + self.num_predict_steps
        rollout_targets = dynamic_data[target_start:target_end]  # [T, C_dynamic, H, W]
        
        # Pin memory if enabled
        if self.pin_memory:
            initial_state = initial_state.pin_memory()
            rollout_targets = rollout_targets.pin_memory()
        
        return initial_state, rollout_targets
            return self.augmented_samples[aug_idx]
    
    # ... (rest of implementation)
```

**Benefits**:
- âœ… Clear separation of concerns
- âœ… Easier to test (single responsibility)
- âœ… No conditional logic based on `return_fields`
- âœ… Augmentation built-in (no wrapper needed)
- âœ… **No code duplication** - common logic in `AbstractDataset`
- âœ… **Consistent pattern** - matches trainer hierarchy
- âœ… **Easy to extend** - add new dataset types by subclassing

---

### Step 3: Simplify Factory

**Issue**: Complex factory with multiple creation methods and config extraction

**Solution**: Single unified factory method

#### 3.1 Create `DataLoaderFactory`

**File**: `src/factories/dataloader_factory.py` (NEW)

```python
"""
Simplified DataLoader Factory

Single creation method for all data loading scenarios.
"""

from typing import List, Optional, Literal
from pathlib import Path
import torch
from torch.utils.data import DataLoader

from src.data import DataManager, TensorDataset, FieldDataset
from src.config import ConfigHelper


class DataLoaderFactory:
    """
    Factory for creating data loaders with minimal configuration.
    
    This replaces the complex TrainerFactory data methods with a
    single, clear creation method.
    """
    
    @staticmethod
    def create(
        config: dict,
        mode: Literal['tensor', 'field'],
        sim_indices: Optional[List[int]] = None,
        batch_size: Optional[int] = None,
        shuffle: bool = True,
        use_sliding_window: bool = True,
        enable_augmentation: bool = None,
    ) -> DataLoader:
        """
        Create a DataLoader for training.
        
        Args:
            config: Full configuration dictionary
            mode: 'tensor' for synthetic, 'field' for physical
            sim_indices: Simulation indices (default: from config)
            batch_size: Batch size (default: from config)
            shuffle: Whether to shuffle
            use_sliding_window: Use sliding window
            enable_augmentation: Enable augmentation (default: from config)
            
        Returns:
            PyTorch DataLoader (or Dataset for physical mode)
            
        Example:
            >>> # Synthetic training
            >>> loader = DataLoaderFactory.create(
            ...     config, mode='tensor', shuffle=True
            ... )
            >>> 
            >>> # Physical training
            >>> dataset = DataLoaderFactory.create(
            ...     config, mode='field', batch_size=None
            ... )
        """
        # Extract config using helper
        cfg = ConfigHelper(config)
        
        # Get parameters
        sim_indices = sim_indices or cfg.get_train_sim_indices()
        batch_size = batch_size or cfg.get_batch_size()
        num_frames = cfg.get_num_frames(use_sliding_window)
        num_predict_steps = cfg.get_num_predict_steps()
        
        # Create DataManager
        data_manager = DataLoaderFactory._create_data_manager(config, cfg)
        
        # Get field specifications
        field_names = cfg.get_field_names()
        dynamic_fields, static_fields = cfg.get_field_types()
        
        # Check augmentation
        enable_augmentation = (
            enable_augmentation 
            if enable_augmentation is not None 
            else cfg.is_augmentation_enabled()
        )
        
        augmentation_config = None
        if enable_augmentation:
            augmentation_config = cfg.get_augmentation_config()
        
        # Create dataset based on mode
        if mode == 'tensor':
            dataset = TensorDataset(
                data_manager=data_manager,
                sim_indices=sim_indices,
                field_names=field_names,
                num_frames=num_frames,
                num_predict_steps=num_predict_steps,
                dynamic_fields=dynamic_fields,
                static_fields=static_fields,
                use_sliding_window=use_sliding_window,
                augmentation_config=augmentation_config,
            )
            
            # Return DataLoader for tensor mode
            return DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=0,
                pin_memory=torch.cuda.is_available(),
            )
        
        elif mode == 'field':
            dataset = FieldDataset(
                data_manager=data_manager,
                sim_indices=sim_indices,
                field_names=field_names,
                num_frames=num_frames,
                num_predict_steps=num_predict_steps,
                use_sliding_window=use_sliding_window,
                augmentation_config=augmentation_config,
            )
            
            # Return dataset directly for field mode (no batching)
            return dataset
        
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    @staticmethod
    def _create_data_manager(config: dict, cfg: 'ConfigHelper') -> DataManager:
        """Create DataManager from config."""
        project_root = Path(config.get("project_root", "."))
        raw_data_dir = project_root / cfg.get_raw_data_dir()
        cache_dir = project_root / cfg.get_cache_dir()
        
        return DataManager(
            raw_data_dir=str(raw_data_dir),
            cache_dir=str(cache_dir),
            config=config,
            validate_cache=cfg.should_validate_cache(),
            auto_clear_invalid=cfg.should_auto_clear_invalid(),
        )
```

**Benefits**:
- âœ… Single creation method
- âœ… Clear parameters
- âœ… Consistent interface
- âœ… Easy to test

---

### Step 4: Extract Config Helper

**Issue**: Config extraction logic duplicated in multiple places

**Solution**: Centralized config helper class

#### 4.1 Create `ConfigHelper`

**File**: `src/config/config_helper.py` (NEW)

```python
"""
Configuration Helper

Centralizes all config extraction logic to reduce coupling.
"""

from typing import List, Tuple, Dict, Any, Optional
from pathlib import Path


class ConfigHelper:
    """
    Helper class for extracting configuration values.
    
    This reduces coupling between components and config structure,
    making refactoring easier.
    
    Args:
        config: Full configuration dictionary
        
    Example:
        >>> cfg = ConfigHelper(config)
        >>> field_names = cfg.get_field_names()
        >>> dynamic, static = cfg.get_field_types()
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.data_config = config.get("data", {})
        self.model_config = config.get("model", {})
        self.trainer_config = config.get("trainer_params", {})
        self.augmentation_config = self.trainer_config.get("augmentation", {})
    
    # === Data Configuration ===
    
    def get_field_names(self) -> List[str]:
        """Get list of field names to load."""
        return self.data_config.get("fields", [])
    
    def get_dataset_name(self) -> str:
        """Get dataset name."""
        return self.data_config.get("dset_name", "unknown")
    
    def get_raw_data_dir(self) -> Path:
        """Get raw data directory path."""
        data_dir = self.data_config.get("data_dir", "data")
        dset_name = self.get_dataset_name()
        return Path(data_dir) / dset_name
    
    def get_cache_dir(self) -> Path:
        """Get cache directory path."""
        data_dir = self.data_config.get("data_dir", "data")
        return Path(data_dir) / "cache"
    
    def should_validate_cache(self) -> bool:
        """Check if cache validation is enabled."""
        return self.data_config.get("validate_cache", True)
    
    def should_auto_clear_invalid(self) -> bool:
        """Check if auto-clear invalid cache is enabled."""
        return self.data_config.get("auto_clear_invalid", False)
    
    # === Field Specifications ===
    
    def get_field_types(self) -> Tuple[List[str], List[str]]:
        """
        Get dynamic and static field names.
        
        Returns:
            Tuple of (dynamic_fields, static_fields)
        """
        model_type = self.config.get("run_params", {}).get("model_type", "synthetic")
        
        if model_type == "synthetic":
            input_specs = self.model_config.get("synthetic", {}).get("input_specs", {})
            output_specs = self.model_config.get("synthetic", {}).get("output_specs", {})
            
            dynamic_fields = list(output_specs.keys())
            static_fields = [f for f in input_specs.keys() if f not in output_specs]
            
            return dynamic_fields, static_fields
        else:
            # Physical model: all fields are dynamic
            field_names = self.get_field_names()
            return field_names, []
    
    # === Training Configuration ===
    
    def get_train_sim_indices(self) -> List[int]:
        """Get training simulation indices."""
        return self.trainer_config.get("train_sim", [])
    
    def get_batch_size(self) -> int:
        """Get batch size."""
        return self.trainer_config.get("batch_size", 16)
    
    def get_num_predict_steps(self) -> int:
        """Get number of prediction steps."""
        return self.trainer_config.get("num_predict_steps", 10)
    
    def get_num_frames(self, use_sliding_window: bool) -> Optional[int]:
        """
        Get number of frames to load.
        
        Args:
            use_sliding_window: If True, return None (load all frames)
            
        Returns:
            Number of frames or None for all frames
        """
        if use_sliding_window:
            return None  # Load all available
        else:
            return self.get_num_predict_steps() + 1
    
    # === Augmentation Configuration ===
    
    def is_augmentation_enabled(self) -> bool:
        """Check if augmentation is enabled."""
        return self.augmentation_config.get("enabled", False)
    
    def get_augmentation_alpha(self) -> float:
        """Get augmentation alpha parameter."""
        return self.augmentation_config.get("alpha", 0.1)
    
    def get_augmentation_mode(self) -> str:
        """Get augmentation mode (memory/cache/on_the_fly)."""
        strategy = self.augmentation_config.get("strategy", "cache")
        # Map strategy to mode
        if strategy == "cached":
            return "cache"
        elif strategy == "on_the_fly":
            return "on_the_fly"
        else:
            return strategy
    
    def get_augmentation_config(self) -> Dict[str, Any]:
        """
        Get complete augmentation configuration.
        
        Returns:
            Dictionary with:
            - mode: 'memory', 'cache', or 'on_the_fly'
            - alpha: Augmentation proportion
            - cache_dir: Cache directory (for cache mode)
            - data: Pre-loaded data (for memory mode)
        """
        if not self.is_augmentation_enabled():
            return None
        
        mode = self.get_augmentation_mode()
        alpha = self.get_augmentation_alpha()
        
        config = {
            'mode': mode,
            'alpha': alpha,
        }
        
        if mode == 'cache':
            # Build cache directory path
            cache_root = self.config.get("cache", {}).get("root", "data/cache")
            experiment_name = self.augmentation_config.get("cache", {}).get(
                "experiment_name", self.get_dataset_name()
            )
            cache_dir = Path(cache_root) / "hybrid_generated" / experiment_name
            config['cache_dir'] = str(cache_dir)
        
        return config
```

**Benefits**:
- âœ… Centralized config access
- âœ… Reduces coupling
- âœ… Easier to refactor config structure
- âœ… Clear documentation of config paths
- âœ… Type hints for all methods

---

### Step 5: Remove Unnecessary Classes

**Classes to Remove**:

1. âŒ `AdaptiveAugmentedDataLoader` - Strategy selection built into datasets
2. âŒ `AugmentedTensorDataset` - Functionality merged into `TensorDataset`
3. âŒ `AugmentedFieldDataset` - Functionality merged into `FieldDataset`
4. âŒ `HybridDataset` - Split into `TensorDataset` and `FieldDataset`

**Classes to Keep**:

1. âœ… `AbstractDataset` - Base class (NEW)
2. âœ… `TensorDataset` - Synthetic training (NEW)
3. âœ… `FieldDataset` - Physical training (NEW)
4. âœ… `DataManager` - Core caching system (unchanged)
5. âœ… `CacheManager` - Useful utility (unchanged)
6. âœ… Generation utilities - Still needed

---

### Step 6: Update Import Structure

**File**: `src/data/__init__.py`

```python
"""
Data Loading Module

Simplified structure with clear hierarchy:
- AbstractDataset: Base class (common functionality)
  - TensorDataset: For synthetic models (tensors)
  - FieldDataset: For physical models (Fields)
- DataManager: Caching and loading
"""

from .abstract_dataset import AbstractDataset
from .data_manager import DataManager
from .tensor_dataset import TensorDataset
from .field_dataset import FieldDataset

# Backward compatibility (optional)
from .hybrid_dataset import HybridDataset  # Deprecated

__all__ = [
    "AbstractDataset",
    "DataManager",
    "TensorDataset",
    "FieldDataset",
    # Deprecated
    "HybridDataset",
]
```

---

## 3. Before & After Comparisons

### 3.1 Creating a DataLoader

#### Before (Complex)

```python
# In TrainerFactory
def create_data_loader_for_synthetic(
    config, sim_indices=None, batch_size=None, 
    shuffle=True, use_sliding_window=True
):
    # Extract config (20+ lines)
    data_config = config["data"]
    trainer_config = config["trainer_params"]
    model_config = config["model"]["synthetic"]
    
    if sim_indices is None:
        sim_indices = trainer_config["train_sim"]
    if batch_size is None:
        batch_size = trainer_config["batch_size"]
    
    # Setup paths (10+ lines)
    project_root = Path(config.get("project_root", "."))
    raw_data_dir = project_root / data_config["data_dir"] / data_config["dset_name"]
    cache_dir = project_root / data_config["data_dir"] / "cache"
    
    # Create DataManager (5+ lines)
    data_manager = DataManager(
        raw_data_dir=str(raw_data_dir),
        cache_dir=str(cache_dir),
        config=config,
        validate_cache=data_config.get("validate_cache", True),
        auto_clear_invalid=data_config.get("auto_clear_invalid", False),
    )
    
    # Extract field specs (10+ lines)
    field_names = data_config["fields"]
    input_specs = model_config["input_specs"]
    output_specs = model_config["output_specs"]
    dynamic_fields = list(output_specs.keys())
    static_fields = [f for f in input_specs.keys() if f not in output_specs]
    num_predict_steps = trainer_config["num_predict_steps"]
    num_frames = None if use_sliding_window else num_predict_steps + 1
    
    # Create base dataset (10+ lines)
    dataset = HybridDataset(
        data_manager=data_manager,
        sim_indices=sim_indices,
        field_names=field_names,
        num_frames=num_frames,
        num_predict_steps=num_predict_steps,
        dynamic_fields=dynamic_fields,
        static_fields=static_fields,
        use_sliding_window=use_sliding_window,
    )
    
    # Check augmentation (30+ lines)
    augmentation_config = trainer_config.get("augmentation", {})
    aug_config = AugmentationConfig(augmentation_config)
    
    if not aug_config.enabled:
        data_loader = DataLoader(dataset, batch_size, shuffle)
        return data_loader
    
    # Create cache manager (10+ lines)
    cache_root = config.get("cache", {}).get("root", "data/cache")
    cache_root = project_root / cache_root
    experiment_name = aug_config.get_cache_config().get("experiment_name", ...)
    cache_manager = CacheManager(cache_root, experiment_name, auto_create=True)
    
    # Create adaptive loader (15+ lines)
    strategy = aug_config.get_strategy()
    adaptive_loader = AdaptiveAugmentedDataLoader(
        real_dataset=dataset,
        alpha=aug_config.get_alpha(),
        generated_data=None,
        cache_dir=str(cache_manager.cache_dir) if strategy == "cached" else None,
        cache_size=aug_config.get_cache_config().get("max_memory_samples", 1000),
        strategy=strategy_map.get(strategy, strategy),
        validate_count=True,
    )
    
    loader = adaptive_loader.get_loader(batch_size, shuffle, num_workers=0)
    return loader

# Total: ~120 lines of code
```

#### After (Simple)

```python
# In DataLoaderFactory
from src.config import ConfigHelper

def create(config, mode='tensor', shuffle=True):
    """Create DataLoader with clear parameters."""
    cfg = ConfigHelper(config)
    
    # Create data manager
    data_manager = create_data_manager(config, cfg)
    
    # Get specifications
    field_names = cfg.get_field_names()
    dynamic_fields, static_fields = cfg.get_field_types()
    
    # Create dataset
    dataset = TensorDataset(
        data_manager=data_manager,
        sim_indices=cfg.get_train_sim_indices(),
        field_names=field_names,
        num_frames=cfg.get_num_frames(use_sliding_window=True),
        num_predict_steps=cfg.get_num_predict_steps(),
        dynamic_fields=dynamic_fields,
        static_fields=static_fields,
        use_sliding_window=True,
        augmentation_config=cfg.get_augmentation_config(),
    )
    
    # Return DataLoader
    return DataLoader(
        dataset,
        batch_size=cfg.get_batch_size(),
        shuffle=shuffle,
        num_workers=0,
        pin_memory=torch.cuda.is_available(),
    )

# Total: ~30 lines of code

# Usage
loader = DataLoaderFactory.create(config, mode='tensor')
```

**Reduction**: 120 lines â†’ 30 lines (75% reduction)

---

### 3.2 Usage in Training Code

#### Before

```python
# In run.py
from src.factories.trainer_factory import TrainerFactory

# Synthetic training
data_loader = TrainerFactory.create_data_loader_for_synthetic(
    config,
    use_sliding_window=True,
)

# Physical training
dataset = TrainerFactory.create_dataset_for_physical(
    config,
    use_sliding_window=True,
)
```

#### After

```python
# In run.py
from src.factories import DataLoaderFactory

# Synthetic training
data_loader = DataLoaderFactory.create(
    config, 
    mode='tensor'
)

# Physical training
dataset = DataLoaderFactory.create(
    config, 
    mode='field'
)
```

**Benefits**:
- âœ… Consistent interface
- âœ… Clear mode parameter
- âœ… Less verbose

---

### 3.3 Hybrid Training

#### Before

```python
# In HybridTrainer
def _create_hybrid_dataset(self, sim_indices, return_fields=False):
    # Duplicate code extraction (30+ lines)
    data_config = self.config["data"]
    model_config = self.config["model"]
    trainer_config = self.trainer_config
    project_root = Path(self.config["project_root"])
    
    raw_data_dir = project_root / data_config.get("data_dir", "data") / data_config["dset_name"]
    cache_dir = project_root / self.config.get("cache", {}).get("root", "data/cache")
    
    data_manager = DataManager(raw_data_dir, cache_dir, self.config)
    
    field_names = data_config["fields"]
    if "synthetic" in model_config and "input_specs" in model_config["synthetic"]:
        input_specs = model_config["synthetic"]["input_specs"]
        output_specs = model_config["synthetic"]["output_specs"]
    else:
        input_specs = {f: {} for f in field_names}
        output_specs = {f: {} for f in field_names}
    
    dynamic_fields = list(output_specs.keys())
    static_fields = [f for f in input_specs.keys() if f not in output_specs]
    num_predict_steps = trainer_config["num_predict_steps"]
    
    return HybridDataset(
        data_manager=data_manager,
        sim_indices=sim_indices,
        field_names=field_names,
        num_frames=None,
        num_predict_steps=num_predict_steps,
        dynamic_fields=dynamic_fields,
        static_fields=static_fields,
        use_sliding_window=True,
        return_fields=return_fields,  # â† Dual mode
    )
```

#### After

```python
# In HybridTrainer
def _create_tensor_dataset(self, sim_indices):
    """Create tensor dataset for synthetic training."""
    return DataLoaderFactory.create(
        self.config,
        mode='tensor',
        sim_indices=sim_indices,
        batch_size=None,  # Will wrap in DataLoader later
    )

def _create_field_dataset(self, sim_indices):
    """Create field dataset for physical training."""
    return DataLoaderFactory.create(
        self.config,
        mode='field',
        sim_indices=sim_indices,
    )
```

**Reduction**: 50+ lines â†’ 15 lines (70% reduction)

---

## 4. Migration Guide

### 4.1 Step-by-Step Migration

#### Phase 1: Create New Components (No Breaking Changes)

1. âœ… Create `AbstractDataset` base class
2. âœ… Create `ConfigHelper` class
3. âœ… Create `TensorDataset` class (inheriting from `AbstractDataset`)
4. âœ… Create `FieldDataset` class (inheriting from `AbstractDataset`)
5. âœ… Create `DataLoaderFactory` class

**Status**: New code coexists with old code

---

#### Phase 2: Update TrainerFactory (Backward Compatible)

1. âœ… Add new methods to `TrainerFactory`:
   ```python
   @staticmethod
   def create_data_loader_v2(config, mode='tensor', **kwargs):
       """New simplified method."""
       return DataLoaderFactory.create(config, mode, **kwargs)
   ```

2. âœ… Deprecate old methods with warnings:
   ```python
   @staticmethod
   def create_data_loader_for_synthetic(config, **kwargs):
       warnings.warn(
           "create_data_loader_for_synthetic is deprecated. "
           "Use DataLoaderFactory.create(config, mode='tensor') instead.",
           DeprecationWarning
       )
       return DataLoaderFactory.create(config, mode='tensor', **kwargs)
   ```

**Status**: Both APIs work, warnings guide migration

---

#### Phase 3: Update Trainers

1. âœ… Update `SyntheticTrainer` to use new factory
2. âœ… Update `PhysicalTrainer` to use new factory
3. âœ… Update `HybridTrainer` to use new factory

```python
# Before
data_loader = TrainerFactory.create_data_loader_for_synthetic(config)

# After
data_loader = DataLoaderFactory.create(config, mode='tensor')
```

---

#### Phase 4: Update run.py

```python
# Before
if model_type == "synthetic":
    data_loader = TrainerFactory.create_data_loader_for_synthetic(
        config, use_sliding_window=True
    )
    trainer.train(data_source=data_loader, num_epochs=num_epochs)

# After
if model_type == "synthetic":
    data_loader = DataLoaderFactory.create(config, mode='tensor')
    trainer.train(data_source=data_loader, num_epochs=num_epochs)
```

---

#### Phase 5: Remove Old Code

1. âŒ Remove `HybridDataset` (or mark as deprecated)
2. âŒ Remove `AdaptiveAugmentedDataLoader`
3. âŒ Remove `AugmentedTensorDataset`
4. âŒ Remove `AugmentedFieldDataset`
5. âŒ Remove old `TrainerFactory` methods

**Status**: Clean, simplified codebase

---

### 4.2 Configuration Changes

#### Minimal Changes Required

Most config stays the same. Only augmentation config changes slightly:

**Before**:
```yaml
trainer_params:
  augmentation:
    enabled: true
    alpha: 0.1
    strategy: "cached"  # or "on_the_fly"
    
    cache:
      enabled: true
      experiment_name: "burgers_128"
      max_memory_samples: 1000
      format: "dict"
```

**After** (simpler):
```yaml
trainer_params:
  augmentation:
    enabled: true
    alpha: 0.1
    mode: "cache"  # or "memory"
    cache_dir: "data/cache/hybrid_generated/burgers_128"
```

**Changes**:
- âœ… `strategy` â†’ `mode` (clearer name)
- âœ… Flat structure (no nested `cache` dict)
- âœ… Direct `cache_dir` path (explicit)

---

## 5. Risk Assessment

### 5.1 High-Level Risks

| Risk | Severity | Mitigation |
|------|----------|------------|
| **Breaking Changes** | Medium | Phased migration with deprecation warnings |
| **Data Compatibility** | Low | DataManager unchanged, cache format same |
| **Performance Regression** | Low | Core loading logic unchanged |
| **Testing Burden** | Medium | Comprehensive test suite needed |
| **Documentation** | Medium | Update all docs and examples |

### 5.2 Detailed Risk Analysis

#### Risk 1: Breaking Existing Code

**Likelihood**: Medium  
**Impact**: High

**Mitigation**:
1. Phased migration (4 phases)
2. Deprecation warnings
3. Backward compatibility layer
4. Comprehensive release notes

**Rollback**: Keep old code until migration complete

---

#### Risk 2: Performance Regression

**Likelihood**: Low  
**Impact**: Medium

**Why Low**: Core caching and loading unchanged

**Testing**:
1. Benchmark loading times
2. Memory usage profiling
3. Training speed comparison

**Acceptance Criteria**:
- âœ… Loading time within 5% of current
- âœ… Memory usage within 10% of current
- âœ… Training throughput unchanged

---

#### Risk 3: Hidden Dependencies

**Likelihood**: Low  
**Impact**: Medium

**Mitigation**:
1. Comprehensive code search
2. Import analysis
3. Unit test coverage

**Testing**:
1. Run full test suite
2. Test all experiment configs
3. Integration tests

---

## 6. Implementation Plan

### 6.1 Timeline

**Total Duration**: 2-3 weeks

| Phase | Duration | Tasks | Status |
|-------|----------|-------|--------|
| **Phase 1** | 3-4 days | Create new components | ðŸ”„ Pending |
| **Phase 2** | 2-3 days | Add v2 methods, deprecation | ðŸ”„ Pending |
| **Phase 3** | 3-4 days | Update trainers | ðŸ”„ Pending |
| **Phase 4** | 1-2 days | Update run.py | ðŸ”„ Pending |
| **Phase 5** | 2-3 days | Remove old code, testing | ðŸ”„ Pending |
| **Documentation** | 2-3 days | Update all docs | ðŸ”„ Pending |

### 6.2 Detailed Tasks

#### Week 1: Foundation

**Days 1-2**: Create base components
- [ ] Implement `AbstractDataset` base class
  - [ ] Port common logic from `HybridDataset`
  - [ ] Implement template methods
  - [ ] Add augmentation management
  - [ ] Write comprehensive tests
- [ ] Implement `ConfigHelper`
  - [ ] Extract all config logic
  - [ ] Document all methods
  - [ ] Write tests

**Days 3-4**: Create concrete datasets and factory
- [ ] Implement `TensorDataset` (inherits from `AbstractDataset`)
  - [ ] Implement abstract methods
  - [ ] Add tensor-specific logic
  - [ ] Write tests
- [ ] Implement `FieldDataset` (inherits from `AbstractDataset`)
  - [ ] Implement abstract methods
  - [ ] Add field reconstruction logic
  - [ ] Write tests
- [ ] Implement `DataLoaderFactory`
  - [ ] Single creation method
  - [ ] Use `ConfigHelper`
  - [ ] Write tests

---

#### Week 2: Migration

**Days 5-6**: Backward compatibility
- [ ] Add v2 methods to `TrainerFactory`
- [ ] Add deprecation warnings
- [ ] Update trainer classes
  - [ ] `SyntheticTrainer`
  - [ ] `PhysicalTrainer`
  - [ ] `HybridTrainer`

**Days 7-8**: Integration
- [ ] Update `run.py`
- [ ] Test all experiment configs
- [ ] Fix any issues

---

#### Week 3: Cleanup & Documentation

**Days 9-10**: Remove old code
- [ ] Remove deprecated classes
- [ ] Clean up imports
- [ ] Run full test suite
- [ ] Performance benchmarks

**Days 11-12**: Documentation
- [ ] Update README
- [ ] Update architecture docs
- [ ] Update tutorial notebooks
- [ ] Write migration guide

---

### 6.3 Testing Strategy

#### Unit Tests

```python
# test_abstract_dataset.py
def test_abstract_dataset_cannot_instantiate():
    """AbstractDataset cannot be instantiated directly."""
    with pytest.raises(TypeError):
        AbstractDataset(...)

def test_sliding_window_index():
    """Test sliding window index building."""
    # Create mock subclass for testing
    class MockDataset(AbstractDataset):
        def _load_simulation_uncached(self, sim_idx):
            return {}
        def _get_real_sample(self, idx):
            return None
    
    dataset = MockDataset(
        data_manager=mock_manager,
        sim_indices=[0, 1],
        field_names=['velocity'],
        num_frames=None,
        num_predict_steps=10,
        use_sliding_window=True,
    )
    
    # Verify sliding window index built correctly
    assert len(dataset.sample_index) > 0
    assert all(isinstance(entry, tuple) for entry in dataset.sample_index)

def test_augmentation_routing():
    """Test that indices route correctly to real vs augmented."""
    dataset = MockDataset(..., augmentation_config={'mode': 'memory', 'data': [...]})
    
    # Real samples
    for i in range(dataset.num_real):
        assert dataset[i] is not None
    
    # Augmented samples
    for i in range(dataset.num_real, len(dataset)):
        assert dataset[i] is not None

# test_config_helper.py
def test_get_field_names():
    config = {...}
    cfg = ConfigHelper(config)
    assert cfg.get_field_names() == ['velocity', 'density']

def test_get_field_types():
    config = {...}
    cfg = ConfigHelper(config)
    dynamic, static = cfg.get_field_types()
    assert dynamic == ['velocity', 'density']
    assert static == ['inflow']

# test_tensor_dataset.py
def test_tensor_dataset_without_augmentation():
    dataset = TensorDataset(...)
    assert len(dataset) == expected_count
    initial, targets = dataset[0]
    assert initial.shape == (C_all, H, W)
    assert targets.shape == (T, C_dynamic, H, W)

def test_tensor_dataset_with_augmentation():
    aug_config = {'mode': 'memory', 'alpha': 0.1, 'data': [...]}
    dataset = TensorDataset(..., augmentation_config=aug_config)
    assert len(dataset) == real_count + augmented_count

# test_dataloader_factory.py
def test_create_tensor_loader():
    loader = DataLoaderFactory.create(config, mode='tensor')
    assert isinstance(loader, DataLoader)
    batch = next(iter(loader))
    assert len(batch) == 2  # (inputs, targets)

def test_create_field_dataset():
    dataset = DataLoaderFactory.create(config, mode='field')
    assert isinstance(dataset, FieldDataset)
    initial, targets = dataset[0]
    assert isinstance(initial, dict)
```

#### Integration Tests

```python
def test_synthetic_training_pipeline():
    """Test complete synthetic training pipeline."""
    config = load_config("burgers_experiment")
    
    # Create loader
    loader = DataLoaderFactory.create(config, mode='tensor')
    
    # Create trainer
    trainer = TrainerFactory.create_trainer(config)
    
    # Train for 1 epoch
    trainer.train(data_source=loader, num_epochs=1)
    
    # Verify checkpoint saved
    assert checkpoint_exists()

def test_hybrid_training_cycle():
    """Test hybrid training with augmentation."""
    config = load_config("hybrid_experiment")
    
    # Create hybrid trainer
    trainer = TrainerFactory.create_hybrid_trainer(config)
    
    # Train for 1 cycle
    trainer.train()  # Internally uses DataLoaderFactory
    
    # Verify both models updated
    assert synthetic_checkpoint_exists()
    assert physical_parameters_updated()
```

---

## 7. Summary

### 7.1 Key Improvements

| Aspect | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Layers** | 5 | 3 | â¬‡ï¸ 40% |
| **Classes** | 10 | 7 | â¬‡ï¸ 30% |
| **Factory LOC** | ~350 | ~120 | â¬‡ï¸ 66% |
| **Config Coupling** | High | Low | â¬‡ï¸ 70% |
| **Code Duplication** | High | None | â¬‡ï¸ 100% |
| **Complexity** | 8/10 | 4/10 | â¬‡ï¸ 50% |

### 7.2 What Gets Simpler

1. âœ… **Data loading flow** - Linear, easy to trace
2. âœ… **Factory usage** - Single method, clear parameters
3. âœ… **Config access** - Centralized helper
4. âœ… **Testing** - Smaller, focused components
5. âœ… **Debugging** - Fewer layers to navigate
6. âœ… **Extension** - Clear extension points (inherit from `AbstractDataset`)
7. âœ… **Documentation** - Less to explain
8. âœ… **Code duplication** - Eliminated via inheritance
9. âœ… **Consistency** - Same pattern as trainer hierarchy

### 7.3 What Stays the Same

1. âœ… **DataManager** - Core caching unchanged
2. âœ… **Cache format** - Backward compatible
3. âœ… **Config structure** - Minimal changes
4. âœ… **Performance** - Same loading efficiency
5. âœ… **Features** - All functionality preserved

### 7.4 Trade-offs

**Pros** âœ…:
- Much simpler to understand
- Easier to maintain
- Less code to test
- Clearer responsibilities
- Easier to extend

**Cons** âš ï¸:
- Migration effort required
- Need comprehensive testing
- Documentation updates needed
- Possible temporary disruption

**Verdict**: **Worth it!** The long-term benefits far outweigh the short-term migration cost.

---

## 8. Next Steps

### 8.1 Decision Points

Before proceeding, we should discuss:

1. **Timeline**: Is 2-3 weeks acceptable?
2. **Phasing**: Should we do incremental migration or all at once?
3. **Backward Compatibility**: How long should we maintain old APIs?
4. **Testing**: What's the acceptable test coverage?
5. **Config Changes**: Are minimal config changes acceptable?

### 8.2 Quick Wins

If full refactoring is too much, we could start with:

1. **Extract ConfigHelper only** (2-3 days)
   - Immediate reduction in coupling
   - No breaking changes
   - Easy to test

2. **Add DataLoaderFactory v2** (3-4 days)
   - New simplified API
   - Coexists with old code
   - Proves concept

3. **Split HybridDataset** (1 week)
   - Biggest complexity reduction
   - Clear responsibility separation
   - Easier to maintain

### 8.3 Questions for Discussion

1. **Priority**: Is simplification a priority now or later?
2. **Scope**: Full refactoring or incremental improvements?
3. **Risk Tolerance**: Comfortable with phased migration?
4. **Testing**: Can we pause features to write comprehensive tests?
5. **Documentation**: Will you help update documentation?

---

**Document Status**: ðŸ“‹ **Ready for Discussion**

**Next Action**: Review and discuss this proposal to determine:
- Go/No-Go decision
- Scope adjustments
- Timeline preferences
- Phasing strategy

---

**Created**: November 3, 2025  
**Author**: GitHub Copilot  
**Reviewers**: [To be assigned]  
**Status**: Draft for Review
