# HYCO Hybrid Training Configuration
# Note: Hardcoded behaviors:
# - validation_rollout: always true
# - save_best_only: always true
# - augmentation uses cached strategy only
# - augmentation is always enabled (inherent to hybrid training)

# General training settings
epochs: 50
num_predict_steps: 10
train_sim: [0, 1, 2]
val_sim: [3]

# Hybrid cycle configuration
# These control the iterative training cycle between synthetic and physical models
hybrid:
  num_cycles: 10  # Number of hybrid training cycles
  synthetic_epochs_per_cycle: 5  # Epochs to train synthetic model per cycle
  physical_epochs_per_cycle: 3  # Epochs to train physical model per cycle
  warmup_synthetic_epochs: 10  # Initial warmup epochs for synthetic model
  
  # ============================================================================
  # Real Data Access Control
  # ============================================================================
  # Controls which models are allowed to train on real data vs. generated data
  # Options:
  #   - 'both': Both models see real data + augmentation (default, standard hybrid)
  #   - 'synthetic_only': Only synthetic model sees real data; physical trains on synthetic predictions
  #   - 'physical_only': Only physical model sees real data; synthetic trains on physical predictions
  #   - 'neither': Neither model sees real data; each trains only on the other's predictions
  #
  # Use cases:
  #   - 'synthetic_only': When you want synthetic model to learn from data, physics to adapt to it
  #   - 'physical_only': When you trust physics more, use data only for parameter calibration
  #   - 'neither': Extreme case - models learn from each other's predictions only
  real_data_access: 'both'  # Default: both models see real data

# Data augmentation configuration (always enabled in hybrid training)
augmentation:
  alpha: 0.1  # Augmentation ratio: num_synthetic = alpha * num_real
  
  # Cache settings
  cache:
    experiment_name: "${data.dset_name}"  # Use dataset name for cache directory
    max_memory_samples: 1000  # LRU cache size (number of samples)
    reuse_existing: true  # Reuse cache from previous runs
  
  # Generation device
  device: "cuda"  # Device for prediction generation (cuda/cpu)

# Synthetic model settings
synthetic:
  learning_rate: 1e-4
  batch_size: 16
  optimizer: adam
  scheduler: cosine
  weight_decay: 0.0

# Physical model settings (epochs controls max iterations)
physical:
  method: 'L-BFGS-B'
  abs_tol: 1e-6
  suppress_convergence_errors: true  # Don't raise errors on non-convergence

# Checkpointing
checkpoint_freq: 10
checkpoint_dir: 'results/models/hybrid'
